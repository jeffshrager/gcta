
@article{adrion2012,
  title = {Bayesian Model Selection Techniques as Decision Support for Shaping a Statistical Analysis Plan of a Clinical Trial: {{An}} Example from a Vertigo Phase {{III}} Study with Longitudinal Count Data as Primary Endpoint},
  volume = {12},
  issn = {1471-2288},
  shorttitle = {Bayesian Model Selection Techniques as Decision Support for Shaping a Statistical Analysis Plan of a Clinical Trial},
  abstract = {Background: A statistical analysis plan (SAP) is a critical link between how a clinical trial is conducted and the clinical study report. To secure objective study results, regulatory bodies expect that the SAP will meet requirements in pre-specifying inferential analyses and other important statistical techniques. To write a good SAP for model-based sensitivity and ancillary analyses involves non-trivial decisions on and justification of many aspects of the chosen setting. In particular, trials with longitudinal count data as primary endpoints pose challenges for model choice and model validation. In the random effects setting, frequentist strategies for model assessment and model diagnosis are complex and not easily implemented and have several limitations. Therefore, it is of interest to explore Bayesian alternatives which provide the needed decision support to finalize a SAP. Methods: We focus on generalized linear mixed models (GLMMs) for the analysis of longitudinal count data. A series of distributions with over- and under-dispersion is considered. Additionally, the structure of the variance components is modified. We perform a simulation study to investigate the discriminatory power of Bayesian tools for model criticism in different scenarios derived from the model setting. We apply the findings to the data from an open clinical trial on vertigo attacks. These data are seen as pilot data for an ongoing phase III trial. To fit GLMMs we use a novel Bayesian computational approach based on integrated nested Laplace approximations (INLAs). The INLA methodology enables the direct computation of leave-one-out predictive distributions. These distributions are crucial for Bayesian model assessment. We evaluate competing GLMMs for longitudinal count data according to the deviance information criterion (DIC) or probability integral transform (PIT), and by using proper scoring rules (e.g. the logarithmic score). Results: The instruments under study provide excellent tools for preparing decisions within the SAP in a transparent way when structuring the primary analysis, sensitivity or ancillary analyses, and specific analyses for secondary endpoints. The mean logarithmic score and DIC discriminate well between different model scenarios. It becomes obvious that the naive choice of a conventional random effects Poisson model is often inappropriate for real-life count data. The findings are used to specify an appropriate mixed model employed in the sensitivity analyses of an ongoing phase III trial.},
  language = {en},
  number = {1},
  journal = {BMC Medical Research Methodology},
  doi = {10.1186/1471-2288-12-137},
  author = {Adrion, Christine and Mansmann, Ulrich},
  month = dec,
  year = {2012},
  pages = {137},
  file = {/home/asher/Zotero/storage/QIG6AQP5/Adrion and Mansmann - 2012 - Bayesian model selection techniques as decision su.pdf}
}

@incollection{alaa2017,
  title = {Bayesian {{Inference}} of {{Individualized Treatment Effects}} Using {{Multi}}-Task {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Alaa, Ahmed M. and {van der Schaar}, Mihaela},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {3424--3432},
  file = {/home/asher/Zotero/storage/65HNCCRY/Alaa and van der Schaar - 2017 - Bayesian Inference of Individualized Treatment Eff.pdf;/home/asher/Zotero/storage/T6C3GLGR/6934-bayesian-inference-of-individualized-treatment-effects-using-multi-task-gaussian-processes.html}
}

@book{berg1997,
  address = {{Cambridge}},
  title = {Rationalizing Medical Work Decision-Support Techniques and Medical Practices},
  publisher = {{MIT Press}},
  author = {Berg, M.},
  year = {1997}
}

@book{berzuini2012,
  address = {{Hoboken, N.J}},
  series = {Wiley Series in Probability and Statistics},
  title = {Causality: Statistical Perspectives and Applications},
  isbn = {978-0-470-66556-5},
  lccn = {QA276.8 .B475 2012},
  shorttitle = {Causality},
  abstract = {"This book looks at a broad collection of contributions from experts in their fields"--},
  language = {en},
  publisher = {{Wiley}},
  author = {Berzuini, Carlo and Dawid, Philip and Bernardinelli, Luisa},
  year = {2012},
  keywords = {Causality (Physics),Causation,Estimation theory,MATHEMATICS / Probability \& Statistics / General},
  file = {/home/asher/Zotero/storage/RVC8W2R5/Berzuini et al. - 2012 - Causality statistical perspectives and applicatio.pdf}
}

@article{betancourt,
  title = {Calibrating {{Model}}-{{Based Inferences}} and {{Decisions}}},
  abstract = {As the frontiers of applied statistics progress through increasingly complex experiments we must exploit increasingly sophisticated inferential models to analyze the observations we make. In order to avoid misleading or outright erroneous inferences we then have to be increasingly diligent in scrutinizing the consequences of those modeling assumptions. Fortunately model-based methods of statistical inference naturally define procedures for quantifying the scope of inferential outcomes and calibrating corresponding decision making processes. In this paper I review the construction and implementation of the particular procedures that arise within frequentist and Bayesian methodologies.},
  language = {en},
  author = {Betancourt, Michael},
  pages = {35},
  file = {/home/asher/Zotero/storage/Z6XH4A8B/Betancourt - Calibrating Model-Based Inferences and Decisions.pdf}
}

@article{boughorbel2016,
  title = {Model {{Comparison}} for {{Breast Cancer Prognosis Based}} on {{Clinical Data}}},
  volume = {11},
  issn = {1932-6203},
  language = {en},
  number = {1},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0146413},
  author = {Boughorbel, Sabri and {Al-Ali}, Rashid and Elkum, Naser},
  editor = {Ebrahimi, Mansour},
  month = jan,
  year = {2016},
  pages = {e0146413},
  file = {/home/asher/Zotero/storage/46DUQYDY/Boughorbel et al. - 2016 - Model Comparison for Breast Cancer Prognosis Based.pdf}
}

@article{boulet2019,
  title = {Bayesian Variable Selection Based on Clinical Relevance Weights in Small Sample Studies\textemdash{{Application}} to Colon Cancer},
  volume = {38},
  issn = {1097-0258},
  abstract = {Using clinical data to model the medical decisions behind sequential treatment actions raises methodological challenges. Physicians often have access to many covariates that may be used when making sequential treatment decisions for individual patients. Statistical variable selection methods may help finding which of these variables are used for this decision in everyday practice. When the sample size is not large, Bayesian variable selection methods can address this setting and allow for expert information to be incorporated into prior distributions. Motivated by clinical practice data involving repeated dose adaptation for Irinotecan in colorectal metastatic cancer, we propose a modification of the stochastic search variable selection (SSVS) method, which we call weight-based SSVS (WBS). We use clinical relevance weights elicited from physician experts to construct prior distributions, with the goal to identify the most influential toxicities and other covariates used for dose adjustment. We evaluate and compare the WBS model performance to the Lasso and SSVS through an extensive simulation study. The simulations show that WBS has better performance and lower rates of false positives and false negatives than the other methods but depends strongly on the covariate weights.},
  language = {en},
  number = {12},
  journal = {Statistics in Medicine},
  doi = {10.1002/sim.8107},
  author = {Boulet, Sandrine and Ursino, Moreno and Thall, Peter and Jannot, Anne-Sophie and Zohar, Sarah},
  year = {2019},
  keywords = {clinical relevance weights elicitation,informative priors,repeated measures,stochastic search variable selection},
  pages = {2228-2247},
  file = {/home/asher/Zotero/storage/34LZJ28R/Boulet et al. - 2019 - Bayesian variable selection based on clinical rele.pdf;/home/asher/Zotero/storage/PYJBY6MX/sim.html}
}

@article{cai2013,
  title = {Bayesian Adaptive Phase {{II}} Screening Design for Combination Trials},
  volume = {10},
  issn = {1740-7745},
  abstract = {Background
Trials of combination therapies for the treatment of cancer are
playing an increasingly important role in the battle against this disease.
To more efficiently handle the large number of combination therapies that
must be tested, we propose a novel Bayesian phase II adaptive screening
design to simultaneously select among possible treatment combinations
involving multiple agents.

Methods
Our design is based on formulating the selection procedure as a
Bayesian hypothesis testing problem in which the superiority of each
treatment combination is equated to a single hypothesis. During the trial
conduct, we use the current values of the posterior probabilities of all
hypotheses to adaptively allocate patients to treatment combinations.

Results
Simulation studies show that the proposed design substantially
outperforms the conventional multiarm balanced factorial trial design. The
proposed design yields a significantly higher probability for selecting the
best treatment while allocating substantially more patients to efficacious
treatments.

Limitations
The proposed design is most appropriate for the trials combining
multiple agents and screening out the efficacious combination to be further
investigated.

Conclusions
The proposed Bayesian adaptive phase II screening design
substantially outperformed the conventional complete factorial design. Our
design allocates more patients to better treatments while providing higher
power to identify the best treatment at the end of the trial.},
  number = {3},
  journal = {Clinical trials (London, England)},
  doi = {10.1177/1740774512470316},
  author = {Cai, Chunyan and Yuan, Ying and Johnson, Valen E},
  year = {2013},
  file = {/home/asher/Zotero/storage/79P6P9ZX/Cai et al. - 2013 - Bayesian adaptive phase II screening design for co.pdf;/home/asher/Zotero/storage/C48E95JJ/Cai et al. - 2013 - Bayesian adaptive phase II screening design for co.pdf},
  pmid = {23359875},
  pmcid = {PMC3867529}
}

@article{chipman2010,
  title = {{{BART}}: {{Bayesian}} Additive Regression Trees},
  volume = {4},
  issn = {1932-6157},
  shorttitle = {{{BART}}},
  abstract = {We develop a Bayesian ``sum-of-trees'' model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
  language = {en},
  number = {1},
  journal = {The Annals of Applied Statistics},
  doi = {10.1214/09-AOAS285},
  author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  month = mar,
  year = {2010},
  pages = {266-298},
  file = {/home/asher/Zotero/storage/NMBSZH62/Chipman et al. - 2010 - BART Bayesian additive regression trees.pdf}
}

@article{chu2018,
  title = {A {{Bayesian}} Basket Trial Design Using a Calibrated {{Bayesian}} Hierarchical Model},
  volume = {15},
  issn = {1740-7745, 1740-7753},
  abstract = {Background: The basket trial evaluates the treatment effect of a targeted therapy in patients with the same genetic or molecular aberration, regardless of their cancer types. Bayesian hierarchical modeling has been proposed to adaptively borrow information across cancer types to improve the statistical power of basket trials. Although conceptually attractive, research has shown that Bayesian hierarchical models cannot appropriately determine the degree of information borrowing and may lead to substantially inflated type I error rates.
Methods: We propose a novel calibrated Bayesian hierarchical model approach to evaluate the treatment effect in basket trials. In our approach, the shrinkage parameter that controls information borrowing is not regarded as an unknown parameter. Instead, it is defined as a function of a similarity measure of the treatment effect across tumor subgroups. The key is that the function is calibrated using simulation such that information is strongly borrowed across subgroups if their treatment effects are similar and barely borrowed if the treatment effects are heterogeneous.
Results: The simulation study shows that our method has substantially better controlled type I error rates than the Bayesian hierarchical model. In some scenarios, for example, when the true response rate is between the null and alternative, the type I error rate of the proposed method can be inflated from 10\% up to 20\%, but is still better than that of the Bayesian hierarchical model. Limitation: The proposed design assumes a binary endpoint. Extension of the proposed design to ordinal and time-toevent endpoints is worthy of further investigation.
Conclusion: The calibrated Bayesian hierarchical model provides a practical approach to design basket trials with more flexibility and better controlled type I error rates than the Bayesian hierarchical model. The software for implementing the proposed design is available at http://odin.mdacc.tmc.edu/\textasciitilde{}yyuan/index\_code.html},
  language = {en},
  number = {2},
  journal = {Clinical Trials},
  doi = {10.1177/1740774518755122},
  author = {Chu, Yiyi and Yuan, Ying},
  month = apr,
  year = {2018},
  pages = {149-158},
  file = {/home/asher/Zotero/storage/6K7IELYQ/Chu and Yuan - 2018 - A Bayesian basket trial design using a calibrated .pdf}
}

@article{chu2018a,
  title = {{{BLAST}}: {{Bayesian}} Latent Subgroup Design for Basket Trials Accounting for Patient Heterogeneity},
  volume = {67},
  issn = {00359254},
  shorttitle = {{{BLAST}}},
  abstract = {The basket trial refers to a new type of phase II cancer trial that evaluates the therapeutic effect of a targeted agent simultaneously in patients with different types of cancer that involve the same genetic or molecular aberration. Although patients who are enrolled in the basket trial have the same molecular aberration, it is common for the targeted agent to be effective for patients with some types of cancer, but not others. We propose a Bayesian latent subgroup trial (BLAST) design to accommodate such treatment heterogeneity across cancer types. We assume that a cancer type may belong to the sensitive subgroup, which is responsive to the treatment, or the insensitive subgroup, which is not responsive to the treatment. Conditionally on the latent subgroup membership of the cancer type, we jointly model the binary treatment response and the longitudinal biomarker measurement that represents the biological activity of the targeted agent. The BLAST design makes the interim go\textendash{}no-go treatment decision in a group sequential fashion for each cancer type on the basis of accumulating data. The simulation study shows that the BLAST design outperforms existing trial designs. It yields high power to detect the treatment effect for sensitive cancer types that are responsive to the treatment and maintains a reasonable type I error rate for insensitive cancer types that are not responsive to the treatment.},
  language = {en},
  number = {3},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  doi = {10.1111/rssc.12255},
  author = {Chu, Yiyi and Yuan, Ying},
  month = apr,
  year = {2018},
  pages = {723-740},
  file = {/home/asher/Zotero/storage/2EGIW7K4/Chu and Yuan - 2018 - BLAST Bayesian latent subgroup design for basket .pdf}
}

@article{cox2019,
  title = {Effects of a {{Personalized Web}}-{{Based Decision Aid}} for {{Surrogate Decision Makers}} of {{Patients With Prolonged Mechanical Ventilation}}: {{A Randomized Clinical Trial}}},
  volume = {170},
  issn = {0003-4819},
  shorttitle = {Effects of a {{Personalized Web}}-{{Based Decision Aid}} for {{Surrogate Decision Makers}} of {{Patients With Prolonged Mechanical Ventilation}}},
  language = {en},
  number = {5},
  journal = {Annals of Internal Medicine},
  doi = {10.7326/M18-2335},
  author = {Cox, Christopher E. and White, Douglas B. and Hough, Catherine L. and Jones, Derek M. and Kahn, Jeremy M. and Olsen, Maren K. and Lewis, Carmen L. and Hanson, Laura C. and Carson, Shannon S.},
  month = mar,
  year = {2019},
  pages = {285},
  file = {/home/asher/Zotero/storage/K9GNCDZV/Cox et al. - 2019 - Effects of a Personalized Web-Based Decision Aid f.pdf}
}

@article{cunanan2017,
  title = {An {{Efficient Basket Trial Design}}},
  volume = {36},
  issn = {0277-6715},
  abstract = {The landscape for early phase cancer clinical trials is changing
dramatically due to the advent of targeted therapy. Increasingly, new drugs are
designed to work against a target such as the presence of a specific tumor
mutation. Since typically only a small proportion of cancer patients will
possess the mutational target, but the mutation is present in many different
cancers, a new class of basket trials is emerging, whereby the drug is tested
simultaneously in different baskets, i.e., sub-groups of different tumor types.
Investigators not only desire to test whether the drug works, but also to
determine which types of tumors are sensitive to the drug. A natural strategy is
to conduct parallel trials, with the drug's effectiveness being tested
separately, using for example, the popular Simon two-stage design independently
in each basket. The work presented is motivated by the premise that the
efficiency of this strategy can be improved by assessing the homogeneity of the
baskets' response rates at an interim analysis and aggregating the
baskets in the second stage if the results suggest the drug might be effective
in all or most baskets. Via simulations we assess the relative efficiencies of
the two strategies. Since the operating characteristics depend on how many tumor
types are sensitive to the drug, there is no uniformly efficient strategy.
However, our investigation demonstrates substantial efficiencies are possible if
the drug works in most or all baskets, at the cost of modest losses of power if
the drug works in only a single basket.},
  number = {10},
  journal = {Statistics in medicine},
  doi = {10.1002/sim.7227},
  author = {Cunanan, Kristen M. and Iasonos, Alexia and Shen, Ronglai and Begg, Colin B. and G{\"o}nen, Mithat},
  month = may,
  year = {2017},
  pages = {1568-1579},
  file = {/home/asher/Zotero/storage/NE4JWYKA/Cunanan et al. - 2017 - An Efficient Basket Trial Design.pdf},
  pmid = {28098411},
  pmcid = {PMC5380524}
}

@article{cunanan2019,
  title = {Variance Prior Specification for a Basket Trial Design Using {{Bayesian}} Hierarchical Modeling},
  volume = {16},
  issn = {1740-7753},
  abstract = {BACKGROUND:: In the era of targeted therapies, clinical trials in oncology are rapidly evolving, wherein patients from multiple diseases are now enrolled and treated according to their genomic mutation(s). In such trials, known as basket trials, the different disease cohorts form the different baskets for inference. Several approaches have been proposed in the literature to efficiently use information from all baskets while simultaneously screening to find individual baskets where the drug works. Most proposed methods are developed in a Bayesian paradigm that requires specifying a prior distribution for a variance parameter, which controls the degree to which information is shared across baskets.
METHODS:: A common approach used to capture the correlated binary endpoints across baskets is Bayesian hierarchical modeling. We evaluate a Bayesian adaptive design in the context of a non-randomized basket trial and investigate three popular prior specifications: an inverse-gamma prior on the basket-level variance, a uniform prior and half-t prior on the basket-level standard deviation.
RESULTS:: From our simulation study, we can see that the inverse-gamma prior is highly sensitive to the input hyperparameters. When the prior mean value of the variance parameter is set to be near zero 
                        
                            (
                            {$\leq$}
                            0
                            .
                            5
                            )
                        
                     , this can lead to unacceptably high false-positive rates 
                        
                            (
                            {$\geq$}
                            40
                            \%
                            )
                        
                     in some scenarios. Thus, use of this prior requires a fully comprehensive sensitivity analysis before implementation. Alternatively, we see that a prior that places sufficient mass in the tail, such as the uniform or half-t prior, displays desirable and robust operating characteristics over a wide range of prior specifications, with the caveat that the upper bound of the uniform prior and the scale parameter of the half-t prior must be larger than 1.
CONCLUSION:: Based on the simulation results, we recommend that those involved in designing basket trials that implement hierarchical modeling avoid using a prior distribution that places a majority of the density mass near zero for the variance parameter. Priors with this property force the model to share information regardless of the true efficacy configuration of the baskets. Many commonly used inverse-gamma prior specifications have this undesirable property. We recommend to instead consider the more robust uniform prior or half-t prior on the standard deviation.},
  language = {eng},
  number = {2},
  journal = {Clinical Trials (London, England)},
  doi = {10.1177/1740774518812779},
  author = {Cunanan, Kristen M. and Iasonos, Alexia and Shen, Ronglai and G{\"o}nen, Mithat},
  month = apr,
  year = {2019},
  keywords = {adaptive design,Basket trial,Bayesian method,phase II,variance prior},
  pages = {142-153},
  file = {/home/asher/Zotero/storage/YS9NGF5R/Cunanan et al. - 2019 - Variance prior specification for a basket trial de.pdf},
  pmid = {30526008}
}

@article{daskalakis2017,
  title = {Optimal {{Stopping Rules}} for {{Sequential Hypothesis Testing}}},
  language = {en},
  doi = {10.4230/lipics.esa.2017.32},
  author = {Daskalakis, Constantinos and Kawase, Yasushi},
  collaborator = {Herbstritt, Marc},
  year = {2017},
  keywords = {000 Computer science; knowledge; general works,Computer Science},
  pages = {14 pages},
  file = {/home/asher/Zotero/storage/6SKLGB68/Daskalakis and Kawase - 2017 - Optimal Stopping Rules for Sequential Hypothesis T.pdf}
}

@article{delgado2019,
  title = {Enhancing {{Confusion Entropy}} ({{CEN}}) for Binary and Multiclass Classification},
  volume = {14},
  issn = {1932-6203},
  abstract = {Different performance measures are used to assess the behaviour, and to carry out the comparison, of classifiers in Machine Learning. Many measures have been defined on the literature, and among them, a measure inspired by Shannon's entropy named the Confusion Entropy (CEN). In this work we introduce a new measure, MCEN, by modifying CEN to avoid its unwanted behaviour in the binary case, that disables it as a suitable performance measure in classification. We compare MCEN with CEN and other performance measures, presenting analytical results in some particularly interesting cases, as well as some heuristic computational experimentation.},
  language = {en},
  number = {1},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0210264},
  author = {Delgado, Rosario and {N{\'u}{\~n}ez-Gonz{\'a}lez}, J. David},
  month = jan,
  year = {2019},
  keywords = {Breast cancer,Decision tree learning,DNA sequence analysis,Entropy,Learning,Machine learning,Machine learning algorithms,Support vector machines},
  pages = {e0210264},
  file = {/home/asher/Zotero/storage/GQTNL4BA/Delgado and Núñez-González - 2019 - Enhancing Confusion Entropy (CEN) for binary and m.pdf;/home/asher/Zotero/storage/LATL2VY6/article.html}
}

@article{eager2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.04858},
  primaryClass = {stat},
  title = {Mixed {{Effects Models}} Are {{Sometimes Terrible}}},
  abstract = {Mixed-effects models have emerged as the gold standard of statistical analysis in different sub-fields of linguistics (Baayen, Davidson \& Bates, 2008; Johnson, 2009; Barr, et al, 2013; Gries, 2015). One problematic feature of these models is their failure to converge under maximal (or even near-maximal) random effects structures. The lack of convergence is relatively unaddressed in linguistics and when it is addressed has resulted in statistical practices (e.g. Jaeger, 2009; Gries, 2015; Bates, et al, 2015b) that are premised on the idea that non-convergence is an indication that a random effects structure is over-specified (or not parsimonious), the parsimonious convergence hypothesis (PCH). We test the PCH by running simulations in lme4 under two sets of assumptions for both a linear dependent variable and a binary dependent variable in order to assess the rate of non-convergence for both types of mixed effects models when a known maximal effect structure is used to generate the data (i.e. when non-convergence cannot be explained by random effects with zero variance). Under the PCH, lack of convergence is treated as evidence against a more maximal random effects structure, but that result is not upheld with our simulations. We provide an alternative model, fully specified Bayesian models implemented in rstan (Stan Development Team, 2016; Carpenter, et al, in press) that removed the convergence problems almost entirely in simulations of the same conditions. These results indicate that when there is known non-zero variance for all slopes and intercepts, under realistic distributions of data and with moderate to severe imbalance, mixed effects models in lme4 have moderate to high non-convergence rates which can cause linguistic researchers to wrongfully exclude random effect terms.},
  journal = {arXiv:1701.04858 [stat]},
  author = {Eager, Christopher and Roy, Joseph},
  month = jan,
  year = {2017},
  keywords = {Statistics - Applications,Statistics - Computation},
  file = {/home/asher/Zotero/storage/VKJYFXM4/Eager and Roy - 2017 - Mixed Effects Models are Sometimes Terrible.pdf;/home/asher/Zotero/storage/Y3WX6S4A/1701.html}
}

@book{elsaghir2014,
  title = {Tumor {{Boards}}: {{Optimizing}} the {{Structure}} and {{Improving Efficiency}} of {{Multidisciplinary Management}} of {{Patients}} with {{Cancer Worldwide}}. {{ASCO ASCO EDUCATIONAL BOOK}}},
  author = {El Saghir, N. S. and Keating, N. L. and Carlson, R. W. and Khoury, K. E. and Fallowfield, L.},
  year = {2014}
}

@book{elstein1978,
  address = {{Cambridge}},
  title = {Medical Problem Solving: An Analysis of Clinical Reasoning},
  publisher = {{Harvard University Press}},
  author = {Elstein, A. S. and Shulman, L. S. and Sprafka, S. A.},
  year = {1978},
  doi = {10.4159/harvard.9780674189089}
}

@article{eslick,
  title = {Crowdsourcing {{Health Discoveries}}: From {{Anecdotes}} to {{Aggregated Self}}-{{Experiments}}},
  abstract = {Nearly one quarter of U.S. adults read patient-generated health information found on blogs, forums, and social media; many say they use this information to influence everyday health decisions. Topics of discussion in online forums are often poorly addressed by existing clinical research, so a patient's reported experiences are the only relevant evidence. No rigorous methods exist to help patients leverage anecdotal evidence to make better decisions. This dissertation reports on multiple prototype systems that help patients augment anecdote with data to improve individual decision making, optimize healthcare delivery, and accelerate research. The web-based systems were developed through a multi-year collaboration with individuals, advocacy organizations, healthcare providers, and biomedical researchers. The result of this work is a new scientific model for crowdsourcing health insights: Aggregated Self-Experiments.},
  language = {en},
  author = {Eslick, Ian Scott},
  pages = {315},
  file = {/home/asher/Zotero/storage/JQAIKQ4N/Eslick - Crowdsourcing Health Discoveries from Anecdotes t.pdf}
}

@article{faria,
  title = {{{REPORT BY THE DECISION SUPPORT UNIT}}},
  language = {en},
  author = {Faria, Rita and Alava, Monica Hernandez and Manca, Andrea and Wailoo, Allan J},
  pages = {86},
  file = {/home/asher/Zotero/storage/PYRLW43R/Faria et al. - REPORT BY THE DECISION SUPPORT UNIT.pdf}
}

@article{fiore2011,
  title = {A Point-of-Care Clinical Trial Comparing Insulin Administered Using a Sliding Scale versus a Weight-Based Regimen},
  volume = {8},
  journal = {Clin Trials},
  doi = {10.1177/1740774511398368},
  author = {Fiore, L. D. and Brophy, M. and Ferguson, R. E. and D'Avolio, L. and Hermos, J. A. and Lew, R. A. and Doros, G. and Conrad, C. H. and O'Neil, J. A. and Sabin, T. P. and Kaufman, J. and Swartz, S. L. and Lawler, E. and Liang, M. H. and Gaziano, J. M. and Lavori, P. W.},
  year = {2011},
  file = {/home/asher/Zotero/storage/JDV7R5DG/Fiore et al. - 2011 - A point-of-care clinical trial comparing insulin a.pdf}
}

@article{franklin2019,
  title = {Perspectives of {{Patients With Cancer}} on the {{Quality}}-{{Adjusted Life Year}} as a {{Measure}} of {{Value}} in {{Healthcare}}},
  volume = {22},
  issn = {10983015},
  abstract = {Objectives: Healthcare expenditures in the United States continue to grow; to control costs, there has been a shift away from volumefocused care to value-based care. The incorporation of patient perspectives in the development of value-based healthcare is critical, yet research addressing this issue is limited. This study explores awareness and understanding of patients with cancer about the quality-adjusted life year (QALY), as well as their perspectives regarding the use of the QALY to measure value in healthcare. Methods: This cross-sectional study used survey methodology to explore patient awareness, understanding, and perspectives on the QALY. A total of 774 patients with cancer and survivors completed this survey in June and July of 2017. Quantitative and qualitative analyses were conducted. Results: Results showed that there is limited awareness of the QALY among patients with cancer and survivors and minimal understanding of how the QALY is used. Only one quarter of respondents believed that the QALY was a good way to measure value in healthcare. Some participants (5\%) stated that the QALY could be personally helpful to them in their own decision making, indicating the possible usefulness of the QALY as a decision aid in cancer care. Nevertheless, participants expressed concern about other decision makers using the QALY to allocate cancer care and resources and maintained a strong desire for autonomy over personal healthcare choices. Conclusions: Although participants believed that the QALY could help them make more informed decisions, there was concern about how it would be used by payers, policymakers, and other decision makers in determining access to care. Implications for policy and research are discussed.},
  language = {en},
  number = {4},
  journal = {Value in Health},
  doi = {10.1016/j.jval.2018.09.2844},
  author = {Franklin, Elizabeth F. and Nichols, Helen M. and Charap, Ellyn and Buzaglo, Joanne S. and Zaleta, Alexandra K. and House, Linda},
  month = apr,
  year = {2019},
  pages = {474-481},
  file = {/home/asher/Zotero/storage/2JYH5WD8/Franklin et al. - 2019 - Perspectives of Patients With Cancer on the Qualit.pdf}
}

@article{frattaroli2008,
  title = {Clinical Events in Prostate Cancer Lifestyle Trial: Results from Two Years of Follow-Up},
  volume = {72},
  journal = {Urology},
  doi = {10.1016/j.urology.2008.04.050},
  author = {Frattaroli, J. and Weidner, G. and Dnistrian, A. M. and Kemp, C. and Daubenmier, J. J. and Marlin, R. O. and Crutchfield, L. and Yglecias, L. and Carroll, P. R. and Ornish, D.},
  year = {2008}
}

@article{freidlin2013,
  title = {Borrowing {{Information}} across {{Subgroups}} in {{Phase II Trials}}: {{Is It Useful}}?},
  volume = {19},
  issn = {1078-0432, 1557-3265},
  shorttitle = {Borrowing {{Information}} across {{Subgroups}} in {{Phase II Trials}}},
  abstract = {Because of the heterogeneity of human tumors, cancer patient populations are usually composed of multiple subgroups with different molecular and/or histologic characteristics. In screening new anticancer agents, there might be a scientific rationale to expect some degree of similarity in clinical activity across the subgroups. This poses a challenge to the design of phase II trials assessing clinical activity: Conducting an independent evaluation in each subgroup requires considerable time and resources, whereas a pooled evaluation that completely ignores patient heterogeneity can miss treatments that are only active in some subgroups. It has been suggested that approaches that borrow information across subgroups can improve efficiency in this setting. In particular, the hierarchical Bayesian approach putatively uses the outcome data to decide whether borrowing of information is appropriate. We evaluated potential benefits of the hierarchical Bayesian approach (using models suggested previously) and a simpler pooling approach by simulations. In the phase II setting, the hierarchical Bayesian approach is shown not to work well in the simulations considered, as there appears to be insufficient information in the outcome data to determine whether borrowing across subgroups is appropriate. When there is strong rationale for expecting a uniform level of activity across the subgroups, approaches using simple pooling of information across subgroups may be useful. Clin Cancer Res; 19(6); 1326\textendash{}34. {\'O}2012 AACR.},
  language = {en},
  number = {6},
  journal = {Clinical Cancer Research},
  doi = {10.1158/1078-0432.CCR-12-1223},
  author = {Freidlin, B. and Korn, E. L.},
  month = mar,
  year = {2013},
  pages = {1326-1334},
  file = {/home/asher/Zotero/storage/GY73EUNV/Freidlin and Korn - 2013 - Borrowing Information across Subgroups in Phase II.pdf}
}

@article{fu2013,
  title = {A Survey on Instance Selection for Active Learning},
  volume = {35},
  issn = {0219-3116},
  abstract = {Active learning aims to train an accurate prediction model with minimum cost by labeling most informative instances. In this paper, we survey existing works on active learning from an instance-selection perspective and classify them into two categories with a progressive relationship: (1) active learning merely based on uncertainty of independent and identically distributed (IID) instances, and (2) active learning by further taking into account instance correlations. Using the above categorization, we summarize major approaches in the field, along with their technical strengths/weaknesses, followed by a simple runtime performance comparison, and discussion about emerging active learning applications and instance-selection challenges therein. This survey intends to provide a high-level summarization for active learning and motivates interested readers to consider instance-selection approaches for designing effective active learning solutions.},
  language = {en},
  number = {2},
  journal = {Knowledge and Information Systems},
  doi = {10.1007/s10115-012-0507-8},
  author = {Fu, Yifan and Zhu, Xingquan and Li, Bin},
  month = may,
  year = {2013},
  keywords = {Active learning survey,Instance correlations,Instance selection,Uncertainty sampling},
  pages = {249-283}
}

@article{gelman2004,
  title = {Parameterization and {{Bayesian Modeling}}},
  volume = {99},
  issn = {0162-1459, 1537-274X},
  language = {en},
  number = {466},
  journal = {Journal of the American Statistical Association},
  doi = {10.1198/016214504000000458},
  author = {Gelman, Andrew},
  month = jun,
  year = {2004},
  pages = {537-545},
  file = {/home/asher/Zotero/storage/BUZRSMFY/Gelman - 2004 - Parameterization and Bayesian Modeling.pdf}
}

@article{golchi2018,
  title = {A Frequency-Calibrated {{Bayesian}} Search for New Particles},
  volume = {12},
  issn = {1932-6157},
  abstract = {The statistical procedure used in the search for new particles is investigated in this paper. The discovery of the Higgs particles is used to lay out the problem and the existing procedures. A Bayesian hierarchical model is proposed to address inference about the parameters of interest while incorporating uncertainty about the nuisance parameters into the model. In addition to inference, a decision making procedure is proposed. A loss function is introduced that mimics the important features of a discovery problem. Given the importance of controlling the ``false discovery'' and ``missed detection'' error rates in discovering new phenomena, the proposed procedure is calibrated to control for these error rates.},
  language = {en},
  number = {3},
  journal = {The Annals of Applied Statistics},
  doi = {10.1214/18-AOAS1138},
  author = {Golchi, Shirin and Lockhart, Richard},
  month = sep,
  year = {2018},
  pages = {1939-1968},
  file = {/home/asher/Zotero/storage/UJQQM575/Golchi and Lockhart - 2018 - A frequency-calibrated Bayesian search for new par.pdf}
}

@article{hernan2006,
  title = {Instruments for {{Causal Inference}}: {{An Epidemiologist}}'s {{Dream}}?},
  volume = {17},
  issn = {1044-3983},
  shorttitle = {Instruments for {{Causal Inference}}},
  abstract = {The use of instrumental variable (IV) methods is attractive because, even in the presence of unmeasured confounding, such methods may consistently estimate the average causal effect of an exposure on an outcome. However, for this consistent estimation to be achieved, several strong conditions must hold. We review the definition of an instrumental variable, describe the conditions required to obtain consistent estimates of causal effects, and explore their implications in the context of a recent application of the instrumental variables approach. We also present (1) a description of the connection between 4 causal models-counterfactuals, causal directed acyclic graphs, nonparametric structural equation models, and linear structural equation models--that have been used to describe instrumental variables methods; (2) a unified presentation of IV methods for the average causal effect in the study population through structural mean models; and (3) a discussion and new extensions of instrumental variables methods based on assumptions of monotonicity.},
  number = {4},
  journal = {Epidemiology},
  author = {Hern{\'a}n, Miguel A. and Robins, James M.},
  year = {2006},
  pages = {360-372},
  file = {/home/asher/Zotero/storage/V53K89N9/Hernán and Robins - 2006 - Instruments for Causal Inference An Epidemiologis.pdf}
}

@incollection{hernan2015,
  title = {Longitudinal {{Causal Inference}}},
  isbn = {978-0-08-097087-5},
  language = {en},
  booktitle = {International {{Encyclopedia}} of the {{Social}} \& {{Behavioral Sciences}}},
  publisher = {{Elsevier}},
  author = {Hern{\'a}n, Miguel A. and Robins, James M.},
  year = {2015},
  pages = {340-344},
  file = {/home/asher/Zotero/storage/74DEMB9K/Hernán and Robins - 2015 - Longitudinal Causal Inference.pdf},
  doi = {10.1016/B978-0-08-097086-8.42100-8}
}

@article{hickey2018,
  title = {{{joineRML}}: A Joint Model and Software Package for Time-to-Event and Multivariate Longitudinal Outcomes},
  volume = {18},
  issn = {1471-2288},
  shorttitle = {{{joineRML}}},
  abstract = {Background: Joint modelling of longitudinal and time-to-event outcomes has received considerable attention over recent years. Commensurate with this has been a rise in statistical software options for fitting these models. However, these tools have generally been limited to a single longitudinal outcome. Here, we describe the classical joint model to the case of multiple longitudinal outcomes, propose a practical algorithm for fitting the models, and demonstrate how to fit the models using a new package for the statistical software platform R, joineRML.
Results: A multivariate linear mixed sub-model is specified for the longitudinal outcomes, and a Cox proportional hazards regression model with time-varying covariates is specified for the event time sub-model. The association between models is captured through a zero-mean multivariate latent Gaussian process. The models are fitted using a Monte Carlo Expectation-Maximisation algorithm, and inferences are based on approximate standard errors from the empirical profile information matrix, which are contrasted to an alternative bootstrap estimation approach. We illustrate the model and software on a real data example for patients with primary biliary cirrhosis with three repeatedly measured biomarkers.
Conclusions: An open-source software package capable of fitting multivariate joint models is available. The underlying algorithm and source code makes use of several methods to increase computational speed.},
  language = {en},
  number = {1},
  journal = {BMC Medical Research Methodology},
  doi = {10.1186/s12874-018-0502-1},
  author = {Hickey, Graeme L. and Philipson, Pete and Jorgensen, Andrea and {Kolamunnage-Dona}, Ruwanthi},
  month = dec,
  year = {2018},
  pages = {50},
  file = {/home/asher/Zotero/storage/MDM52EVV/Hickey et al. - 2018 - joineRML a joint model and software package for t.pdf}
}

@incollection{hilbe2011,
  address = {{Berlin, Heidelberg}},
  title = {Generalized {{Linear Models}}},
  isbn = {978-3-642-04898-2},
  language = {en},
  booktitle = {International {{Encyclopedia}} of {{Statistical Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Hilbe, Joseph M.},
  editor = {Lovric, Miodrag},
  year = {2011},
  pages = {591-596},
  file = {/home/asher/Zotero/storage/A2F64ZXK/Hilbe - 2011 - Generalized Linear Models.pdf},
  doi = {10.1007/978-3-642-04898-2_273}
}

@article{hill2011,
  title = {Bayesian {{Nonparametric Modeling}} for {{Causal Inference}}},
  volume = {20},
  issn = {1061-8600, 1537-2715},
  language = {en},
  number = {1},
  journal = {Journal of Computational and Graphical Statistics},
  doi = {10.1198/jcgs.2010.08162},
  author = {Hill, Jennifer L.},
  month = jan,
  year = {2011},
  pages = {217-240},
  file = {/home/asher/Zotero/storage/BWW8U49D/Hill - 2011 - Bayesian Nonparametric Modeling for Causal Inferen.pdf}
}

@article{imai2005,
  title = {A {{Bayesian}} Analysis of the Multinomial Probit Model Using Marginal Data Augmentation},
  volume = {124},
  issn = {0304-4076},
  abstract = {We introduce a set of new Markov chain Monte Carlo algorithms for Bayesian analysis of the multinomial probit model. Our Bayesian representation of the model places a new, and possibly improper, prior distribution directly on the identifiable parameters and thus is relatively easy to interpret and use. Our algorithms, which are based on the method of marginal data augmentation, involve only draws from standard distributions and dominate other available Bayesian methods in that they are as quick to converge as the fastest methods but with a more attractive prior specification. C-code along with an R interface for our algorithms is publicly available.11R is a freely available statistical computing environment that runs on any platform. The R software that implements the algorithms introduced in this article is available from the first author's website at http://www.princeton.edu/\textasciitilde{}kimai/.},
  number = {2},
  journal = {Journal of Econometrics},
  doi = {10.1016/j.jeconom.2004.02.002},
  author = {Imai, Kosuke and {van Dyk}, David A.},
  month = feb,
  year = {2005},
  keywords = {Bayesian analysis,Data augmentation,Prior distributions,Probit models,Rate of convergence},
  pages = {311-334},
  file = {/home/asher/Zotero/storage/QYAAWHVG/S0304407604000351.html}
}

@article{imbens2004,
  title = {Nonparametric {{Estimation}} of {{Average Treatment Effects Under Exogeneity}}: {{A Review}}},
  volume = {86},
  issn = {0034-6535, 1530-9142},
  shorttitle = {Nonparametric {{Estimation}} of {{Average Treatment Effects Under Exogeneity}}},
  language = {en},
  number = {1},
  journal = {Review of Economics and Statistics},
  doi = {10.1162/003465304323023651},
  author = {Imbens, Guido W.},
  month = feb,
  year = {2004},
  pages = {4-29},
  file = {/home/asher/Zotero/storage/5QTC6IM5/Imbens - 2004 - Nonparametric Estimation of Average Treatment Effe.pdf}
}

@book{imbens2015,
  edition = {1},
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}},
  isbn = {978-0-521-88588-1 978-1-139-02575-1},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  publisher = {{Cambridge University Press}},
  author = {Imbens, Guido W. and Rubin, Donald B.},
  month = apr,
  year = {2015},
  doi = {10.1017/CBO9781139025751}
}

@article{ioannidis2005,
  title = {Why Most Published Research Findings Are False},
  volume = {2},
  journal = {PLoS Med},
  doi = {10.1371/journal.pmed.0020124},
  author = {Ioannidis, J. P. A.},
  year = {2005},
  file = {/home/asher/Zotero/storage/7MH4LR88/Ioannidis - 2005 - Why most published research findings are false.pdf}
}

@article{isakov,
  title = {Is the {{FDA Too Conservative}} or {{Too Aggressive}}?: {{A Bayesian Decision Analysis}} of {{Clinical Trial Design}}},
  abstract = {Implicit in the drug-approval process is a host of decisions\textemdash{}target patient population, control group, primary endpoint, sample size, follow-up period\textemdash{}all of which determine the trade-off between Type I and Type II error. We explore the application of Bayesian decision analysis (BDA) to minimize the expected cost of drug approval, where the relative costs of the two types of errors are calibrated using U.S. Burden of Disease Study 2010 data. The results for conventional fixed-sample randomized clinical-trial designs suggest that for terminal illnesses with no existing therapies such as pancreatic cancer, the standard threshold of 2.5\% is substantially more conservative than the BDA-optimal threshold of 23.9\% to 27.8\%. For relatively less deadly conditions such as prostate cancer, 2.5\% is more risk-tolerant or aggressive than the BDA-optimal threshold of 1.2\% to 1.5\%. We compute BDA-optimal sizes for 25 of the most lethal diseases and show how a BDA-informed approval process can incorporate all stakeholders' views in a systematic, transparent, internally consistent, and repeatable manner.},
  language = {en},
  author = {Isakov, Leah and Lo, Andrew W and Montazerhodjat, Vahid},
  pages = {49},
  file = {/home/asher/Zotero/storage/UGQEFHGK/Isakov et al. - Is the FDA Too Conservative or Too Aggressive A .pdf}
}

@book{kleinbaum2012,
  address = {{New York}},
  edition = {3rd ed},
  series = {Statistics for Biology and Health},
  title = {Survival Analysis: A Self-Learning Text},
  isbn = {978-1-4419-6645-2 978-1-4419-6646-9},
  lccn = {R853.S7 K543 2012},
  shorttitle = {Survival Analysis},
  publisher = {{Springer}},
  author = {Kleinbaum, David G. and Klein, Mitchel},
  year = {2012},
  keywords = {Survival Analysis,Programmed Instruction,Survival analysis (Biometry)},
  file = {/home/asher/Zotero/storage/72U3RNCI/Kleinbaum and Klein - 2012 - Survival analysis a self-learning text.pdf},
  note = {OCLC: ocn760292284}
}

@book{knorr-cetina1981,
  address = {{NY}},
  title = {The Manufacture of Knowledge},
  publisher = {{Pergamon Press}},
  author = {{Knorr-Cetina}, K. D.},
  year = {1981}
}

@article{kopp-schneider2019,
  title = {Monitoring Futility and Efficacy in Phase {{II}} Trials with {{Bayesian}} Posterior Distributions\textemdash{{A}} Calibration Approach},
  volume = {61},
  copyright = {\textcopyright{} 2018 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  issn = {1521-4036},
  abstract = {A multistage single arm phase II trial with binary endpoint is considered. Bayesian posterior probabilities are used to monitor futility in interim analyses and efficacy in the final analysis. For a beta-binomial model, decision rules based on Bayesian posterior probabilities are converted to ``traditional'' decision rules in terms of number of responders among patients observed so far. Analytical derivations are given for the probability of stopping for futility and for the probability to declare efficacy. A workflow is presented on how to select the parameters specifying the Bayesian design, and the operating characteristics of the design are investigated. It is outlined how the presented approach can be transferred to statistical models other than the beta-binomial model.},
  language = {en},
  number = {3},
  journal = {Biometrical Journal},
  doi = {10.1002/bimj.201700209},
  author = {Kopp-Schneider, Annette and Wiesenfarth, Manuel and Witt, Ruth and Edelmann, Dominic and Witt, Olaf and Abel, Ulrich},
  year = {2019},
  keywords = {Bayesian design,calibration,futility,INFORM study,interim analyses},
  pages = {488-502},
  file = {/home/asher/Zotero/storage/HSC9UUB6/Kopp‐Schneider et al. - 2019 - Monitoring futility and efficacy in phase II trial.pdf;/home/asher/Zotero/storage/ESZHW6XU/bimj.html}
}

@article{kuhn2014,
  title = {A Survey and Classification of Controlled Natural Languages},
  volume = {40},
  journal = {Comput Linguist},
  doi = {10.1162/COLI_a_00168},
  author = {Kuhn, T.},
  year = {2014},
  file = {/home/asher/Zotero/storage/GUMU8FYT/Kuhn - 2014 - A survey and classification of controlled natural .pdf}
}

@book{kuhn2013,
  title = {Broadening the Scope of {{Nanopublications}}},
  author = {Kuhn, T. and Barbano, E. P. and Nagy, M. L. and Krauthammer, M.},
  year = {2013},
  file = {/home/asher/Zotero/storage/VWVNCC32/Kuhn et al. - 2013 - Broadening the scope of Nanopublications.pdf},
  doi = {10.1007/978-3-642-38288-8_33}
}

@article{kuhn2016,
  title = {Decentralized Provenance-Aware Publishing with Nanopublications},
  volume = {2},
  issn = {2376-5992},
  abstract = {Publication and archival of scientific results is still commonly considered the responsability of classical publishing companies. Classical forms of publishing, however, which center around printed narrative articles, no longer seem well-suited in the digital age. In particular, there exist currently no efficient, reliable, and agreed-upon methods for publishing scientific datasets, which have become increasingly important for science. In this article, we propose to design scientific data publishing as a web-based bottom-up process, without top-down control of central authorities such as publishing companies. Based on a novel combination of existing concepts and technologies, we present a server network to decentrally store and archive data in the form of nanopublications, an RDF-based format to represent scientific data. We show how this approach allows researchers to publish, retrieve, verify, and recombine datasets of nanopublications in a reliable and trustworthy manner, and we argue that this architecture could be used as a low-level data publication layer to serve the Semantic Web in general. Our evaluation of the current network shows that this system is efficient and reliable.},
  journal = {PeerJ Computer Science},
  doi = {10.7717/peerj-cs.78},
  author = {Kuhn, Tobias and Chichester, Christine and Krauthammer, Michael and {Queralt-Rosinach}, N{\'u}ria and Verborgh, Ruben and Giannakopoulos, George and Ngonga Ngomo, Axel-Cyrille and Viglianti, Raffaele and Dumontier, Michel},
  month = aug,
  year = {2016},
  keywords = {Data publishing,Linked Data,Nanopublications,Provenance,Semantic Web},
  pages = {e78},
  file = {/home/asher/Zotero/storage/25PJCE5X/Kuhn et al. - 2016 - Decentralized provenance-aware publishing with nan.pdf}
}

@article{lyden2010,
  title = {An Ethical Hierarchy for Decision Making during Medical Emergencies},
  volume = {67},
  journal = {Ann Neurol},
  doi = {10.1002/ana.21997},
  author = {Lyden, P. D. and Meyer, B. C. and Hemmen, T. M. and Rapp, K. S.},
  year = {2010},
  file = {/home/asher/Zotero/storage/NFWSM46P/Lyden et al. - 2010 - An ethical hierarchy for decision making during me.pdf}
}

@article{mitchell1986,
  title = {Explanation-Based Generalization: A Unifying View},
  volume = {1},
  journal = {Mach Learn},
  author = {Mitchell, T. M. and Keller, R. M. and {Kedar-Cabelli}, S. T.},
  year = {1986}
}

@article{mocellin2010,
  title = {Targeted Therapy Database ({{TTD}}): A Model to Match {{Patient}}'s Molecular Profile with Current Knowledge on {{Cancer}} Biology},
  volume = {5},
  journal = {PLoS One},
  doi = {10.1371/journal.pone.0011965},
  author = {Mocellin, S. and Shrager, J. and Scolyer, R. and Pasquali, S. and Verdi, D. and Marincola, F. M.},
  year = {2010},
  file = {/home/asher/Zotero/storage/PY739KPW/Mocellin et al. - 2010 - Targeted therapy database (TTD) a model to match .pdf}
}

@article{mohamed2010,
  title = {Patient {{Preferences}} and {{Linear Scoring Rules}} for {{Patient}}-{{Reported Outcomes}}:},
  volume = {3},
  issn = {1178-1653},
  shorttitle = {Patient {{Preferences}} and {{Linear Scoring Rules}} for {{Patient}}-{{Reported Outcomes}}},
  abstract = {Background: Many patient-reported outcomes (PRO) instruments are scored the copyright of the by averaging or summing Likert category values over all items or domains of the elicitation instrument, yielding domain-specific scores or a total score for the entire instrument.
Objective: To evaluate differences between conventional linear and preferenceoriginal publisher. weighted scores for PRO instruments used in asthma, oncology, and obesity.
Methods: We estimated preference-weighted scores for all the items and response categories in the Onset-of-Effect Questionnaire (OEQ), the European Organisation for Research and Treatment of Cancer Quality of Life QuesUnauthorised copying tionnaire (EORTC QLQ)-C30, and the Impact of Weight on Quality of Life Questionnaire-Lite version (IWQOL-Lite) using choice-format conjoint analysis, known also as discrete-choice experiments.
Results: Conventional linear scoring rules can overstate the relative importance to patients of improvements in some domains and understate the relaand distribution tive importance of improvements in other domains.
Conclusions: Patient preference-weighted scores estimated by conjointanalysis methods allow for non-linearities and account for the relative contribution of individual items and domains to patient well-being. Conventional is prohibited. linear scores and preference-weighted scores can result in different conclusions about the size of patient-reported treatment effects.},
  language = {en},
  number = {4},
  journal = {The Patient: Patient-Centered Outcomes Research},
  doi = {10.2165/11537880-000000000-00000},
  author = {Mohamed, Ateesha F. and Hauber, A. Brett and Johnson, F. Reed and Coon, Cheryl D.},
  month = dec,
  year = {2010},
  pages = {217-227},
  file = {/home/asher/Zotero/storage/VTISK6EY/Mohamed et al. - 2010 - Patient Preferences and Linear Scoring Rules for P.pdf}
}

@book{morgan2015,
  title = {Counterfactuals and {{Causal Inference}}},
  isbn = {978-1-107-06507-9},
  abstract = {In this second edition of Counterfactuals and Causal Inference, completely revised and expanded, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which, conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed.},
  language = {en},
  publisher = {{Cambridge University Press}},
  author = {Morgan, Stephen L. and Winship, Christopher},
  year = {2015},
  keywords = {Mathematics / Probability \& Statistics / General,Social Science / Research,Social Science / Sociology / General}
}

@article{mullera,
  title = {{{FDR}} and {{Bayesian Multiple Comparisons Rules}}},
  abstract = {We discuss Bayesian approaches to multiple comparison problems, using a decision theoretic perspective to critically compare competing approaches. We set up decision problems that lead to the use of FDR-based rules and generalizations. Alternative definitions of the probability model and the utility function lead to different rules and problem-specific adjustments. Using a loss function that controls realized FDR we derive an optimal Bayes rule that is a variation of the Benjamini and Hochberg (1995) procedure. The cutoff is based on increments in ordered posterior probabilities instead of ordered p- values. Throughout the discussion we take a Bayesian perspective. In particular, we focus on conditional expected FDR, conditional on the data. Variations of the probability model include explicit modeling for dependence. Variations of the utility function include weighting by the extent of a true negative and accounting for the impact in the final decision.},
  language = {en},
  author = {Muller, Peter and Parmigiani, Giovanni and Rice, Kenneth},
  pages = {20},
  file = {/home/asher/Zotero/storage/RF3UF2EP/Muller et al. - FDR and Bayesian Multiple Comparisons Rules.pdf}
}

@misc{muller,
  title = {Bayesian {{Clustering}} with {{Regression}}},
  abstract = {We propose a probability model for random partitions in the presence of covariates. In other words, we develop a model-based clustering algorithm that exploits available covariates. The motivating application is predicting time to progression for patients in a breast cancer trial. We proceed by reporting a weighted average of the responses of clusters of earlier patients. The weights should be determined by the similarity of the new  patient's covariate with the covariates of patients in each cluster. We achieve the desired inference by defining a random partition model that includes a regression on covariates. Patients with similar covariates are a priori more likely to be clustered together. Posterior predictive inference in this model formalizes the desired prediction. We build on product partition models (PPM). We define an extension of the PPM to include a regression on covariates by including in the cohesion function a new factor that increases the probability of experimental units with similar covariates to be included in the same cluster. We discuss implementations suitable for any combination of continuous, categorical, count and ordinal covariates.},
  howpublished = {https://web.ma.utexas.edu/users/pmueller/pap/MQR08.pdf},
  author = {Muller, Peter and Quintana, Fernando and Gary, Rosner},
  file = {/home/asher/Zotero/storage/NJZTFHKT/MQR08.pdf}
}

@article{noren2013,
  title = {Shrinkage Observed-to-Expected Ratios for Robust and Transparent Large-Scale Pattern Discovery},
  volume = {22},
  issn = {0962-2802},
  abstract = {Large observational data sets are a great asset to better understand the effects of medicines in clinical practice and, ultimately, improve patient care. For an empirical pattern in observational data to be of practical relevance, it should represent a substantial deviation from the null model. For the purpose of identifying such deviations, statistical significance tests are inadequate, as they do not on their own distinguish the magnitude of an effect from its data support. The observed-to-expected (OE) ratio on the other hand directly measures strength of association and is an intuitive basis to identify a range of patterns related to event rates, including pairwise associations, higher order interactions and temporal associations between events over time. It is sensitive to random fluctuations for rare events with low expected counts but statistical shrinkage can protect against spurious associations. Shrinkage OE ratios provide a simple but powerful framework for large-scale pattern discovery. In this article, we outline a range of patterns that are naturally viewed in terms of OE ratios and propose a straightforward and effective statistical shrinkage transformation that can be applied to any such ratio. The proposed approach retains emphasis on the practical relevance and transparency of highlighted patterns, while protecting against spurious associations.},
  language = {en},
  number = {1},
  journal = {Statistical Methods in Medical Research},
  doi = {10.1177/0962280211403604},
  author = {Nor{\'e}n, G Niklas and Hopstadius, Johan and Bate, Andrew},
  month = feb,
  year = {2013},
  pages = {57-69},
  file = {/home/asher/Zotero/storage/3M8BT9SL/Norén et al. - 2013 - Shrinkage observed-to-expected ratios for robust a.pdf;/home/asher/Zotero/storage/FCVKPSEF/Norén et al. - 2013 - Shrinkage observed-to-expected ratios for robust a.pdf}
}

@article{nunes2018,
  title = {Issues in the {{Probability Elicitation Process}} of {{Expert}}-{{Based Bayesian Networks}}},
  abstract = {A major challenge in constructing a Bayesian network (BN) is defining the node probability tables (NPT), which can be learned from data or elicited from domain experts. In practice, it is common not to have enough data for learning, and elicitation from experts is the only option. However, the complexity of defining NPT grows exponentially, making their elicitation process costly and error-prone. In this research, we conducted an exploratory study through a literature review that identified the main issues related to the task of probability elicitation and solutions to construct large-scale NPT while reducing the exposure to these issues. In this chapter, we present in detail three semiautomatic methods that reduce the burden for experts. We discuss the benefits and drawbacks of these methods, and present directions on how to improve them.},
  language = {en},
  journal = {Enhanced Expert Systems},
  doi = {10.5772/intechopen.81602},
  author = {Nunes, Jo{\~a}o and Barbosa, Mirko and Silva, Luiz and Gorg{\^o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  month = nov,
  year = {2018},
  file = {/home/asher/Zotero/storage/F2VXZIRL/Nunes et al. - 2018 - Issues in the Probability Elicitation Process of E.pdf;/home/asher/Zotero/storage/HUNK4IRJ/issues-in-the-probability-elicitation-process-of-expert-based-bayesian-networks.html}
}

@article{oleson2010,
  title = {Bayesian Credible Intervals for Binomial Proportions in a Single Patient Trial},
  volume = {19},
  issn = {0962-2802},
  abstract = {Practitioners are often asking if the treatment successfully improved performance. Many times this question is directed towards the outcome of a single individual. In this article, we develop a method to assess the improvement of a single individual who is administered a test of percent correct at pre-treatment and post-treatment. A Bayesian approach is taken where the number correct is modelled as a binomial random variable and the percent correct is set to a beta prior distribution. The first model assumes percent correct at pre-test is equal to the percent correct at post-test and the posterior predictive distribution is used to evaluate the change in the number correct. We subsequently model the proportions correct at pre-test and post-test as unequal. The second model then assumes independent proportions and the third assumes correlated beta distributions for the two proportions. 95\% credible intervals are calculated for the various methods for number of correct at post-test given a particular level at pre-test. An example using data from a cochlear implant clinical trial is presented where clinicians recorded percent correct in a consonant-nucleus-consonant test.},
  number = {6},
  journal = {Statistical Methods in Medical Research},
  doi = {10.1177/0962280209349008},
  author = {Oleson, Jacob J},
  month = dec,
  year = {2010},
  pages = {559-574},
  file = {/home/asher/Zotero/storage/HNYU2M3D/Oleson - 2010 - Bayesian credible intervals for binomial proportio.pdf},
  pmid = {20181779},
  pmcid = {PMC3307549}
}

@article{orbanz,
  title = {Lecture {{Notes}} on {{Bayesian Nonparametrics}}},
  language = {en},
  author = {Orbanz, Peter},
  pages = {108},
  file = {/home/asher/Zotero/storage/CJRNDYMA/Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf}
}

@article{pearl2009,
  title = {Causal Inference in Statistics: {{An}} Overview},
  volume = {3},
  journal = {Statistics Surveys},
  doi = {10.1214/09-SS057},
  author = {Pearl, Judea},
  year = {2009},
  pages = {96--146},
  file = {/home/asher/Zotero/storage/AY5FXRX3/2012_Shollar_Molecular-GuidedTherapyinPatientswithRecurrent.pdf;/home/asher/Zotero/storage/SPQU5YIW/Pearl - 2009 - Causal inference in statistics An overview.pdf}
}

@article{rappaport2004,
  title = {Smart Search and Analysis of {{ASCO}} Abstracts: The 2003 {{ASCO}} Pilot Breast {{Cancer}} Information Exchange ({{BCIE}}) Project},
  volume = {22, no. 14\_suppl},
  journal = {J Clin Oncol},
  doi = {10.1200/jco.2004.22.14_suppl.6066},
  author = {Rappaport, A. T. and Adamson, D. R. and Shih, L. and Smith, R. G. and Tenenbaum, J. M. and Khoo, B. and Cho, S. and Wolff, A. C. and Carlson, R. W. and Whippen, D. A.},
  year = {2004}
}

@misc{research2019,
  title = {Rare {{Diseases}}: {{Common Issues}} in {{Drug Development Guidance}} for {{Industry}}},
  shorttitle = {Rare {{Diseases}}},
  abstract = {Rare Diseases: Common Issues in Drug Development Guidance for Industry},
  language = {en},
  howpublished = {/regulatory-information/search-fda-guidance-documents/rare-diseases-common-issues-drug-development-guidance-industry},
  author = {and Research, Center for Drug Evaluation},
  year = {Fri, 02/08/2019 - 21:52},
  file = {/home/asher/Zotero/storage/SZU94GXD/rare-diseases-common-issues-drug-development-guidance-industry.html}
}

@article{rl,
  title = {Design and {{Implementation}} of {{N}}-of-1 {{Trials}}: {{A User}}'s {{Guide}}},
  language = {en},
  author = {Rl, Kravitz and Nb, Gabler and Hc, Kaplan and Rl, Kravitz and Eb, Larson and Wd, Pace and Ch, Schmid},
  pages = {94},
  file = {/home/asher/Zotero/storage/AN2BIRCH/Rl et al. - Design and Implementation of N-of-1 Trials A User.pdf}
}

@article{roest1997,
  title = {The Use of Confidence Intervals for Individual Utilities: Limits to Formal Decision Analysis for Treatment Choice},
  volume = {17},
  issn = {0272-989X},
  shorttitle = {The Use of Confidence Intervals for Individual Utilities},
  abstract = {This paper discusses the use of confidence intervals for utility measurements. Classic test theory is applied to estimate confidence intervals for utilities. The theory is enhanced to calculate confidence areas for combined utilities and confidence bands for the threshold line. As an example it is shown that, if confidence intervals are taken into account, the implied preferred treatment of T3-larynx carcinoma patients is uncertain for a wide range of utilities, considering the mediocre reliability of most methods of utility assessment. This implies that although utility measurement and formal decision analysis can be a useful way to look at the decision problem, ambiguity, which must be resolved by other means, will often remain.},
  language = {eng},
  number = {3},
  journal = {Medical Decision Making: An International Journal of the Society for Medical Decision Making},
  doi = {10.1177/0272989X9701700304},
  author = {Roest, F. H. and Eijkemans, M. J. and {van der Donk}, J. and Levendag, P. C. and Meeuwis, C. A. and Schmitz, P. I. and Habbema, J. D.},
  year = {1997 Jul-Sep},
  keywords = {Aged,Confidence Intervals,Decision Support Techniques,Decision Trees,Humans,Laryngeal Neoplasms,Male,Neoplasm Staging,Survival Analysis,Therapeutics,Treatment Outcome},
  pages = {285-291},
  pmid = {9219188}
}

@article{rubin1978,
  title = {Bayesian {{Inference}} for {{Causal Effects}}: {{The Role}} of {{Randomization}}},
  volume = {6},
  issn = {0090-5364},
  shorttitle = {Bayesian {{Inference}} for {{Causal Effects}}},
  language = {en},
  number = {1},
  journal = {The Annals of Statistics},
  doi = {10.1214/aos/1176344064},
  author = {Rubin, Donald B.},
  month = jan,
  year = {1978},
  pages = {34-58},
  file = {/home/asher/Zotero/storage/ZII3QM8Q/Rubin - 1978 - Bayesian Inference for Causal Effects The Role of.pdf}
}

@incollection{rubin2005,
  series = {Bayesian {{Thinking}}},
  title = {Bayesian {{Inference}} for {{Causal Effects}}},
  volume = {25},
  abstract = {A central problem in statistics is how to draw inferences about the causal effects of treatments (i.e., interventions) from randomized and nonrandomized data. For example, does the new job-training program really improve the quality of jobs for those trained, or does exposure to that chemical in drinking water increase cancer rates? This presentation provides a brief overview of the Bayesian approach to the estimation of such causal effects based on the concept of potential outcomes.},
  booktitle = {Handbook of {{Statistics}}},
  publisher = {{Elsevier}},
  author = {Rubin, Donald B.},
  editor = {Dey, D. K. and Rao, C. R.},
  month = jan,
  year = {2005},
  pages = {1-16},
  file = {/home/asher/Zotero/storage/XRHNU3D2/S0169716105250010.html},
  doi = {10.1016/S0169-7161(05)25001-0}
}

@incollection{schulam2017,
  title = {Reliable {{Decision Support}} Using {{Counterfactual Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Schulam, Peter and Saria, Suchi},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {1697--1708},
  file = {/home/asher/Zotero/storage/3E5RVIIL/Schulam and Saria - 2017 - Reliable Decision Support using Counterfactual Mod.pdf}
}

@article{settles,
  title = {Active {{Learning Literature Survey}}},
  language = {en},
  author = {Settles, Burr},
  pages = {67},
  file = {/home/asher/Zotero/storage/ACZMP6SS/Settles - Active Learning Literature Survey.pdf}
}

@article{shrager2016,
  title = {Precision Medicine: Fantasy Meets Reality},
  volume = {353},
  journal = {Science},
  doi = {10.1126/science.aai8483},
  author = {Shrager, J.},
  year = {2016}
}

@article{shrager2014,
  title = {Rapid {{Learning Precision Oncology}}},
  volume = {11},
  journal = {Nat Rev Clin Oncol},
  doi = {10.1038/nrclinonc.2013.244},
  author = {Shrager, J. and Tenenbaum, J. M.},
  year = {2014}
}

@article{shrager2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1308.1066v1},
  title = {Theoretical {{Issues}} for {{Global Cumulative Treatment Analysis}} ({{GCTA}})},
  abstract = {Adaptive trials are now mainstream science. Recently, researchers have taken the adaptive trial concept to its natural conclusion, proposing what we call "Global Cumulative Treatment Analysis" (GCTA). Similar to the adaptive trial, decision making and data collection and analysis in the GCTA are continuous and integrated, and treatments are ranked in accord with the statistics of this information, combined with what offers the most information gain. Where GCTA differs from an adaptive trial, or, for that matter, from any trial design, is that all patients are implicitly participants in the GCTA process, regardless of whether they are formally enrolled in a trial. This paper discusses some of the theoretical and practical issues that arise in the design of a GCTA, along with some preliminary thoughts on how they might be approached.},
  author = {Shrager, Jeff},
  month = aug,
  year = {2013},
  keywords = {cs.LG,stat.AP}
}

@article{shukuya2016,
  title = {Relationship between {{Overall Survival}} and {{Response}} or {{Progression}}-{{Free Survival}} in {{Advanced Non}}\textendash{{Small Cell Lung Cancer Patients Treated}} with {{Anti}}\textendash{{PD}}-1/{{PD}}-{{L1 Antibodies}}},
  volume = {11},
  issn = {15560864},
  abstract = {Introduction: Alternative predictive end points for overall survival (OS), such as tumor response and progression-free survival (PFS), are useful in the early detection of drug efficacy; however, they have not been fully investigated in patients with advanced NSCLC treated with anti\textendash{}programmed death protein 1 (PD-1)/programmed death ligand 1 (PD-L1) antibodies.},
  language = {en},
  number = {11},
  journal = {Journal of Thoracic Oncology},
  doi = {10.1016/j.jtho.2016.07.017},
  author = {Shukuya, Takehito and Mori, Keita and Amann, Joseph M. and Bertino, Erin M. and Otterson, Gregory A. and Shields, Peter G. and Morita, Satoshi and Carbone, David P.},
  month = nov,
  year = {2016},
  pages = {1927-1939},
  file = {/home/asher/Zotero/storage/4YLLSLTA/Shukuya et al. - 2016 - Relationship between Overall Survival and Response.pdf}
}

@article{simon1989,
  title = {Optimal Two-Stage Designs for Phase {{II}} Clinical Trials},
  volume = {10},
  issn = {01972456},
  abstract = {The primary objective of a phase II clinical trial of a new drug or regimen is to determine whether it has sufficient biological activity against the disease under study to warrant more extensive development. Such trials are often conducted in a multiinstitution setting where designs of more than two stages are difficult to manage. This paper presents two-stage designs that are optimal in the sense that the expected sample size is minimized if the regimen has low activity subject to constraints upon the size of the type 1 and type 2 errors. Two-stage designs which minimize the maximum sample size are also determined. Optimum and "minimax" designs for a range of design parameters are tabulated. These designs can also be used for pilot studies of new regimens where toxicity is the endpoint of interest.},
  language = {en},
  number = {1},
  journal = {Controlled Clinical Trials},
  doi = {10.1016/0197-2456(89)90015-9},
  author = {Simon, Richard},
  month = mar,
  year = {1989},
  pages = {1-10},
  file = {/home/asher/Zotero/storage/8B7KSDGE/Simon - 1989 - Optimal two-stage designs for phase II clinical tr.pdf;/home/asher/Zotero/storage/UA92RCPA/Simon - 1989 - Optimal two-stage designs for phase II clinical tr.pdf}
}

@article{simon2016,
  title = {The {{Bayesian}} Basket Design for Genomic Variant-Driven Phase {{II}} Trials},
  volume = {43},
  issn = {00937754},
  abstract = {Basket clinical trials are a new category of early clinical trials in which a treatment is evaluated in a population of patients with tumors of various histologic types and primary sites selected for containing specific genomic abnormalities. The objective of such studies is generally to discover histologic types in which the treatment is active. Basket trials are early discovery trials whose results should be confirmed in expanded histology specific cohorts. In this report, we develop a design for planning, monitoring, and analyzing basket trials. A website for using the new design is available at https://brbnci.shinyapps.io/ BasketTrials/ and the software is available at GitHub in the "Basket Trials" repository of account brbnci. \& 2016 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
  language = {en},
  number = {1},
  journal = {Seminars in Oncology},
  doi = {10.1053/j.seminoncol.2016.01.002},
  author = {Simon, Richard and Geyer, Susan and Subramanian, Jyothi and Roychowdhury, Sameek},
  month = feb,
  year = {2016},
  pages = {13-18},
  file = {/home/asher/Zotero/storage/BWXBDGSP/Simon et al. - 2016 - The Bayesian basket design for genomic variant-dri.pdf}
}

@incollection{smets1993,
  title = {Jeffrey's Rule of Conditioning Generalized to Belief Functions.},
  isbn = {978-1-4832-1451-1},
  abstract = {Jeffrey's rule of conditioning has been proposed in order to revise a probability measure by another probability function. We generalize it within the framework of the models based on belief functions. We show that several forms of Jeffrey's conditionings can be defined that correspond to the geometrical rule of conditioning and to Dempster's rule of conditioning, respectively.},
  language = {en},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  publisher = {{Elsevier}},
  author = {Smets, Philippe},
  year = {1993},
  pages = {500-505},
  file = {/home/asher/Zotero/storage/74TYE2LM/Smets - 1993 - Jeffrey's rule of conditioning generalized to beli.pdf},
  doi = {10.1016/B978-1-4832-1451-1.50065-2}
}

@article{sorensen2016,
  title = {Bayesian Linear Mixed Models Using {{Stan}}: {{A}} Tutorial for Psychologists, Linguists, and Cognitive Scientists},
  volume = {12},
  issn = {2292-1354},
  shorttitle = {Bayesian Linear Mixed Models Using {{Stan}}},
  abstract = {With the arrival of the R packages nlme and lme4, linear mixed models (LMMs) have come to be widely used in experimentally-driven areas like psychology, linguistics, and cognitive science. This tutorial provides a practical introduction to fitting LMMs in a Bayesian framework using the probabilistic programming language Stan. We choose Stan (rather than WinBUGS or JAGS) because it provides an elegant and scalable framework for fitting models in most of the standard applications of LMMs. We ease the reader into fitting increasingly complex LMMs, using a twocondition repeated measures self-paced reading study.},
  language = {en},
  number = {3},
  journal = {The Quantitative Methods for Psychology},
  doi = {10.20982/tqmp.12.3.p175},
  author = {Sorensen, Tanner and Hohenstein, Sven and Vasishth, Shravan},
  month = oct,
  year = {2016},
  pages = {175-200},
  file = {/home/asher/Zotero/storage/V58DK9RB/Sorensen et al. - 2016 - Bayesian linear mixed models using Stan A tutoria.pdf}
}

@inproceedings{stevovic2013,
  title = {Adding Individual Patient Case Data to the {{Melanoma Targeted Therapy Advisor}}},
  booktitle = {Proc. 7th {{Int}}. {{Conf}}. {{Pervasive Computing Technologies}} for {{Healthcare}} and {{Workshops}}},
  author = {Stevovic, J. and Shrager, J. and Maxhuni, A. and Convertino, G. and Khaghanifar, I. and Gobbel, R.},
  month = may,
  year = {2013},
  keywords = {biomedical knowledge,cancer,Cancer,Cryptography,direct patient profiling,Drugs,genomic information,genomically targeted cancer treatments,genomics,Genomics,individual patient case data addition,knowledge based systems,literature evidence,Malignant tumors,medical computing,Medical diagnostic imaging,melanoma,melanoma targeted therapy advisor,patient treatment,patient TTA,personalized medicine,personalized TTA,PTTA,Sensitivity,statistical analysis,statistical evidence,targeted therapies,therapy efficacy,TTA knowledge base},
  pages = {85--88}
}

@article{sundin2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.05268v1},
  title = {Active {{Learning}} for {{Decision}}-{{Making}} from {{Imbalanced Observational Data}}},
  abstract = {Machine learning can help personalized decision support by learning models to predict individual treatment effects (ITE). This work studies the reliability of prediction-based decision-making in a task of deciding which action \$a\$ to take for a target unit after observing its covariates \$\textbackslash{}tildex\$ and predicted outcomes \$\textbackslash{}hatp(\textbackslash{}tildey \textbackslash{}mid \textbackslash{}tildex, a)\$. An example case is personalized medicine and the decision of which treatment to give to a patient. A common problem when learning these models from observational data is imbalance, that is, difference in treated/control covariate distributions, which is known to increase the upper bound of the expected ITE estimation error. We propose to assess the decision-making reliability by estimating the ITE model's Type S error rate, which is the probability of the model inferring the sign of the treatment effect wrong. Furthermore, we use the estimated reliability as a criterion for active learning, in order to collect new (possibly expensive) observations, instead of making a forced choice based on unreliable predictions. We demonstrate the effectiveness of this decision-making aware active learning in two decision-making tasks: in simulated data with binary outcomes and in a medical dataset with synthetic and continuous treatment outcomes.},
  author = {Sundin, Iiris and Schulam, Peter and Siivola, Eero and Vehtari, Aki and Saria, Suchi and Kaski, Samuel},
  month = apr,
  year = {2019},
  keywords = {cs.LG,stat.ML},
  file = {/home/asher/Zotero/storage/KZSYFRLC/Sundin et al. - 2019 - Active Learning for Decision-Making from Imbalance.pdf}
}

@article{swanson2018,
  title = {Partial {{Identification}} of the {{Average Treatment Effect Using Instrumental Variables}}: {{Review}} of {{Methods}} for {{Binary Instruments}}, {{Treatments}}, and {{Outcomes}}},
  volume = {113},
  issn = {0162-1459, 1537-274X},
  shorttitle = {Partial {{Identification}} of the {{Average Treatment Effect Using Instrumental Variables}}},
  language = {en},
  number = {522},
  journal = {Journal of the American Statistical Association},
  doi = {10.1080/01621459.2018.1434530},
  author = {Swanson, Sonja A. and Hern{\'a}n, Miguel A. and Miller, Matthew and Robins, James M. and Richardson, Thomas S.},
  month = apr,
  year = {2018},
  pages = {933-947},
  file = {/home/asher/Zotero/storage/7GXFQBL5/Swanson et al. - 2018 - Partial Identification of the Average Treatment Ef.pdf}
}

@article{sweetnam2018,
  title = {Prototyping a Precision Oncology 3.0 Rapid Learning Platform},
  volume = {19},
  number = {1},
  journal = {BMC Bioinformatics},
  doi = {10.1186/s12859-018-2374-0},
  author = {Sweetnam, Connor and Mocellin, Simone and Krauthammer, Michael and Knopf, Nathaniel and Baertsch, Robert and Shrager, Jeff},
  month = sep,
  year = {2018},
  file = {/home/asher/Zotero/storage/YM5F2RFC/Sweetnam et al. - 2018 - Prototyping a precision oncology 3.0 rapid learnin.pdf}
}

@article{tetenov2012,
  title = {Statistical Treatment Choice Based on Asymmetric Minimax Regret Criteria},
  volume = {166},
  issn = {03044076},
  abstract = {This paper studies the problem of treatment choice between a status quo treatment with a known outcome distribution and an innovation whose outcomes are observed only in a representative finite sample. I evaluate statistical decision rules, which are functions that map sample outcomes into the planner's treatment choice for the population, based on regret, which is the expected welfare loss due to assigning inferior treatments. I extend previous work that applied the minimax regret criterion to treatment choice problems by considering decision criteria that asymmetrically treat Type I regret (due to mistakenly choosing an inferior new treatment) and Type II regret (due to mistakenly rejecting a superior innovation). I derive exact finite sample solutions to these problems for experiments with normal, Bernoulli and bounded distributions of individual outcomes. In conclusion, I discuss approaches to the problem for other classes of distributions. Along the way, the paper compares asymmetric minimax regret criteria with statistical decision rules based on classical hypothesis tests.},
  language = {en},
  number = {1},
  journal = {Journal of Econometrics},
  doi = {10.1016/j.jeconom.2011.06.013},
  author = {Tetenov, Aleksey},
  month = jan,
  year = {2012},
  pages = {157-165},
  file = {/home/asher/Zotero/storage/ZDDRP4UC/Tetenov - 2012 - Statistical treatment choice based on asymmetric m.pdf}
}

@article{thall2019,
  title = {Bayesian Treatment Comparison Using Parametric Mixture Priors Computed from Elicited Histograms},
  volume = {28},
  issn = {0962-2802},
  abstract = {A Bayesian methodology is proposed for constructing a parametric prior on two
treatment effect parameters, based on graphical information elicited from a
group of expert physicians. The motivating application is a 70-patient
randomized trial to compare two treatments for idiopathic nephrotic syndrome in
children. The methodology relies on histograms of the treatment parameters
constructed manually by each physician, applying the method of Johnson et~al.
(2010). For each physician, a marginal prior for each treatment parameter
characterized by location and precision hyperparameters is fit to the elicited
histogram. A bivariate prior is obtained by averaging the marginals over a
latent physician effect distribution. An overall prior is constructed as a
mixture of the individual physicians' priors. A simulation study evaluating
several versions of the methodology is presented. A framework is given for
performing a sensitivity analysis of posterior inferences to prior location and
precision and illustrated based on the idiopathic nephrotic syndrome trial.},
  number = {2},
  journal = {Statistical Methods in Medical Research},
  doi = {10.1177/0962280217726803},
  author = {Thall, Peter F and Ursino, Moreno and Baudouin, V{\'e}ronique and Alberti, Corinne and Zohar, Sarah},
  month = feb,
  year = {2019},
  pages = {404-418},
  file = {/home/asher/Zotero/storage/Z5YMA5IS/Thall et al. - 2019 - Bayesian treatment comparison using parametric mix.pdf},
  pmid = {28870123},
  pmcid = {PMC5658278}
}

@article{thorlund2019,
  title = {Highly {{Efficient Clinical Trials Simulator}} ({{HECT}}): {{Software}} Application for Planning and Simulating Platform Adaptive Trials},
  volume = {3},
  issn = {2572-4754},
  shorttitle = {Highly {{Efficient Clinical Trials Simulator}} ({{HECT}})},
  abstract = {Background:
              Adaptive designs and platform designs are among two common clinical trial innovations that are increasingly being used to manage medical intervention portfolios and attain faster regulatory approvals. Planning of adaptive and platform trials necessitate simulations to understand how a set of adaptation rules will likely affect the properties of the trial. Clinical trial simulations, however, remain a black box to many clinical trials researchers who are not statisticians.
            
            
              Results:
              In this article we introduce a simple intuitive open-source browser-based clinical trial simulator for planning adaptive and platform trials. The software application is implemented in
              RShiny
              and features a graphical user interface that allows the user to set key clinical trial parameters and explore multiple scenarios such as varying treatment effects, control response and adherence, as well as number of interim looks and adaptation rules. The software provides simulation options for a number of designs such as dropping treatment arms for futility, adding a new treatment arm (i.e., platform design), and stopping a trial early based on superiority. All available adaptations are based on underlying Bayesian probabilities. The software comes with a number of graphical outputs to examine properties of individual simulated trials. The main output is a comparison of trial design performance across several simulations, graphically summarizing type I error (false positive risk), power, and expected cost/time to completion of the considered designs.
            
            
              Conclusion:
              We have developed and validated an intuitive highly efficient clinical trial simulator for planning of clinical trials. The software is open-source and caters to clinical trial investigators who do not have the statistical capacity for trial simulations available in their team. The software can be accessed via any web browser via the following link:
              https://mtek.shinyapps.io/hect/},
  language = {en},
  journal = {Gates Open Research},
  doi = {10.12688/gatesopenres.12912.2},
  author = {Thorlund, Kristian and Golchi, Shirin and Haggstrom, Jonas and Mills, Edward},
  month = mar,
  year = {2019},
  pages = {780},
  file = {/home/asher/Zotero/storage/LM775NG3/Thorlund et al. - 2019 - Highly Efficient Clinical Trials Simulator (HECT).pdf;/home/asher/Zotero/storage/98SU9VVT/v2.html}
}

@article{thorlund2013,
  title = {Modelling Heterogeneity Variances in Multiple Treatment Comparison Meta-Analysis \textendash{} {{Are}} Informative Priors the Better Solution?},
  volume = {13},
  issn = {1471-2288},
  abstract = {Multiple treatment comparison (MTC) meta-analyses are commonly modeled in a Bayesian framework, and weakly informative priors are typically preferred to mirror familiar data driven frequentist approaches. Random-effects MTCs have commonly modeled heterogeneity under the assumption that the between-trial variance for all involved treatment comparisons are equal (i.e., the `common variance' assumption). This approach `borrows strength' for heterogeneity estimation across treatment comparisons, and thus, ads valuable precision when data is sparse. The homogeneous variance assumption, however, is unrealistic and can severely bias variance estimates. Consequently 95\% credible intervals may not retain nominal coverage, and treatment rank probabilities may become distorted. Relaxing the homogeneous variance assumption may be equally problematic due to reduced precision. To regain good precision, moderately informative variance priors or additional mathematical assumptions may be necessary.},
  number = {1},
  journal = {BMC Medical Research Methodology},
  doi = {10.1186/1471-2288-13-2},
  author = {Thorlund, Kristian and Thabane, Lehana and Mills, Edward J.},
  month = jan,
  year = {2013},
  pages = {2},
  file = {/home/asher/Zotero/storage/43UTBQYY/Thorlund et al. - 2013 - Modelling heterogeneity variances in multiple trea.pdf;/home/asher/Zotero/storage/RXE3CF2J/1471-2288-13-2.html}
}

@book{velterop2010,
  title = {Nanopublications: {{The}} Future of Coping with Information Overload. {{LOGOS J}}. World {{B}}. {{Community}}},
  author = {Velterop, J.},
  year = {2010}
}

@article{villar2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.08025},
  title = {Multi-Armed {{Bandit Models}} for the {{Optimal Design}} of {{Clinical Trials}}: {{Benefits}} and {{Challenges}}},
  volume = {30},
  issn = {0883-4237},
  shorttitle = {Multi-Armed {{Bandit Models}} for the {{Optimal Design}} of {{Clinical Trials}}},
  abstract = {Multi-armed bandit problems (MABPs) are a special type of optimal control problem well suited to model resource allocation under uncertainty in a wide variety of contexts. Since the first publication of the optimal solution of the classic MABP by a dynamic index rule, the bandit literature quickly diversified and emerged as an active research topic. Across this literature, the use of bandit models to optimally design clinical trials became a typical motivating application, yet little of the resulting theory has ever been used in the actual design and analysis of clinical trials. To this end, we review two MABP decision-theoretic approaches to the optimal allocation of treatments in a clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the finite-horizon variant. These models possess distinct theoretical properties and lead to separate allocation rules in a clinical trial design context. We evaluate their performance compared to other allocation rules, including fixed randomization. Our results indicate that bandit approaches offer significant advantages, in terms of assigning more patients to better treatments, and severe limitations, in terms of their resulting statistical power. We propose a novel bandit-based patient allocation rule that overcomes the issue of low power, thus removing a potential barrier for their use in practice.},
  language = {en},
  number = {2},
  journal = {Statistical Science},
  doi = {10.1214/14-STS504},
  author = {Villar, Sof{\'i}a S. and Bowden, Jack and Wason, James},
  month = may,
  year = {2015},
  keywords = {Statistics - Methodology},
  pages = {199-215},
  file = {/home/asher/Zotero/storage/B2EBTCD6/Villar et al. - 2015 - Multi-armed Bandit Models for the Optimal Design o.pdf}
}

@article{wager2018,
  title = {Estimation and {{Inference}} of {{Heterogeneous Treatment Effects}} Using {{Random Forests}}},
  volume = {113},
  issn = {0162-1459, 1537-274X},
  abstract = {Many scientific and engineering challenges\textemdash{}ranging from personalized medicine to customized marketing recommendations\textemdash{}require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
  language = {en},
  number = {523},
  journal = {Journal of the American Statistical Association},
  doi = {10.1080/01621459.2017.1319839},
  author = {Wager, Stefan and Athey, Susan},
  month = jul,
  year = {2018},
  pages = {1228-1242},
  file = {/home/asher/Zotero/storage/G2TYLEKG/Wager and Athey - 2018 - Estimation and Inference of Heterogeneous Treatmen.pdf}
}

@article{wang2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.03058},
  primaryClass = {cs},
  title = {Online {{Interactive Collaborative Filtering Using Multi}}-{{Armed Bandit}} with {{Dependent Arms}}},
  abstract = {Online interactive recommender systems strive to promptly suggest to consumers appropriate items (e.g., movies, news articles) according to the current context including both the consumer and item content information. However, such context information is o en unavailable in practice for the recommendation, where only the users' interaction data on items can be utilized. Moreover, the lack of interaction records, especially for new users and items, worsens the performance of recommendation further. To address these issues, collaborative ltering (CF), one of the recommendation techniques relying on the interaction data only, as well as the online multi-armed bandit mechanisms, capable of achieving the balance between exploitation and exploration, are adopted in the online interactive recommendation se ings, by assuming independent items (i.e., arms). Nonetheless, the assumption rarely holds in reality, since the real-world items tend to be correlated with each other (e.g., two articles with similar topics).},
  language = {en},
  journal = {arXiv:1708.03058 [cs]},
  author = {Wang, Qing and Zeng, Chunqiu and Zhou, Wubai and Li, Tao and Shwartz, Larisa and Grabarnik, Genady Ya},
  month = aug,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Information Retrieval,H.3.3,I.2.6},
  file = {/home/asher/Zotero/storage/WTLCSTKP/Wang et al. - 2017 - Online Interactive Collaborative Filtering Using M.pdf}
}

@article{xu,
  title = {A {{Bayesian Nonparametic Approach}} for {{Estimating Individualized Treatment}}-{{Response Curves}}},
  abstract = {We study the problem of estimating the continuous response over time of interventions from observational time series\textemdash{}a retrospective dataset where the policy by which the data are generated are unknown to the learner. We are motivated by applications where response varies by individuals and therefore, estimating responses at the individual-level are valuable for personalizing decisionmaking. We refer to this as the problem of estimating individualized treatment response (ITR) curves. In statistics, G-computation formula (Robins, 1986) has been commonly used for estimating treatment responses from observational data containing sequential treatment assignments. However, past studies have focused predominantly on obtaining point-in-time estimates at the population level. We leverage G-computation formula and develop a novel method based on Bayesian nonparametrics (BNP) that can flexibly model functional data and provide posterior inference over the treatment response curves both at the individual and population level. On a challenging dataset containing time series from patients admitted to a hospital, we estimate treatment responses for treatments used in managing kidney function and show that the resulting fits are more accurate than alternative approaches. Accurate methods for obtaining ITRs from observational data can dramatically accelerate the pace at which personalized treatment plans become possible.},
  language = {en},
  author = {Xu, Yanbo and Xu, Yanxun and Saria, Suchi},
  pages = {18},
  file = {/home/asher/Zotero/storage/K2XBM3V5/Xu et al. - A Bayesian Nonparametic Approach for Estimating In.pdf}
}

@article{youngs2014,
  title = {Negative Example Selection for Protein Function Prediction: The {{NoGO}} Database},
  volume = {10},
  journal = {PLoS Comput Biol},
  doi = {10.1371/journal.pcbi.1003644},
  author = {Youngs, N. and {Penfold-Brown}, D. and Bonneau, R. and Shasha, D.},
  year = {2014},
  file = {/home/asher/Zotero/storage/7BF4Y2U2/Youngs et al. - 2014 - Negative example selection for protein function pr.pdf}
}

@article{zaharchuk2018,
  title = {Deep {{Learning}} in {{Neuroradiology}}},
  volume = {39},
  issn = {0195-6108, 1936-959X},
  abstract = {SUMMARY: Deep learning is a form of machine learning using a convolutional neural network architecture that shows tremendous promise for imaging applications. It is increasingly being adapted from its original demonstration in computer vision applications to medical imaging. Because of the high volume and wealth of multimodal imaging information acquired in typical studies, neuroradiology is poised to be an early adopter of deep learning. Compelling deep learning research applications have been demonstrated, and their use is likely to grow rapidly. This review article describes the reasons, outlines the basic methods used to train and test deep learning models, and presents a brief overview of current and potential clinical applications with an emphasis on how they are likely to change future neuroradiology practice. Facility with these methods among neuroimaging researchers and clinicians will be important to channel and harness the vast potential of this new method.},
  language = {en},
  number = {10},
  journal = {American Journal of Neuroradiology},
  doi = {10.3174/ajnr.A5543},
  author = {Zaharchuk, G. and Gong, E. and Wintermark, M. and Rubin, D. and Langlotz, C.P.},
  month = oct,
  year = {2018},
  pages = {1776-1784},
  file = {/home/asher/Zotero/storage/27HA2TJG/Zaharchuk et al. - 2018 - Deep Learning in Neuroradiology.pdf}
}

@article{zhang2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.07082},
  primaryClass = {math, stat},
  title = {Distributions and {{Statistical Power}} of {{Optimal Signal}}-{{Detection Methods In Finite Cases}}},
  abstract = {In big data analysis for detecting rare and weak signals among \$n\$ features, some grouping-test methods such as Higher Criticism test (HC), Berk-Jones test (B-J), and \$\textbackslash{}phi\$-divergence test share the similar asymptotical optimality when \$n \textbackslash{}rightarrow \textbackslash{}infty\$. However, in practical data analysis \$n\$ is frequently small and moderately large at most. In order to properly apply these optimal tests and wisely choose them for practical studies, it is important to know how to get the p-values and statistical power of them. To address this problem in an even broader context, this paper provides analytical solutions for a general family of goodness-of-fit (GOF) tests, which covers these optimal tests. For any given i.i.d. and continuous distributions of the input test statistics of the \$n\$ features, both p-value and statistical power of such a GOF test can be calculated. By calculation we compared the finite-sample performances of asymptotically optimal tests under the normal mixture alternative. Results show that HC is the best choice when signals are rare, while B-J is more robust over various signal patterns. In the application to a real genome-wide association study, results illustrate that the p-value calculation works well, and the optimal tests have potentials for detecting novel disease genes with weak genetic effects. The calculations have been implemented in an R package SetTest and published on the CRAN.},
  journal = {arXiv:1702.07082 [math, stat]},
  author = {Zhang, Hong and Jin, Jiashun and Wu, Zheyang},
  month = feb,
  year = {2017},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/asher/Zotero/storage/6NLFBWCH/Zhang et al. - 2017 - Distributions and Statistical Power of Optimal Sig.pdf;/home/asher/Zotero/storage/7DKN2GD7/1702.html}
}

@article{zou2015,
  title = {On {{Model Selections}} for {{Repeated Measurement Data}} in {{Clinical Studies}}},
  volume = {34},
  issn = {0277-6715},
  abstract = {Repeated measurement designs have been widely used in various randomized controlled trials for evaluating long term intervention efficacies. For some clinical trials, the primary research question is to compare two treatments at a fixed time, using a t-test. Though simple, robust, and convenient, this type of analysis fails to utilize a large amount of collected information. Alternatively, the mixed effects model is commonly used for repeated measurement data. It models all available data jointly and allows explicit assessment of the overall treatment effects across the entire time spectrum. In this paper, we propose an analytic strategy for longitudinal clinical trial data where the mixed effects model is coupled with a model selection scheme. The proposed test statistics not only make full use of all available data but also utilize the information from the optimal model deemed for the data. The performance of the proposed method under various setups, including different data missing mechanisms, is evaluated via extensive Monte Carlo simulations. Our numerical results demonstrate that the proposed analytic procedure is more powerful than the t-test when the primary interest is to test for the treatment effect at the last time point. Simulations also reveal that the proposed method outperforms the usual mixed effects model for testing the overall treatment effects across time. In addition, the proposed framework is more robust and flexible in dealing with missing data compared to several competing methods. The utility of the proposed method is demonstrated by analyzing a clinical trial on the cognitive effect of testosterone in geriatric men with low baseline testosterone levels.},
  number = {10},
  journal = {Statistics in medicine},
  doi = {10.1002/sim.6414},
  author = {Zou, Baiming and Jin, Bo and Koch, Gary G. and Zhou, Haibo and Borst, Stephen E. and Menon, Sandeep and Shuster, Jonathan J.},
  month = may,
  year = {2015},
  pages = {1621-1633},
  file = {/home/asher/Zotero/storage/5RJZYSMH/Zou et al. - 2015 - On Model Selections for Repeated Measurement Data .pdf},
  pmid = {25645442},
  pmcid = {PMC4390448}
}

@article{zucker2010,
  title = {Individual ({{N}}-of-1) Trials Can Be Combined to Give Population Comparative Treatment Effect Estimates: Methodologic Considerations},
  volume = {63},
  issn = {0895-4356},
  shorttitle = {Individual ({{N}}-of-1) Trials Can Be Combined to Give Population Comparative Treatment Effect Estimates},
  abstract = {Objective
To compare different statistical models for combining N-of-1 trials to estimate a population treatment effect.
Study Design and Setting
Data from a published series of N-of-1 trials comparing amitriptyline (AMT) therapy and combination treatment (AMT+fluoxetine [FL]) were analyzed to compare summary and individual participant data meta-analysis; repeated-measure models; Bayesian hierarchical models; and single-period, single-pair, and averaged outcome crossover models.
Results
The best-fitting model included a random intercept (response on AMT) and fixed treatment effect (added FL). Results supported a common, uncorrelated within-patient covariance structure that is equal between treatments and across patients. Assuming unequal within-patient variances, a random-effect model was favored. Bayesian hierarchical models improved precision and were highly sensitive to within-patient variance priors.
Conclusion
Optimal models for combining N-of-1 trials need to consider goals, data sources, and relative within- and between-patient variances. Without sufficient patients, between-patient variation will be hard to explain with covariates. N-of-1 data with few observations per patients may not support models with heterogeneous within-patient variation. With common variances, models appear robust. Bayesian models may improve parameter estimation but are sensitive to prior assumptions about variance components. With limited resources, improving within-patient precision must be balanced by increased participants to explain population variation.},
  number = {12},
  journal = {Journal of Clinical Epidemiology},
  doi = {10.1016/j.jclinepi.2010.04.020},
  author = {Zucker, Deborah R. and Ruthazer, Robin and Schmid, Christopher H.},
  month = dec,
  year = {2010},
  keywords = {-of-1 trials,Comparative effectiveness,Comparisons,Meta-analysis,Methodology,Population estimate},
  pages = {1312-1323},
  file = {/home/asher/Zotero/storage/AV768C6R/Zucker et al. - 2010 - Individual (N-of-1) trials can be combined to give.pdf;/home/asher/Zotero/storage/KVR758IR/S0895435610001940.html}
}

@misc{zotero-440,
  title = {Targeted {{Therapy Database User Guide}}},
  howpublished = {http://www.mmmp.org/mmmpFile/TTDguide\_1.pdf},
  file = {/home/asher/Zotero/storage/RJTTSETI/TTDguide_1.pdf}
}

@article{foti2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1211.4798},
  primaryClass = {cs, stat},
  title = {A Survey of Non-Exchangeable Priors for {{Bayesian}} Nonparametric Models},
  abstract = {Dependent nonparametric processes extend distributions over measures, such as the Dirichlet process and the beta process, to give distributions over collections of measures, typically indexed by values in some covariate space. Such models are appropriate priors when exchangeability assumptions do not hold, and instead we want our model to vary fluidly with some set of covariates. Since the concept of dependent nonparametric processes was formalized by MacEachern [1], there have been a number of models proposed and used in the statistics and machine learning literatures. Many of these models exhibit underlying similarities, an understanding of which, we hope, will help in selecting an appropriate prior, developing new models, and leveraging inference techniques.},
  language = {en},
  journal = {arXiv:1211.4798 [cs, stat]},
  author = {Foti, Nicholas J. and Williamson, Sinead},
  month = nov,
  year = {2012},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/TRHHKD8N/Foti and Williamson - 2012 - A survey of non-exchangeable priors for Bayesian n.pdf}
}

@article{rue2009,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  volume = {71},
  copyright = {\textcopyright{} 2009 Royal Statistical Society},
  issn = {1467-9868},
  abstract = {Summary. Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
  language = {en},
  number = {2},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  doi = {10.1111/j.1467-9868.2008.00700.x},
  author = {Rue, H{\aa}vard and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  keywords = {Approximate Bayesian inference,Gaussian Markov random fields,Generalized additive mixed models,Laplace approximation,Parallel computing,Sparse matrices,Structured additive regression models},
  pages = {319-392},
  file = {/home/asher/Zotero/storage/QZC998RH/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf;/home/asher/Zotero/storage/4KNXLPKH/j.1467-9868.2008.00700.html}
}

@article{gelman2006,
  title = {Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by {{Browne}} and {{Draper}})},
  volume = {1},
  issn = {1936-0975, 1931-6690},
  abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-ttt family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of "noninformative" prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-ttt family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-ttt family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
  language = {EN},
  number = {3},
  journal = {Bayesian Analysis},
  doi = {10.1214/06-BA117A},
  author = {Gelman, Andrew},
  month = sep,
  year = {2006},
  keywords = {Bayesian inference,conditional conjugacy,folded-noncentral-$t$ distribution,half-$t$ distribution,hierarchical model,multilevel model,noninformative prior distribution,weakly informative prior distribution},
  pages = {515-534},
  file = {/home/asher/Zotero/storage/GT4WABDJ/Gelman - 2006 - Prior distributions for variance parameters in hie.pdf;/home/asher/Zotero/storage/VKJPI37W/1340371048.html}
}

@article{eisenhauer2009,
  title = {New Response Evaluation Criteria in Solid Tumours: {{Revised RECIST}} Guideline (Version 1.1)},
  volume = {45},
  issn = {09598049},
  shorttitle = {New Response Evaluation Criteria in Solid Tumours},
  abstract = {Background: Assessment of the change in tumour burden is an important feature of the clinical evaluation of cancer therapeutics: both tumour shrinkage (objective response) and disease progression are useful endpoints in clinical trials. Since RECIST was published in 2000, many investigators, cooperative groups, industry and government authorities have adopted these criteria in the assessment of treatment outcomes. However, a number of questions and issues have arisen which have led to the development of a revised RECIST guideline (version 1.1). Evidence for changes, summarised in separate papers in this special issue, has come from assessment of a large data warehouse ({$>$}6500 patients), simulation studies and literature reviews.},
  language = {en},
  number = {2},
  journal = {European Journal of Cancer},
  doi = {10.1016/j.ejca.2008.10.026},
  author = {Eisenhauer, E.A. and Therasse, P. and Bogaerts, J. and Schwartz, L.H. and Sargent, D. and Ford, R. and Dancey, J. and Arbuck, S. and Gwyther, S. and Mooney, M. and Rubinstein, L. and Shankar, L. and Dodd, L. and Kaplan, R. and Lacombe, D. and Verweij, J.},
  month = jan,
  year = {2009},
  pages = {228-247},
  file = {/home/asher/Zotero/storage/AQCXVHF2/Eisenhauer et al. - 2009 - New response evaluation criteria in solid tumours.pdf}
}

@article{yu2019,
  title = {The Global Pipeline of Cell Therapies for Cancer},
  copyright = {2019 Nature},
  abstract = {Discover the world's best science and medicine  | Nature.com},
  language = {EN},
  journal = {Nature Reviews Drug Discovery},
  doi = {10.1038/d41573-019-00090-z},
  author = {Yu, Jia Xin and {Hubbard-Lucey}, Vanessa M. and Tang, Jun},
  month = may,
  year = {2019},
  file = {/home/asher/Zotero/storage/QJFDFWBT/d41573-019-00090-z.html}
}

@article{henderson2000,
  title = {Joint Modelling of Longitudinal Measurements and Event Time Data},
  volume = {1},
  issn = {14654644, 14684357},
  abstract = {This paper formulates a class of models for the joint behaviour of a sequence of longitudinal measurements and an associated sequence of event times, including single-event survival data. This class includes and extends a number of specific models which have been proposed recently, and, in the absence of association, reduces to separate models for the measurements and events based, respectively, on a normal linear model with correlated errors and a semi-parametric proportional hazards or intensity model with frailty.},
  language = {en},
  number = {4},
  journal = {Biostatistics},
  doi = {10.1093/biostatistics/1.4.465},
  author = {Henderson, R.},
  month = dec,
  year = {2000},
  pages = {465-480},
  file = {/home/asher/Zotero/storage/DL9PMXSQ/Henderson - 2000 - Joint modelling of longitudinal measurements and e.pdf}
}

@article{recht2003,
  title = {Quantitative Measurement of Quality Outcome in Malignant Glioma Patients Using an Independent Living Score ({{ILS}}). {{Assessment}} of a Retrospective Cohort},
  volume = {61},
  issn = {0167-594X},
  abstract = {PURPOSE: Although a number of tools have been developed to measure 'quality of life' in patients with malignant glioma, there remains no completely satisfactory technique that incorporates a quality of life measure into survival analysis. We propose that a patient's ability to maintain independent activity offers a way to accomplish this goal.
PATIENTS AND METHODS: An independent living score (ILS) is generated by awarding points on a monthly basis based on Karnofsky score and weighing the score based on the particular month of the clinical course. The ILS has a large range for any given survival, and can discriminate important treatment effects to which standard survival analyses are completely insensitive. Using this score and several variations, we were able to retrospectively analyze a patient cohort to assess what correlated with ILS.
RESULTS: We found a strong correlation with survival of all the measures tested. Interestingly, we found that patients for whom a total resection was performed and those who were most intensively treated had significantly higher ILS values, suggesting that not only did more aggressive treatment improve survival but that it did not simply increase survival at the expense of the time a patient remained independent.
CONCLUSION: Since the general course for patients with malignant glioma is one of increasing disability and loss of independence, we feel that these measures can serve as a way to distinguish between those therapies that increase survival at the expense of quality of life versus those that do not. Consideration should be given to incorporating these measures into prospective trials.},
  language = {eng},
  number = {2},
  journal = {Journal of Neuro-Oncology},
  author = {Recht, L. and Glantz, M. and Chamberlain, M. and Hsieh, C. C.},
  month = jan,
  year = {2003},
  keywords = {Aged,Humans,Male,Activities of Daily Living,Adult,Brain Neoplasms,Cohort Studies,Female,Glioblastoma,Middle Aged,Prognosis,Quality of Life,Quality-Adjusted Life Years,Retrospective Studies,Survival Rate},
  pages = {127-136},
  file = {/home/asher/Zotero/storage/UYIC363G/Recht et al. - 2003 - Quantitative measurement of quality outcome in mal.pdf},
  pmid = {12622451}
}

@book{montfort2010,
  address = {{Heidelberg ; New York}},
  title = {Longitudinal Research with Latent Variables},
  isbn = {978-3-642-11759-6},
  lccn = {H62 .L65 2010},
  language = {en},
  publisher = {{Springer Verlag}},
  editor = {van Montfort, Kees},
  year = {2010},
  keywords = {Data-analyse,Latente variabelen,Longitudinaal onderzoek,Research,Social sciences},
  file = {/home/asher/Zotero/storage/VJZRTDWH/Montfort - 2010 - Longitudinal research with latent variables.pdf},
  note = {OCLC: ocn502033971}
}

@article{schmid2009,
  title = {A {{Bayesian}} Hierarchical Model for the Analysis of a Longitudinal Dynamic Contrast-Enhanced {{MRI}} Oncology Study},
  volume = {61},
  issn = {07403194, 15222594},
  language = {en},
  number = {1},
  journal = {Magnetic Resonance in Medicine},
  doi = {10.1002/mrm.21807},
  author = {Schmid, Volker J. and Whitcher, Brandon and Padhani, Anwar R. and Taylor, N. Jane and Yang, Guang-Zhong},
  month = jan,
  year = {2009},
  pages = {163-174},
  file = {/home/asher/Zotero/storage/U7W26AUP/Schmid et al. - 2009 - A Bayesian hierarchical model for the analysis of .pdf}
}

@article{staedtke2016,
  title = {Actionable {{Molecular Biomarkers}} in {{Primary Brain Tumors}}},
  volume = {2},
  issn = {24058033},
  abstract = {Recent genome-wide studies of malignancies of the central nervous system (CNS) have revolutionized our understanding of the biology of these tumors. This newly gained knowledge provides a wealth of opportunity for biomarker driven clinical research. To date, however, only few of the available molecular markers truly influence clinical decision-making and treatment. The most widely validated markers in neuro-oncology presently are: 1) MGMT promoter methylation as a prognostic and predictive marker in glioblastoma, 2) co-deletion of 1p and 19q differentiating oligodendrogliomas from astrocytomas, 3) IDH1/2 mutations, and 4) select pathway-associated mutations. This article focuses on currently impactful biomarkers in adult and pediatric brain cancers and it provides a perspective on the direction of research in this field.},
  language = {en},
  number = {7},
  journal = {Trends in Cancer},
  doi = {10.1016/j.trecan.2016.06.003},
  author = {Staedtke, Verena and a Dzaye, Omar Dildar and Holdhoff, Matthias},
  month = jul,
  year = {2016},
  pages = {338-349},
  file = {/home/asher/Zotero/storage/V6A2KJBA/Staedtke et al. - 2016 - Actionable Molecular Biomarkers in Primary Brain T.pdf}
}

@article{gabry2019,
  title = {Visualization in {{Bayesian}} Workflow},
  volume = {182},
  issn = {1467-985X},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high dimensional models that are used by applied researchers.},
  language = {en},
  number = {2},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  doi = {10.1111/rssa.12378},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  keywords = {Bayesian data analysis,Statistical graphics,Statistical workflow},
  pages = {389-402},
  file = {/home/asher/Zotero/storage/BFLPFSAG/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf;/home/asher/Zotero/storage/2BTS4CTF/rssa.html}
}

@article{herman2011,
  title = {A {{Quantitative Theory}} of {{Solid Tumor Growth}}, {{Metabolic Rate}} and {{Vascularization}}},
  volume = {6},
  issn = {1932-6203},
  abstract = {The relationships between cellular, structural and dynamical properties of tumors have traditionally been studied separately. Here, we construct a quantitative, predictive theory of solid tumor growth, metabolic rate, vascularization and necrosis that integrates the relationships between these properties. To accomplish this, we develop a comprehensive theory that describes the interface and integration of the tumor vascular network and resource supply with the cardiovascular system of the host. Our theory enables a quantitative understanding of how cells, tissues, and vascular networks act together across multiple scales by building on recent theoretical advances in modeling both healthy vasculature and the detailed processes of angiogenesis and tumor growth. The theory explicitly relates tumor vascularization and growth to metabolic rate, and yields extensive predictions for tumor properties, including growth rates, metabolic rates, degree of necrosis, blood flow rates and vessel sizes. Besides these quantitative predictions, we explain how growth rates depend on capillary density and metabolic rate, and why similar tumors grow slower and occur less frequently in larger animals, shedding light on Peto's paradox. Various implications for potential therapeutic strategies and further research are discussed.},
  language = {en},
  number = {9},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0022973},
  author = {Herman, Alexander B. and Savage, Van M. and West, Geoffrey B.},
  month = sep,
  year = {2011},
  keywords = {Blood flow,Capillaries,Cell metabolism,Drug metabolism,Flow rate,Oxygen,Oxygen metabolism,Tumor angiogenesis},
  pages = {e22973},
  file = {/home/asher/Zotero/storage/IB2QIPYY/Herman et al. - 2011 - A Quantitative Theory of Solid Tumor Growth, Metab.pdf;/home/asher/Zotero/storage/ZCKH3F2Y/article.html}
}

@article{jung2001,
  title = {Graphical {{Search}} for {{Two}}-{{Stage Designs}} for {{Phase II Clinical Trials}}},
  volume = {22},
  issn = {01972456},
  language = {en},
  number = {4},
  journal = {Controlled Clinical Trials},
  doi = {10.1016/S0197-2456(01)00142-8},
  author = {Jung, Sin-Ho and Carey, Mark and Kim, Kyung Mann},
  month = aug,
  year = {2001},
  pages = {367-372},
  file = {/home/asher/Zotero/storage/DNXLAEEU/Jung et al. - 2001 - Graphical Search for Two-Stage Designs for Phase I.pdf}
}

@article{vanderbeek2019,
  title = {To Randomize, or Not to Randomize, That Is the Question: Using Data from Prior Clinical Trials to Guide Future Designs},
  issn = {1523-5866},
  shorttitle = {To Randomize, or Not to Randomize, That Is the Question},
  abstract = {BACKGROUND: Understanding the value of randomization is critical in designing clinical trials. Here, we introduce a simple and interpretable quantitative method to compare randomized designs versus single arm designs using indication-specific parameters derived from the literature. We demonstrate the approach through application to phase II trials in newly diagnosed glioblastoma (ndGBM).
METHODS: We abstracted data from prior ndGBM trials and derived relevant parameters to compare phase II RCT and single arm designs within a quantitative framework. Parameters included in our model were (i) the variability of the primary endpoint distributions across studies, (ii) potential for incorrectly specifying the single arm trial's benchmark, and (iii) the hypothesized effect size. Strengths and weaknesses of RCT and single arm designs were quantified by various metrics, including power and false positive errors rates.
RESULTS: We applied our method to show that RCTs should be preferred to single arm trials for evaluating overall survival in ndGBM patients based on parameters estimated from prior trials. More generally, for a given effect size, the utility of randomization compared to single arm designs is highly dependent on (i) inter-study variability of the outcome distributions and (ii) on potential errors in selecting standard of care efficacy estimates for single arm studies.
CONCLUSIONS: A quantitative framework using historical data is useful in understanding the utility of randomization in designing prospective trials. For typical phase II ndGBM trials using OS as the primary endpoint, randomization should be preferred over single arm designs.},
  language = {eng},
  journal = {Neuro-Oncology},
  doi = {10.1093/neuonc/noz097},
  author = {Vanderbeek, Alyssa M. and Ventz, Steffen and Rahman, Rifaquat and Fell, Geoffrey and Cloughesy, Timothy F. and Wen, Patrick Y. and Trippa, Lorenzo and Alexander, Brian M.},
  month = jun,
  year = {2019},
  file = {/home/asher/Zotero/storage/5W2MMHMM/Vanderbeek et al. - 2019 - To randomize, or not to randomize, that is the que.pdf},
  pmid = {31155679}
}

@article{alexander2018,
  title = {Adaptive {{Global Innovative Learning Environment}} for {{Glioblastoma}}: {{GBM AGILE}}},
  volume = {24},
  issn = {1078-0432},
  shorttitle = {Adaptive {{Global Innovative Learning Environment}} for {{Glioblastoma}}},
  abstract = {Glioblastoma (GBM) is a deadly disease with few effective therapies. Although much has been learned about the molecular characteristics of the disease, this knowledge has not been translated into clinical improvements for patients. At the same time, many new therapies are being developed. Many of these therapies have potential biomarkers to identify responders. The result is an enormous amount of testable clinical questions that must be answered efficiently. The GBM Adaptive Global Innovative Learning Environment (GBM AGILE) is a novel, multi-arm, platform trial designed to address these challenges. It is the result of the collective work of over 130 oncologists, statisticians, pathologists, neurosurgeons, imagers, and translational and basic scientists from around the world. GBM AGILE is composed of two stages. The first stage is a Bayesian adaptively randomized screening stage to identify effective therapies based on impact on overall survival compared with a common control. This stage also finds the population in which the therapy shows the most promise based on clinical indication and biomarker status. Highly effective therapies transition in an inferentially seamless manner in the identified population to a second confirmatory stage. The second stage uses fixed randomization to confirm the findings from the first stage to support registration. Therapeutic arms with biomarkers may be added to the trial over time, while others complete testing. The design of GBM AGILE enables rapid clinical testing of new therapies and biomarkers to speed highly effective therapies to clinical practice. Clin Cancer Res; 24(4); 737-43. \textcopyright{}2017 AACR.},
  language = {eng},
  number = {4},
  journal = {Clinical Cancer Research: An Official Journal of the American Association for Cancer Research},
  doi = {10.1158/1078-0432.CCR-17-0764},
  author = {Alexander, Brian M. and Ba, Sujuan and Berger, Mitchel S. and Berry, Donald A. and Cavenee, Webster K. and Chang, Susan M. and Cloughesy, Timothy F. and Jiang, Tao and Khasraw, Mustafa and Li, Wenbin and Mittman, Robert and Poste, George H. and Wen, Patrick Y. and Yung, W. K. Alfred and Barker, Anna D. and {GBM AGILE Network}},
  month = feb,
  year = {2018},
  pages = {737-743},
  file = {/home/asher/Zotero/storage/5YN6KVNK/Alexander et al. - 2018 - Adaptive Global Innovative Learning Environment fo.pdf},
  pmid = {28814435}
}

@article{vanemden2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.01926},
  primaryClass = {cs, math, stat},
  title = {Contextual: {{Evaluating Contextual Multi}}-{{Armed Bandit Problems}} in {{R}}},
  shorttitle = {Contextual},
  abstract = {Over the past decade, contextual bandit algorithms have been gaining in popularity due to their effectiveness and flexibility in solving sequential decision problems\textemdash{}from online advertising and finance to clinical trial design and personalized medicine. At the same time, there are, as of yet, surprisingly few options that enable researchers and practitioners to simulate and compare the wealth of new and existing bandit algorithms in a standardized way. To help close this gap between analytical research and empirical evaluation the current paper introduces the objectoriented R package contextual: a user-friendly and, through its object-oriented design, easily extensible framework that facilitates parallelized comparison of contextual and context-free bandit policies through both simulation and offline analysis.},
  language = {en},
  journal = {arXiv:1811.01926 [cs, math, stat]},
  author = {{van Emden}, Robin and Kaptein, Maurits},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Machine Learning,I.2.6,Statistics - Machine Learning,93E35,F.2.0,K.4.4,Mathematics - Optimization and Control},
  file = {/home/asher/Zotero/storage/3Z6RN5TE/van Emden and Kaptein - 2018 - contextual Evaluating Contextual Multi-Armed Band.pdf}
}

@article{morioka2002,
  title = {Structured {{Reporting}} in {{Neuroradiology}}},
  volume = {980},
  issn = {1749-6632},
  abstract = {Abstract: We have developed a system to structure free-text neuroradiology reports using a natural language processing program and formatted the output into the digital image and communication in medicine (DICOM) standard for structured reporting (SR). DICOM SR formats the correspondence of pertinent diagnostic images to the radiologist's dictated report of clinical findings. In addition, DICOM SR allows the information to be organized into a tree structure. Individual nodes of the tree can contain individual items or lists. Structuring the content of free-text information allows the creation of hierarchies with defined relationships between the concepts contained within the report.},
  language = {en},
  number = {1},
  journal = {Annals of the New York Academy of Sciences},
  doi = {10.1111/j.1749-6632.2002.tb04902.x},
  author = {Morioka, Craig A. and Sinha, Usha and Taira, Ricky and El-Saden, Suzie and Duckwiler, Gary and Kangarloo, Hooshang},
  year = {2002},
  keywords = {DICOM,natural language processing,neuroradiology,radiology,structured report},
  pages = {259-266},
  file = {/home/asher/Zotero/storage/9RBSJLKF/Morioka et al. - 2002 - Structured Reporting in Neuroradiology.pdf;/home/asher/Zotero/storage/QZ7JUF83/j.1749-6632.2002.tb04902.html}
}

@article{taira2001a,
  title = {Automatic {{Structuring}} of {{Radiology Free}}-{{Text Reports}}},
  volume = {21},
  issn = {0271-5333},
  abstract = {A natural language processor was developed that automatically structures the important medical information (eg, the existence, properties, location, and diagnostic interpretation of findings) contained in a radiology free-text document as a formal information model that can be interpreted by a computer program. The input to the system is a free-text report from a radiologic study. The system requires no reporting style changes on the part of the radiologist. Statistical and machine learning methods are used extensively throughout the system. A graphical user interface has been developed that allows the creation of hand-tagged training examples. Various aspects of the difficult problem of implementing an automated structured reporting system have been addressed, and the relevant technology is progressing well. Extensible Markup Language is emerging as the preferred syntactic standard for representing and distributing these structured reports within a clinical environment. Early successes hold out hope that similar statistically based models of language will allow deep understanding of textual reports. The success of these statistical methods will depend on the availability of large numbers of high-quality training examples for each radiologic subdomain. The acceptability of automated structured reporting systems will ultimately depend on the results of comprehensive evaluations.},
  number = {1},
  journal = {RadioGraphics},
  doi = {10.1148/radiographics.21.1.g01ja18237},
  author = {Taira, Ricky K. and Soderland, Stephen G. and Jakobovits, Rex M.},
  month = jan,
  year = {2001},
  pages = {237-245},
  file = {/home/asher/Zotero/storage/2ZPR2Y7Q/Taira et al. - 2001 - Automatic Structuring of Radiology Free-Text Repor.pdf;/home/asher/Zotero/storage/MIQ25683/Taira et al. - 2001 - Automatic Structuring of Radiology Free-Text Repor.pdf;/home/asher/Zotero/storage/KNJTBFDK/radiographics.21.1.html}
}

@inproceedings{morton2016,
  title = {{{TITLE}}: {{A}} Taxonomy of Explanations in a General Practitioner Clinic for Patients with Persistent ``Medically Unexplained'' Physical Symptoms {{RUNNING HEAD}}: {{Explanations}} for Persistent Physical Symptoms},
  shorttitle = {{{TITLE}}},
  abstract = {LaKrista Morton , l.morton@abdn.ac.uk; Alison Elliott , a.m.elliott@abdn.ac.uk; Jennifer Cleland , jen.cleland@abdn.ac.uk; Vincent Deary , vincent.deary@northumbria.ac.uk; Christopher Burton *, c.burton@abdn.ac.uk a Institute of Applied Health Sciences, University of Aberdeen, Aberdeen AB25 2ZD, UK b Division of Medical and Dental Education, University of Aberdeen, Aberdeen AB25 2ZD, UK c Department of Psychology, Northumbria University, Newcastle-upon-Tyne NE1 8ST, UK},
  author = {Morton, L. H. Glyn and Elliott, A. and Cleland, Jennifer and Deary, Vincent and Burton, Christopher},
  year = {2016},
  keywords = {explanation,First Draft of a Report on the EDVAC,Fraction of variance unexplained,funding grant,GTPBP3 gene,interest,Mental disorders,message,Patient Health Questionnaire - 15 Item,PSMC2 wt Allele,Science,Sexually Transmitted Diseases,Taxonomy (general)},
  file = {/home/asher/Zotero/storage/54CEX923/Morton et al. - 2016 - TITLE A taxonomy of explanations in a general pra.pdf}
}

@article{fda/cder/permutt,
  title = {E9({{R1}}) {{Statistical Principles}} for {{Clinical Trials}}: {{Addendum}}: {{Estimands}} and {{Sensitivity Analysis}} in {{Clinical Trials}}},
  language = {en},
  author = {FDA/CDER/"Permutt, Thomas J"},
  pages = {31},
  file = {/home/asher/Zotero/storage/BL3G9YS3/FDACDERPermutt - E9(R1) Statistical Principles for Clinical Trials.pdf}
}

@article{zhang2017a,
  title = {Network-Based Machine Learning and Graph Theory Algorithms for Precision Oncology},
  volume = {1},
  copyright = {2017 The Author(s)},
  issn = {2397-768X},
  abstract = {Network-based analytics plays an increasingly important role in precision oncology. Growing evidence in recent studies suggests that cancer can be better understood through mutated or dysregulated pathways or networks rather than individual mutations and that the efficacy of repositioned drugs can be inferred from disease modules in molecular networks. This article reviews network-based machine learning and graph theory algorithms for integrative analysis of personal genomic data and biomedical knowledge bases to identify tumor-specific molecular mechanisms, candidate targets and repositioned drugs for personalized treatment. The review focuses on the algorithmic design and mathematical formulation of these methods to facilitate applications and implementations of network-based analysis in the practice of precision oncology. We review the methods applied in three scenarios to integrate genomic data and network models in different analysis pipelines, and we examine three categories of network-based approaches for repositioning drugs in drug\textendash{}disease\textendash{}gene networks. In addition, we perform a comprehensive subnetwork/pathway analysis of mutations in 31 cancer genome projects in the Cancer Genome Atlas and present a detailed case study on ovarian cancer. Finally, we discuss interesting observations, potential pitfalls and future directions in network-based precision oncology.},
  language = {En},
  number = {1},
  journal = {npj Precision Oncology},
  doi = {10.1038/s41698-017-0029-7},
  author = {Zhang, Wei and Chien, Jeremy and Yong, Jeongsik and Kuang, Rui},
  month = aug,
  year = {2017},
  pages = {25},
  file = {/home/asher/Zotero/storage/BJLP2WKM/Zhang et al. - 2017 - Network-based machine learning and graph theory al.pdf;/home/asher/Zotero/storage/I8M6RNA5/s41698-017-0029-7.html}
}

@article{noren2010,
  title = {Temporal Pattern Discovery in Longitudinal Electronic Patient Records},
  volume = {20},
  issn = {1573-756X},
  abstract = {Large collections of electronic patient records provide a vast but still underutilised source of information on the real world use of medicines. They are maintained primarily for the purpose of patient administration, but contain a broad range of clinical information highly relevant for data analysis. While they are a standard resource for epidemiological confirmatory studies, their use in the context of exploratory data analysis is still limited. In this paper, we present a framework for open-ended pattern discovery in large patient records repositories. At the core is a graphical statistical approach to summarising and visualising the temporal association between the prescription of a drug and the occurrence of a medical event. The graphical overview contrasts the observed and expected number of occurrences of the medical event in different time periods both before and after the prescription of interest. In order to effectively screen for important temporal relationships, we introduce a new measure of temporal association, which contrasts the observed-to-expected ratio in a time period immediately after the prescription to the observed-to-expected ratio in a control period 2 years earlier. An important feature of both the observed-to-expected graph and the measure of temporal association is a statistical shrinkage towards the null hypothesis of no association, which provides protection against highlighting spurious associations. We demonstrate the usefulness of the proposed pattern discovery methodology by a set of examples from a collection of over two million patient records in the United Kingdom. The identified patterns include temporal relationships between drug prescriptions and medical events suggestive of persistent and transient risks of adverse events, possible beneficial effects of drugs, periodic co-occurrence, and systematic tendencies of patients to switch from one medication to another.},
  language = {en},
  number = {3},
  journal = {Data Mining and Knowledge Discovery},
  doi = {10.1007/s10618-009-0152-3},
  author = {Nor{\'e}n, G. Niklas and Hopstadius, Johan and Bate, Andrew and Star, Kristina and Edwards, I. Ralph},
  month = may,
  year = {2010},
  keywords = {Electronic health records,Longitudinal patient records,Temporal pattern discovery},
  pages = {361-387},
  file = {/home/asher/Zotero/storage/GEW86B86/Norén et al. - 2010 - Temporal pattern discovery in longitudinal electro.pdf}
}

@misc{centerfordevicesandradiologicalhealth2010,
  title = {Guidance for the {{Use}} of {{Bayesian Statistics}} in {{Medical Device Clinical Trials}}},
  abstract = {Guidance for Industry and FDA Staff

This document provides guidance on statistical aspects of the design and analysis of clinical trials for medical devices that use Bayesian statistical methods.

The purpose of this guidance is to discuss important statistical issues in Bayesian clinical trials for medical devices. The purpose is not to describe the content of a medical device submission. Further, while this document provides guidance on many
of the statistical issues that arise in Bayesian clinical trials, it is not intended to be all-inclusive. The statistical literature is rich with books and papers on Bayesian theory and methods; a selected bibliography has been included for further discussion of
specific topics.

FDA's guidance documents, including this guidance, do not establish legally enforceable responsibilities. Instead, guidances describe the Agency's current thinking on a topic and should be viewed only as recommendations, unless specific regulatory or statutory requirements are cited. The use of the word should in Agency guidances means that something is suggested or recommended, but not required.},
  howpublished = {https://www.fda.gov/media/71512/download},
  author = {{Center for Devices {and} Radiological Health}},
  month = may,
  year = {2010},
  keywords = {FDA},
  file = {/home/asher/Zotero/storage/SI8J496H/download.pdf}
}

@article{gittins1979,
  title = {Bandit {{Processes}} and {{Dynamic Allocation Indices}}},
  volume = {41},
  issn = {00359246},
  abstract = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
  language = {en},
  number = {2},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  doi = {10.1111/j.2517-6161.1979.tb01068.x},
  author = {Gittins, J. C.},
  month = jan,
  year = {1979},
  pages = {148-164},
  file = {/home/asher/Zotero/storage/6XNDR99U/Gittins - 1979 - Bandit Processes and Dynamic Allocation Indices.pdf}
}

@article{chapelle,
  title = {An {{Empirical Evaluation}} of {{Thompson Sampling}}},
  abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
  language = {en},
  author = {Chapelle, Olivier and Li, Lihong},
  pages = {9},
  file = {/home/asher/Zotero/storage/P2WQT7ZT/Chapelle and Li - An Empirical Evaluation of Thompson Sampling.pdf}
}

@article{ortega2010,
  title = {A {{Minimum Relative Entropy Principle}} for {{Learning}} and {{Acting}}},
  volume = {38},
  copyright = {Copyright (c)},
  issn = {1076-9757},
  language = {en},
  journal = {Journal of Artificial Intelligence Research},
  doi = {10.1613/jair.3062},
  author = {Ortega, P. A. and Braun, D. A.},
  month = aug,
  year = {2010},
  pages = {475-511},
  file = {/home/asher/Zotero/storage/7UIN9THH/Ortega and Braun - 2010 - A Minimum Relative Entropy Principle for Learning .pdf;/home/asher/Zotero/storage/4RRBQHRE/10659.html}
}

@article{goguen1983,
  title = {Reasoning and Natural Explanation},
  volume = {19},
  issn = {00207373},
  language = {en},
  number = {6},
  journal = {International Journal of Man-Machine Studies},
  doi = {10.1016/S0020-7373(83)80070-4},
  author = {Goguen, J.A. and Weiner, J.L. and Linde, C.},
  month = dec,
  year = {1983},
  pages = {521-559},
  file = {/home/asher/Zotero/storage/U9HJD5YH/Goguen et al. - 1983 - Reasoning and natural explanation.pdf}
}

@article{mamlouk2018,
  title = {Contextual {{Radiology Reporting}}: {{A New Approach}} to {{Neuroradiology Structured Templates}}},
  issn = {0195-6108, 1936-959X},
  shorttitle = {Contextual {{Radiology Reporting}}},
  abstract = {SUMMARY: Structured reporting has many advantages over conventional narrative reporting and has been advocated for standard usage by radiologic societies and literature. Traditional structured reports though are often not tailored to the appropriate clinical situation, are generic, and can be overly constraining. Contextual reporting is an alternative method of structured reporting that is specifically related to the disease or examination indication. Herein, we create a library of 50 contextual structured reports for neuroradiologists and emphasize their clinical value over noncontextual structured reporting. These templates are located in the On-line Appendix, and a downloadable PowerScribe 360 file may be accessed at https://drive.google.com/open?idϭ1AlPUmfAXPzjkMFcHf7vGKF4Q-vIdpflT.},
  language = {en},
  journal = {American Journal of Neuroradiology},
  doi = {10.3174/ajnr.A5697},
  author = {Mamlouk, M.D. and Chang, P.C. and Saket, R.R.},
  month = jun,
  year = {2018},
  pages = {ajnr;ajnr.A5697v2},
  file = {/home/asher/Zotero/storage/ZVHKMU6W/Mamlouk et al. - 2018 - Contextual Radiology Reporting A New Approach to .pdf}
}

@article{rodriguez,
  title = {Parametric {{Survival Models}}},
  language = {en},
  author = {Rodr\i{}guez, German},
  pages = {14},
  file = {/home/asher/Zotero/storage/CZLWNKF8/Rodrıguez - Parametric Survival Models.pdf}
}

@article{miller2012,
  title = {Sequential Changes in Gene Expression Profiles in Breast Cancers during Treatment with the Aromatase Inhibitor, Letrozole},
  volume = {12},
  issn = {1470-269X, 1473-1150},
  language = {en},
  number = {1},
  journal = {The Pharmacogenomics Journal},
  doi = {10.1038/tpj.2010.67},
  author = {Miller, W R and Larionov, A and Anderson, T J and Evans, D B and Dixon, J M},
  month = feb,
  year = {2012},
  pages = {10-21},
  file = {/home/asher/Zotero/storage/GKTAXG9F/Miller et al. - 2012 - Sequential changes in gene expression profiles in .pdf}
}

@article{hong2004,
  title = {Impact of Prior Biopsy Scheme on Pathologic Features of Cancers Detected on Repeat Biopsies},
  volume = {22},
  issn = {10781439},
  abstract = {The object of our study was to characterize the biopsy features of cancers detected in a repeat biopsy population stratified on the basis of the type of prior negative biopsy. We studied 218 patients with a prior negative biopsy who underwent a 10-core extended systematic biopsy scheme, and a subset (n ϭ 139) underwent additional 6 anteriorly directed biopsies. Clinicopathologic features of patients with cancer on the biopsy were compared as a function of type of prior negative biopsy. Overall and unique cancer detection rates were calculated for each of the biopsy sites. Cancer detection rates tended to be higher in patients who had undergone a prior sextant biopsy compared to a prior extended biopsy scheme (39\% vs. 28\%). Trends towards more positive cores and greater total core length of cancer involvement were seen in patients who had undergone a prior negative sextant biopsy. Apical and laterally directed biopsies had higher overall and unique cancer detection rates in patients who had undergone a prior negative sextant biopsy. Anteriorly directed biopsies had a low unique cancer detection rate in all patients. We conclude that in patients undergoing repeat biopsy, the detection rate is affected by the extent of the prior biopsy. Clinicopathologic features of cancers detected on repeat biopsy tend to be worse in patients who have undergone a prior negative sextant biopsy compared to a negative prior extended biopsy. \textcopyright{} 2004 Elsevier Inc. All rights reserved.},
  language = {en},
  number = {1},
  journal = {Urologic Oncology: Seminars and Original Investigations},
  doi = {10.1016/S1078-1439(03)00147-9},
  author = {Hong, Y.Mark and Lai, Frank C and Chon, Chris H and McNeal, John E and Presti, Joseph C},
  month = jan,
  year = {2004},
  pages = {7-10},
  file = {/home/asher/Zotero/storage/HCDX78B3/Hong et al. - 2004 - Impact of prior biopsy scheme on pathologic featur.pdf}
}

@article{blau2013,
  title = {Can We Deconstruct Cancer, One Patient at a Time?},
  volume = {29},
  issn = {01689525},
  language = {en},
  number = {1},
  journal = {Trends in Genetics},
  doi = {10.1016/j.tig.2012.09.004},
  author = {Blau, C. Anthony and Liakopoulou, Effie},
  month = jan,
  year = {2013},
  pages = {6-10},
  file = {/home/asher/Zotero/storage/UASGJCGL/Blau and Liakopoulou - 2013 - Can we deconstruct cancer, one patient at a time.pdf}
}

@article{kim2011,
  title = {The {{BATTLE Trial}}: {{Personalizing Therapy}} for {{Lung Cancer}}},
  volume = {1},
  issn = {2159-8274, 2159-8290},
  shorttitle = {The {{BATTLE Trial}}},
  language = {en},
  number = {1},
  journal = {Cancer Discovery},
  doi = {10.1158/2159-8274.CD-10-0010},
  author = {Kim, Edward S. and Herbst, Roy S. and Wistuba, Ignacio I. and Lee, J. Jack and Blumenschein, George R. and Tsao, Anne and Stewart, David J. and Hicks, Marshall E. and Erasmus, Jeremy and Gupta, Sanjay and Alden, Christine M. and Liu, Suyu and Tang, Ximing and Khuri, Fadlo R. and Tran, Hai T. and Johnson, Bruce E. and Heymach, John V. and Mao, Li and Fossella, Frank and Kies, Merrill S. and Papadimitrakopoulou, Vassiliki and Davis, Suzanne E. and Lippman, Scott M. and Hong, Waun K.},
  month = jun,
  year = {2011},
  pages = {44-53},
  file = {/home/asher/Zotero/storage/37DCSAWU/Kim et al. - 2011 - The BATTLE Trial Personalizing Therapy for Lung C.pdf}
}

@article{berry2012,
  title = {Adaptive Clinical Trials in Oncology},
  volume = {9},
  issn = {1759-4774, 1759-4782},
  abstract = {Modern oncology drug development faces challenges very different from those of the past and it must adapt accordingly. The size and expense of phase III clinical trials continue to increase, but the success rate remains unacceptably low. Adaptive trial designs can make development more informative, addressing whether a drug is safe and effective while showing how it should be delivered and to whom. An adaptive design is one in which the accumulating data are used to modify the trial's course. Adaptive designs are ideal for addressing many questions at once. For example, a single trial might identify the appropriate patient population, dose and regimen, and therapeutic combinations, and then switch seamlessly into a phase III confirmatory trial. Adaptive designs rely on information, including from patients who have not achieved the trial's primary end point. Longitudinal models of biomarkers (including tumor burden assessed via imaging) enable predictions of primary end points. Taking a Bayesian perspective facilitates building an efficient and accurate trial, including using longitudinal information. A wholly new paradigm for drug development exemplifying personalized medicine is evinced by an adaptive trial called I-SPY2, in which drugs from many companies are evaluated in the same trial\textemdash{}a phase II screening process.},
  language = {en},
  number = {4},
  journal = {Nature Reviews Clinical Oncology},
  doi = {10.1038/nrclinonc.2011.165},
  author = {Berry, Donald A.},
  month = apr,
  year = {2012},
  pages = {199-207},
  file = {/home/asher/Zotero/storage/BXKN9WIK/JCE_paper_Revised_Draft_for_re_submission_22June2017notrackchanges.docx;/home/asher/Zotero/storage/JZYZ2NBS/Berry - 2012 - Adaptive clinical trials in oncology.pdf}
}

@article{nass,
  title = {National {{Cancer Policy Forum Board}} on {{Health Care Services}}},
  language = {en},
  author = {Nass, Sharyl J and Wizemann, Theresa},
  pages = {147},
  file = {/home/asher/Zotero/storage/N8QKYGS4/Nass and Wizemann - National Cancer Policy Forum Board on Health Care .pdf}
}

@article{murphy,
  title = {National {{Cancer Policy Forum}}},
  language = {en},
  author = {Murphy, Sharon and Patlak, Margie},
  pages = {125},
  file = {/home/asher/Zotero/storage/CMZ9IBAZ/Murphy and Patlak - National Cancer Policy Forum.pdf}
}

@article{lee2017,
  title = {Clonal {{History}} and {{Genetic Predictors}} of {{Transformation Into Small}}-{{Cell Carcinomas From Lung Adenocarcinomas}}},
  volume = {35},
  issn = {0732-183X, 1527-7755},
  abstract = {Purpose Histologic transformation of EGFR mutant lung adenocarcinoma (LADC) into small-cell lung cancer (SCLC) has been described as one of the major resistant mechanisms for epidermal growth factor receptor (EGFR) tyrosine kinase inhibitors (TKIs). However, the molecular pathogenesis is still unclear.
Methods We investigated 21 patients with advanced EGFR-mutant LADCs that were transformed into EGFR TKI\textendash{}resistant SCLCs. Among them, whole genome sequencing was applied for nine tumors acquired at various time points from four patients to reconstruct their clonal evolutionary history and to detect genetic predictors for small-cell transformation. The findings were validated by immunohistochemistry in 210 lung cancer tissues.
Results We identified that EGFR TKI\textendash{}resistant LADCs and SCLCs share a common clonal origin and undergo branched evolutionary trajectories. The clonal divergence of SCLC ancestors from the LADC cells occurred before the first EGFR TKI treatments, and the complete inactivation of both RB1 and TP53 were observed from the early LADC stages in sequenced tumors. We extended the findings by immunohistochemistry in the early-stage LADC tissues of 75 patients treated with EGFR TKIs; inactivation of both Rb and p53 was strikingly more frequent in the small-cell\textendash{}transformed group than in the nontransformed group (82\% v 3\%; odds ratio, 131; 95\% CI, 19.9 to 859). Among patients registered in a predefined cohort (n = 65), an EGFR mutant LADC that harbored completely inactivated Rb and p53 had a 433 greater risk of small-cell transformation (relative risk, 42.8; 95\% CI, 5.88 to 311). Branch-specific mutational signature analysis revealed that apolipoprotein B mRNA editing enzyme, catalytic polypeptide-like (APOBEC)\textendash{}induced hypermutation was frequent in the branches toward small-cell transformation.
Conclusion EGFR TKI\textendash{}resistant SCLCs are branched out early from the LADC clones that harbor completely inactivated RB1 and TP53. The evaluation of RB1 and TP53 status in EGFR TKI\textendash{}treated LADCs is informative in predicting small-cell transformation.},
  language = {en},
  number = {26},
  journal = {Journal of Clinical Oncology},
  doi = {10.1200/JCO.2016.71.9096},
  author = {Lee, June-Koo and Lee, Junehawk and Kim, Sehui and Kim, Soyeon and Youk, Jeonghwan and Park, Seongyeol and An, Yohan and Keam, Bhumsuk and Kim, Dong-Wan and Heo, Dae Seog and Kim, Young Tae and Kim, Jin-Soo and Kim, Se Hyun and Lee, Jong Seok and Lee, Se-Hoon and Park, Keunchil and Ku, Ja-Lok and Jeon, Yoon Kyung and Chung, Doo Hyun and Park, Peter J. and Kim, Joon and Kim, Tae Min and Ju, Young Seok},
  month = sep,
  year = {2017},
  pages = {3065-3074},
  file = {/home/asher/Zotero/storage/RNKNWJIB/Lee et al. - 2017 - Clonal History and Genetic Predictors of Transform.pdf}
}

@article{betancourt2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.02434},
  primaryClass = {stat},
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  journal = {arXiv:1701.02434 [stat]},
  author = {Betancourt, Michael},
  month = jan,
  year = {2017},
  keywords = {Statistics - Methodology},
  file = {/home/asher/Zotero/storage/YTJDUE3S/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;/home/asher/Zotero/storage/ASXNKCZ8/1701.html}
}

@article{hanin2018,
  title = {Suppression of {{Metastasis}} by {{Primary Tumor}} and {{Acceleration}} of {{Metastasis Following Primary Tumor Resection}}: {{A Natural Law}}?},
  volume = {80},
  issn = {0092-8240, 1522-9602},
  shorttitle = {Suppression of {{Metastasis}} by {{Primary Tumor}} and {{Acceleration}} of {{Metastasis Following Primary Tumor Resection}}},
  language = {en},
  number = {3},
  journal = {Bulletin of Mathematical Biology},
  doi = {10.1007/s11538-017-0388-9},
  author = {Hanin, Leonid and Rose, Jason},
  month = mar,
  year = {2018},
  pages = {519-539},
  file = {/home/asher/Zotero/storage/WAD6EBPV/Hanin and Rose - 2018 - Suppression of Metastasis by Primary Tumor and Acc.pdf}
}

@article{hanin,
  title = {{{MATHEMATICAL DISCOVERY OF NATURAL LAWS IN BIOMEDICAL SCIENCES}}: {{A NEW METHODOLOGY}}},
  abstract = {As biomedical sciences discover new layers of complexity in the mechanisms of life and disease, mathematical models trying to catch up with these developments become mathematically intractable. As a result, in the grand scheme of things, mathematical models have so far played an auxiliary role in biomedical sciences. We propose a new methodology allowing mathematical modeling to give, in certain cases, definitive answers to systemic biomedical questions that elude empirical resolution. Our methodology is based on two ideas: (1) employing mathematical models that are firmly rooted in established biomedical knowledge yet so general that they can account for any, or at least many, biological mechanisms, both known and unknown; (2) finding model parameters whose likelihood-maximizing values are independent of observations (existence of such parameters implies that the model must not meet regularity conditions required for the consistency of maximum likelihood estimator). These universal parameter values may reveal general patterns (that we call natural laws) in biomedical processes. We illustrate this approach with the discovery of a clinically important natural law governing cancer metastasis. Specifically, we found that under minimal, and fairly realistic, mathematical and biomedical assumptions the likelihood-maximizing scenario of metastatic cancer progression in an individual patient is invariably the same: Complete suppression of metastatic growth before primary tumor resection followed by an abrupt growth acceleration after surgery. This scenario is widely observed in clinical practice and supported by a wealth of experimental studies on animals and clinical case reports published over the last 110 years. The above most likely scenario does not preclude other possibilities e.g. metastases may start aggressive growth before primary tumor resection or remain dormant after surgery.},
  language = {en},
  author = {Hanin, Leonid},
  pages = {15},
  file = {/home/asher/Zotero/storage/KGZP7K2F/Hanin - MATHEMATICAL DISCOVERY OF NATURAL LAWS IN BIOMEDIC.pdf}
}

@article{altrock2015,
  title = {The Mathematics of Cancer: Integrating Quantitative Models},
  volume = {15},
  issn = {1474-175X, 1474-1768},
  shorttitle = {The Mathematics of Cancer},
  abstract = {Mathematical modelling approaches have become increasingly abundant in cancer research. The complexity of cancer is well suited to quantitative approaches as it provides challenges and opportunities for new developments. In turn, mathematical modelling contributes to cancer research by helping to elucidate mechanisms and by providing quantitative predictions that can be validated. The recent expansion of quantitative models addresses many questions regarding tumour initiation, progression and metastases as well as intra-tumour heterogeneity, treatment responses and resistance. Mathematical models can complement experimental and clinical studies, but also challenge current paradigms, redefine our understanding of mechanisms driving tumorigenesis and shape future research in cancer biology.},
  language = {en},
  number = {12},
  journal = {Nature Reviews Cancer},
  doi = {10.1038/nrc4029},
  author = {Altrock, Philipp M. and Liu, Lin L. and Michor, Franziska},
  month = dec,
  year = {2015},
  pages = {730-745},
  file = {/home/asher/Zotero/storage/CL4CHKSB/Altrock et al. - 2015 - The mathematics of cancer integrating quantitativ.pdf}
}

@article{markert2015,
  title = {Mathematical Models of Cancer Metabolism},
  volume = {3},
  issn = {2049-3002},
  abstract = {Metabolism is essential for life, and its alteration is implicated in multiple human diseases. The transformation from a normal to a cancerous cell requires metabolic changes to fuel the high metabolic demands of cancer cells, including but not limited to cell proliferation and cell migration. In recent years, there have been a number of new discoveries connecting known aberrations in oncogenic and tumour suppressor pathways with metabolic alterations required to sustain cell proliferation and migration. However, an understanding of the selective advantage of these metabolic alterations is still lacking. Here, we review the literature on mathematical models of metabolism, with an emphasis on their contribution to the identification of the selective advantage of metabolic phenotypes that seem otherwise wasteful or accidental. We will show how the molecular hallmarks of cancer can be related to cell proliferation and tissue remodelling, the two major physiological requirements for the development of a multicellular structure. We will cover different areas such as genome-wide gene expression analysis, flux balance models, kinetic models, reaction diffusion models and models of the tumour microenvironment. We will also highlight current challenges and how their resolution will help to achieve a better understanding of cancer metabolism and the metabolic vulnerabilities of cancers.},
  language = {en},
  number = {1},
  journal = {Cancer \& Metabolism},
  doi = {10.1186/s40170-015-0140-6},
  author = {Markert, Elke Katrin and Vazquez, Alexei},
  month = dec,
  year = {2015},
  pages = {14},
  file = {/home/asher/Zotero/storage/ANPCFXVB/Markert and Vazquez - 2015 - Mathematical models of cancer metabolism.pdf}
}

@article{ribba2006,
  title = {[{{No}} Title Found]},
  volume = {3},
  issn = {17424682},
  abstract = {Background: Radiotherapy outcomes are usually predicted using the Linear Quadratic model. However, this model does not integrate complex features of tumor growth, in particular cell cycle regulation.
Methods: In this paper, we propose a multiscale model of cancer growth based on the genetic and molecular features of the evolution of colorectal cancer. The model includes key genes, cellular kinetics, tissue dynamics, macroscopic tumor evolution and radiosensitivity dependence on the cell cycle phase. We investigate the role of gene-dependent cell cycle regulation in the response of tumors to therapeutic irradiation protocols.
Results: Simulation results emphasize the importance of tumor tissue features and the need to consider regulating factors such as hypoxia, as well as tumor geometry and tissue dynamics, in predicting and improving radiotherapeutic efficacy.
Conclusion: This model provides insight into the coupling of complex biological processes, which leads to a better understanding of oncogenesis. This will hopefully lead to improved irradiation therapy.},
  language = {en},
  number = {1},
  journal = {Theoretical Biology and Medical Modelling},
  doi = {10.1186/1742-4682-3-7},
  author = {Ribba, Benjamin and Colin, Thierry and Schnell, Santiago},
  year = {2006},
  pages = {7},
  file = {/home/asher/Zotero/storage/6KLXJ9S9/Ribba et al. - 2006 - [No title found].pdf}
}

@misc{liu2014,
  type = {Research Article},
  title = {A {{Mathematical Model}} of {{Cancer Treatment}} by {{Radiotherapy}}},
  abstract = {A periodic mathematical model of cancer treatment by radiotherapy is presented and studied in this paper. Conditions on the coexistence of the healthy and cancer cells are obtained. Furthermore, sufficient conditions on the existence and globally asymptotic stability of the positive periodic solution, the cancer eradication periodic solution, and the cancer win periodic solution are established. Some numerical examples are shown to verify the validity of the results. A discussion is presented for further study.},
  language = {en},
  journal = {Computational and Mathematical Methods in Medicine},
  howpublished = {https://www.hindawi.com/journals/cmmm/2014/172923/},
  author = {Liu, Zijian and Yang, Chenxue},
  year = {2014},
  file = {/home/asher/Zotero/storage/3U9REFH5/172923.html;/home/asher/Zotero/storage/7G2VNDBZ/Liu and Yang - 2014 - A Mathematical Model of Cancer Treatment by Radiot.pdf},
  doi = {10.1155/2014/172923},
  pmid = {25478002}
}

@article{hanin2017,
  title = {Why Statistical Inference from Clinical Trials Is Likely to Generate False and Irreproducible Results},
  volume = {17},
  issn = {1471-2288},
  abstract = {One area of biomedical research where the replication crisis is most visible and consequential is clinical trials. Why do outcomes of so many clinical trials contradict each other? Why is the effectiveness of many drugs and other medical interventions so low? Why have prescription medications become the third leading cause of death in the US and Europe after cardiovascular diseases and cancer? In answering these questions, the main culprits identified so far have been various biases and conflicts of interest in planning, execution and analysis of clinical trials as well as reporting their outcomes. In this work, we take an in-depth look at statistical methodology used in planning clinical trials and analyzing trial data. We argue that this methodology is based on various questionable and empirically untestable assumptions, dubious approximations and arbitrary thresholds, and that it is deficient in many other respects. The most objectionable among these assumptions is that of distributional homogeneity of subjects' responses to medical interventions. We analyze this and other assumptions both theoretically and through clinical examples. Our main conclusion is that even a totally unbiased, perfectly randomized, reliably blinded, and faithfully executed clinical trial may still generate false and irreproducible results. We also formulate a few recommendations for the improvement of the design and statistical methodology of clinical trials informed by our analysis.},
  language = {en},
  number = {1},
  journal = {BMC Medical Research Methodology},
  doi = {10.1186/s12874-017-0399-0},
  author = {Hanin, Leonid},
  month = dec,
  year = {2017},
  pages = {127},
  file = {/home/asher/Zotero/storage/HALI6RBK/Hanin - 2017 - Why statistical inference from clinical trials is .pdf}
}

@article{hanin2014,
  title = {Identifiability of Cure Models Revisited},
  volume = {130},
  issn = {0047259X},
  abstract = {We obtained results on identifiability of mixture, mixture proportional hazards and bounded cumulative hazards (or Yakovlev) models of survival in the presence of cured (or non-susceptible) subpopulation. These results specify conditions under which model parameters can, or cannot, be estimated from the observed potentially censored survival times and thus may guide statistical modeling. The results are formulated for larger classes of models and in greater generality than previously and correct some misconceptions that exist in statistical literature on the subject. All results are supplied with rigorous selfcontained proofs.},
  language = {en},
  journal = {Journal of Multivariate Analysis},
  doi = {10.1016/j.jmva.2014.06.002},
  author = {Hanin, Leonid and Huang, Li-Shan},
  month = sep,
  year = {2014},
  pages = {261-274},
  file = {/home/asher/Zotero/storage/I6GXKIEQ/Hanin and Huang - 2014 - Identifiability of cure models revisited.pdf}
}

@inproceedings{granmo2008,
  address = {{San Diego, CA}},
  title = {A {{Bayesian Learning Automaton}} for {{Solving Two}}-{{Armed Bernoulli Bandit Problems}}},
  isbn = {978-0-7695-3495-4},
  abstract = {Purpose \textendash{} The two-armed Bernoulli bandit (TABB) problem is a classical optimization problem where an agent sequentially pulls one of two arms attached to a gambling machine, with each pull resulting either in a reward or a penalty. The reward probabilities of each arm are unknown, and thus one must balance between exploiting existing knowledge about the arms, and obtaining new information. The purpose of this paper is to report research into a completely new family of solution schemes for the TABB problem: the Bayesian learning automaton (BLA) family.},
  language = {en},
  booktitle = {2008 {{Seventh International Conference}} on {{Machine Learning}} and {{Applications}}},
  publisher = {{IEEE}},
  doi = {10.1109/ICMLA.2008.67},
  author = {Granmo, Ole-Christoffer},
  month = dec,
  year = {2008},
  pages = {23-30},
  file = {/home/asher/Zotero/storage/EH6JXZRF/Granmo - 2008 - A Bayesian Learning Automaton for Solving Two-Arme.pdf}
}

@article{hanin2016,
  title = {A Quantitative Insight into Metastatic Relapse of Breast Cancer},
  volume = {394},
  issn = {00225193},
  abstract = {Background: Metastatic relapse is the principal source of breast cancer mortality. This work seeks to uncover unobservable, yet clinically important, aspects of post-surgery metastatic relapse of breast cancer and to quantify effects of surgery on metastatic progression.
Methods: We classified metastases into three categories: (1) solitary cancer cells that were formed before or during surgery and either circulate in blood or are lodged at various secondary sites; (2) dormant or slowly growing avascular metastases; and (3) vascular secondary tumors. We developed a general mathematical model aimed at describing post-surgery dynamics of these three metastatic states. One parametric version of the model assumed that sojourn times of metastases in the three states are exponentially distributed while another was based on Erlang distribution. Model parameters were estimated from a sample of metastatic relapse or censoring times for 673 breast cancer patients treated with surgery.
Results: We estimated the expected number of metastases at surgery and mean sojourn times for the three states and found that both are decreasing with state number. We also computed the probability that metastatic relapse resulted from a metastasis in a given state at surgery. The values of these attribution probabilities suggest that under the Erlang model all three states have a considerable effect on metastatic relapse while in the case of exponential model this is true for states 1 and 2 only.
Conclusions: (1) In some patients metastasis occurred before surgery; (2) our results confirm significance of metastatic dormancy; (3) according to the model surgery stimulates escape from dormancy, promotes angiogenesis and accelerates metastatic growth in a fraction of breast cancer patients. Taken summarily, these findings call into question the benefits of primary tumor resection for certain categories of breast cancer patients.},
  language = {en},
  journal = {Journal of Theoretical Biology},
  doi = {10.1016/j.jtbi.2016.01.014},
  author = {Hanin, Leonid and Pavlova, Lyudmila},
  month = apr,
  year = {2016},
  pages = {172-181},
  file = {/home/asher/Zotero/storage/SIQZV39R/Hanin and Pavlova - 2016 - A quantitative insight into metastatic relapse of .pdf}
}

@article{tan2017,
  title = {Differences in {{Treatment Effect Size Between Overall Survival}} and {{Progression}}-{{Free Survival}} in {{Immunotherapy Trials}}: {{A Meta}}-{{Epidemiologic Study}} of {{Trials With Results Posted}} at {{ClinicalTrials}}.Gov},
  volume = {35},
  issn = {0732-183X, 1527-7755},
  shorttitle = {Differences in {{Treatment Effect Size Between Overall Survival}} and {{Progression}}-{{Free Survival}} in {{Immunotherapy Trials}}},
  abstract = {Purpose We aimed to compare treatment effect sizes between overall survival (OS) and progression-free survival (PFS) in trials of US Food and Drug Administration\textendash{}approved oncology immunotherapy drugs with results posted at ClinicalTrials.gov.
Methods We searched ClinicalTrials.gov for phase II to IV cancer trials of Food and Drug Administration\textendash{}approved immunotherapy drugs and selected those reporting results for both OS and PFS. For each trial, we extracted the hazard ratios (HRs) with 95\% CIs for both outcomes and evaluated the differences by a ratio of HRs (rHRs): the HR for PFS to that for OS. We performed a random effects meta-analysis across trials to obtain a summary rHR. We also evaluated surrogacy of PFS for OS by the coefficient of determination and the surrogacy threshold effect, the minimal value of HR for PFS to predict a non-null effect on OS.
Results We identified 51 trials assessing 14 drugs across 15 conditions. Treatment effect sizes were 17\% greater, on average, with PFS than with OS (rHR, 0.83; 95\% CI, 0.79 to 0.88; I2 = 34.4\%; P = .01; t2 = 0.0129). Nearly one half of the trials (n = 23, 45\%) showed statistically significant benefits for PFS but not for OS. Differences were great for trials of obinutuzumab (rHR, 0.21; 95\% CI, 0.08 to 0.54), bevacizumab (rHR, 0.75; 95\% CI, 0.67 to 0.84), and rituximab (rHR, 0.79; 95\% CI, 0.64 to 0.98). The coefficient of determination was 38\% and the surrogacy threshold effect was 0.50.
Conclusion Treatment effect sizes in trials of immunotherapy drugs were greater for PFS than for OS, with important differences for some drugs, which is consistent with surrogacy metrics. Caution must be taken when interpreting PFS in the absence of OS data.},
  language = {en},
  number = {15},
  journal = {Journal of Clinical Oncology},
  doi = {10.1200/JCO.2016.71.2109},
  author = {Tan, Aidan and Porcher, Raphael and Crequit, Perrine and Ravaud, Philippe and Dechartres, Agnes},
  month = may,
  year = {2017},
  pages = {1686-1694},
  file = {/home/asher/Zotero/storage/9LCCDIX9/Tan et al. - 2017 - Differences in Treatment Effect Size Between Overa.pdf}
}

@article{zotero-604,
  title = {Meta-{{Analysis}} of {{Hazard Ratios}}},
  language = {en},
  pages = {14},
  file = {/home/asher/Zotero/storage/749QZ73W/Meta-Analysis of Hazard Ratios.pdf}
}

@article{fernandez2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.00817},
  primaryClass = {stat},
  title = {Gaussian {{Processes}} for {{Survival Analysis}}},
  abstract = {We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests.},
  language = {en},
  journal = {arXiv:1611.00817 [stat]},
  author = {Fern{\'a}ndez, Tamara and Rivera, Nicol{\'a}s and Teh, Yee Whye},
  month = nov,
  year = {2016},
  keywords = {Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/DLW5UADZ/Fernández et al. - 2016 - Gaussian Processes for Survival Analysis.pdf;/home/asher/Zotero/storage/XKYMKHJB/gupta2018.pdf}
}

@article{snyder,
  title = {Contribution of Systemic and Somatic Factors to Clinical Response and Resistance to {{PD}}-{{L1}} Blockade in Urothelial Cancer: {{An}} Exploratory Multi-Omic Analysis},
  abstract = {Background Inhibition of programmed death-ligand 1 (PD-L1) with atezolizumab can induce durable clinical benefit (DCB) in patients with metastatic urothelial cancers, including complete remissions in patients with chemotherapy refractory disease. Although mutation load and PD-L1 immune cell (IC) staining have been associated with response, they lack sufficient sensitivity and specificity for clinical use. Thus, there is a need to evaluate the peripheral blood immune environment and to conduct detailed analyses of mutation load, predicted neoantigens, and immune cellular infiltration in tumors to enhance our understanding of the biologic underpinnings of response and resistance.},
  language = {en},
  author = {Snyder, Alexandra and Nathanson, Tavi and Funt, Samuel A and Ahuja, Arun and Novik, Jacqueline Buros and Hellmann, Matthew D and Chang, Eliza and Aksoy, Bulent Arman and {Al-Ahmadie}, Hikmat and Yusko, Erik and Vignali, Marissa and Benzeno, Sharon and Boyd, Mariel and Moran, Meredith and Iyer, Gopa and Robins, Harlan S and Mardis, Elaine R and Merghoub, Taha and Hammerbacher, Jeff and Rosenberg, Jonathan E and Bajorin, Dean F},
  pages = {24},
  file = {/home/asher/Zotero/storage/3PQLIPX5/Snyder et al. - Contribution of systemic and somatic factors to cl.pdf}
}

@article{friedman2008,
  title = {Predictive Learning via Rule Ensembles},
  volume = {2},
  issn = {1932-6157, 1941-7330},
  abstract = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principal advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as is its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the identities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects.},
  language = {EN},
  number = {3},
  journal = {The Annals of Applied Statistics},
  doi = {10.1214/07-AOAS148},
  author = {Friedman, Jerome H. and Popescu, Bogdan E.},
  month = sep,
  year = {2008},
  keywords = {classification,data mining,interaction effects,learning ensembles,machine learning,Regression,rules,variable importance},
  pages = {916-954},
  file = {/home/asher/Zotero/storage/R4ZDBP6B/Friedman and Popescu - 2008 - Predictive learning via rule ensembles.pdf;/home/asher/Zotero/storage/EHTZMMFB/1223908046.html}
}

@article{anderson2019,
  title = {The {{Rankability}} of {{Data}}},
  volume = {1},
  issn = {2577-0187},
  abstract = {This paper poses and solves a new problem, the rankability problem, which refers to a dataset's inherent ability to produce a meaningful ranking of its items. Ranking is a fundamental data science task. Its applications are numerous and include web search, data mining, cybersecurity, machine learning, and statistical learning theory. Yet little attention has been paid to the question of whether a dataset is suitable for ranking. As a result, when a ranking method is applied to an unrankable dataset, the resulting ranking may not be reliable. The rankability problem asks the following: How can rankability be quantified? At what point is a dynamic, time-evolving graph rankable? If a dataset has low rankability, can modifications be made and which most improve the graph's rankability? We present a combinatorial approach to a rankability measure and then compare several algorithms for computing this new measure. Finally, we apply our new measure to several datasets.},
  language = {en},
  number = {1},
  journal = {SIAM Journal on Mathematics of Data Science},
  doi = {10.1137/18M1183595},
  author = {Anderson, Paul and Chartier, Timothy and Langville, Amy},
  month = jan,
  year = {2019},
  pages = {121-143},
  file = {/home/asher/Zotero/storage/IES49HRV/Anderson et al. - 2019 - The Rankability of Data.pdf}
}

@article{kurniati2018,
  title = {Process Mining in Oncology Using the {{MIMIC}}-{{III}} Dataset},
  volume = {971},
  issn = {1742-6588, 1742-6596},
  abstract = {Process mining is a data analytics approach to discover and analyse process models based on the real activities captured in information systems. There is a growing body of literature on process mining in healthcare, including oncology, the study of cancer. In earlier work we found 37 peer-reviewed papers describing process mining research in oncology with a regular complaint being the limited availability and accessibility of datasets with suitable information for process mining. Publicly available datasets are one option and this paper describes the potential to use MIMIC-III, for process mining in oncology. MIMIC-III is a large open access dataset of de-identified patient records. There are 134 publications listed as using the MIMIC dataset, but none of them have used process mining. The MIMIC-III dataset has 16 event tables which are potentially useful for process mining and this paper demonstrates the opportunities to use MIMIC-III for process mining in oncology. Our research applied the L* lifecycle method to provide a worked example showing how process mining can be used to analyse cancer pathways. The results and data quality limitations are discussed along with opportunities for further work and reflection on the value of MIMIC-III for reproducible process mining research.},
  language = {en},
  journal = {Journal of Physics: Conference Series},
  doi = {10.1088/1742-6596/971/1/012008},
  author = {Kurniati, Angelina Prima and Hall, Geoff and Hogg, David and Johnson, Owen},
  month = mar,
  year = {2018},
  pages = {012008},
  file = {/home/asher/Zotero/storage/CU83WGWA/Kurniati et al. - 2018 - Process mining in oncology using the MIMIC-III dat.pdf}
}

@article{hogg2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1008.4686},
  primaryClass = {astro-ph, physics:physics},
  title = {Data Analysis Recipes: {{Fitting}} a Model to Data},
  shorttitle = {Data Analysis Recipes},
  abstract = {We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a "generative model" for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation of the likelihood of the parameters or the posterior probability distribution. Construction of a posterior probability distribution is indispensible if there are "nuisance parameters" to marginalize away.},
  journal = {arXiv:1008.4686 [astro-ph, physics:physics]},
  author = {Hogg, David W. and Bovy, Jo and Lang, Dustin},
  month = aug,
  year = {2010},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Data Analysis; Statistics and Probability},
  file = {/home/asher/Zotero/storage/GXXMR256/Hogg et al. - 2010 - Data analysis recipes Fitting a model to data.pdf;/home/asher/Zotero/storage/N9N3YD3K/1008.html}
}

@article{kucukelbir2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.03431},
  primaryClass = {stat},
  title = {Automatic {{Variational Inference}} in {{Stan}}},
  abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
  journal = {arXiv:1506.03431 [stat]},
  author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
  month = jun,
  year = {2015},
  keywords = {Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/W9G6FFG5/Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf;/home/asher/Zotero/storage/H3WANTIG/1506.html}
}

@book{ibrahim2010,
  address = {{New York}},
  edition = {Softcover repr. of the hardcover 1st edition 2001, corr. 2nd printing},
  series = {Springer Series in Statistics},
  title = {Bayesian Survival Analysis},
  isbn = {978-1-4419-2933-4},
  language = {en},
  publisher = {{Springer}},
  author = {Ibrahim, Joseph George and Chen, Ming-Hui and Sinha, Debajyoti},
  year = {2010},
  file = {/home/asher/Zotero/storage/CWIZZZIL/Ibrahim et al. - 2010 - Bayesian survival analysis.pdf}
}

@article{ideta2008,
  title = {A {{Mathematical Model}} of {{Intermittent Androgen Suppression}} for {{Prostate Cancer}}},
  volume = {18},
  issn = {0938-8974, 1432-1467},
  abstract = {For several decades, androgen suppression has been the principal modality for treatment of advanced prostate cancer. Although the androgen deprivation is initially effective, most patients experience a relapse within several years due to the proliferation of so-called androgen-independent tumor cells. Bruchovsky et al. suggested in animal models that intermittent androgen suppression (IAS) can prolong the time to relapse when compared with continuous androgen suppression (CAS). Therefore, IAS has been expected to enhance clinical efficacy in conjunction with reduction in adverse effects and improvement in quality of life of patients during off-treatment periods. This paper presents a mathematical model that describes the growth of a prostate tumor under IAS therapy based on monitoring of the serum prostate-specific antigen (PSA). By treating the cancer tumor as a mixed assembly of androgen-dependent and androgen-independent cells, we investigate the difference between CAS and IAS with respect to factors affecting an androgen-independent relapse. Numerical and bifurcation analyses show how the tumor growth and the relapse time are influenced by the net growth rate of the androgen-independent cells, a protocol of the IAS therapy, and the mutation rate from androgen-dependent cells to androgen-independent ones.},
  language = {en},
  number = {6},
  journal = {Journal of Nonlinear Science},
  doi = {10.1007/s00332-008-9031-0},
  author = {Ideta, Aiko Miyamura and Tanaka, Gouhei and Takeuchi, Takumi and Aihara, Kazuyuki},
  month = dec,
  year = {2008},
  pages = {593-614},
  file = {/home/asher/Zotero/storage/RZCWWDL8/Ideta et al. - 2008 - A Mathematical Model of Intermittent Androgen Supp.pdf}
}

@article{yang2016,
  title = {A Nonlinear Competitive Model of the Prostate Tumor Growth under Intermittent Androgen Suppression},
  volume = {404},
  issn = {00225193},
  abstract = {Hormone suppression has been the primary modality of treatment for prostate cancer. However long-term androgen deprivation may induce androgen-independent (AI) recurrence. Intermittent androgen suppression (IAS) is a potential way to delay or avoid the AI relapse. Mathematical models of tumor growth and treatment are simple while they are capable of capturing the essence of complicated interactions. Game theory models have analyzed that tumor cells can enhance their fitness by adopting genetically determined survival strategies. In this paper, we consider the survival strategies as the competitive advantage of tumor cells and propose a new model to mimic the prostate tumor growth in IAS therapy. Then we investigate the competition effect in tumor development by numerical simulations. The results indicate that successfully IAScontrolled states can be achieved even though the net growth rate of AI cells is positive for any androgen level. There is crucial difference between the previous models and the new one in the phase diagram of successful and unsuccessful tumor control by IAS administration, which means that the suggestions from the models for medication can be different. Furthermore we introduce quadratic logistic terms to the competition model to simulate the tumor growth in the environment with a finite carrying capacity considering the nutrients or inhibitors. The simulations show that the tumor growth can reach an equilibrium state or an oscillatory state with the net growth rate of AI cells being androgen independent. Our results suggest that the competition and the restraint of a limited environment can enhance the possibility of relapse prevention.},
  language = {en},
  journal = {Journal of Theoretical Biology},
  doi = {10.1016/j.jtbi.2016.05.033},
  author = {Yang, Jing and Zhao, Tong-Jun and Yuan, Chang-Qing and Xie, Jing-Hui and Hao, Fang-Fang},
  month = sep,
  year = {2016},
  pages = {66-72},
  file = {/home/asher/Zotero/storage/UMRXWZEP/Yang et al. - 2016 - A nonlinear competitive model of the prostate tumo.pdf}
}

@article{laird1982,
  title = {Random-{{Effects Models}} for {{Longitudinal Data}}},
  volume = {38},
  number = {1},
  journal = {Biometrics},
  author = {Laird, Nan M. and Ware, James H.},
  month = dec,
  year = {1982},
  pages = {963-974},
  file = {/home/asher/Zotero/storage/EVSLVV2P/random_effect.pdf}
}

@inproceedings{caruana2015,
  address = {{Sydney, NSW, Australia}},
  title = {Intelligible {{Models}} for {{HealthCare}}: {{Predicting Pneumonia Risk}} and {{Hospital}} 30-Day {{Readmission}}},
  isbn = {978-1-4503-3664-2},
  shorttitle = {Intelligible {{Models}} for {{HealthCare}}},
  abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
  language = {en},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '15},
  publisher = {{ACM Press}},
  doi = {10.1145/2783258.2788613},
  author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  year = {2015},
  keywords = {interpretable machine learning},
  pages = {1721-1730},
  file = {/home/asher/Zotero/storage/W73V2F6P/Caruana et al. - 2015 - Intelligible Models for HealthCare Predicting Pne.pdf}
}

@inproceedings{lou2012,
  address = {{Beijing, China}},
  title = {Intelligible Models for Classification and Regression},
  isbn = {978-1-4503-1462-6},
  abstract = {Complex models for regression and classification have high accuracy, but are unfortunately no longer interpretable by users. We study the performance of generalized additive models (GAMs), which combine single-feature models called shape functions through a linear function. Since the shape functions can be arbitrarily complex, GAMs are more accurate than simple linear models. But since they do not contain any interactions between features, they can be easily interpreted by users.},
  language = {en},
  booktitle = {Proceedings of the 18th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '12},
  publisher = {{ACM Press}},
  doi = {10.1145/2339530.2339556},
  author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes},
  year = {2012},
  keywords = {interpretable machine learning},
  pages = {150},
  file = {/home/asher/Zotero/storage/IXVA8XXE/Lou et al. - 2012 - Intelligible models for classification and regress.pdf}
}

@article{molnar2019,
  title = {Interpretable {{Machine Learning}}},
  language = {en},
  author = {Molnar, Christoph},
  month = jul,
  year = {2019},
  keywords = {interpretable machine learning},
  pages = {251},
  file = {/home/asher/Zotero/storage/JZ4I2U37/Molnar - Interpretable Machine Learning.pdf}
}

@article{miller2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07269},
  primaryClass = {cs},
  title = {Explanation in {{Artificial Intelligence}}: {{Insights}} from the {{Social Sciences}}},
  shorttitle = {Explanation in {{Artificial Intelligence}}},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  journal = {arXiv:1706.07269 [cs]},
  author = {Miller, Tim},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,interpretable machine learning},
  file = {/home/asher/Zotero/storage/8EQ8ZPDE/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf;/home/asher/Zotero/storage/ARCZ5IT9/1706.html}
}

@article{hickey2018a,
  title = {Joint {{Models}} of {{Longitudinal}} and {{Time}}-to-{{Event Data}} with {{More Than One Event Time Outcome}}: {{A Review}}},
  volume = {14},
  shorttitle = {Joint {{Models}} of {{Longitudinal}} and {{Time}}-to-{{Event Data}} with {{More Than One Event Time Outcome}}},
  abstract = {Methodological development and clinical application of joint models of longitudinal and time-to-event outcomes have grown substantially over the past two decades. However, much of this research has concentrated on a single longitudinal outcome and a single event time outcome. In clinical and public health research, patients who are followed up over time may often experience multiple, recurrent, or a succession of clinical events. Models that utilise such multivariate event time outcomes are quite valuable in clinical decision-making. We comprehensively review the literature for implementation of joint models involving more than a single event time per subject. We consider the distributional and modelling assumptions, including the association structure, estimation approaches, software implementations, and clinical applications. Research into this area is proving highly promising, but to-date remains in its infancy.},
  number = {1},
  journal = {The International Journal of Biostatistics},
  doi = {10.1515/ijb-2017-0047},
  author = {Hickey, Graeme L. and Philipson, Pete and Jorgensen, Andrea and {Kolamunnage-Dona}, Ruwanthi},
  year = {2018},
  keywords = {Joint models,longitudinal data,multivariate data,recurrent events,time-to-event data},
  file = {/home/asher/Zotero/storage/TASU5HG3/Hickey et al. - 2018 - Joint Models of Longitudinal and Time-to-Event Dat.pdf}
}

@article{song2002,
  title = {An Estimator for the Proportional Hazards Model with Multiple Longitudinal Covariates Measured with Error},
  volume = {3},
  issn = {1465-4644},
  abstract = {Abstract.  In many longitudinal studies, it is of interest to characterize the relationship between a time-to-event (e.g. survival) and several time-dependent a},
  language = {en},
  number = {4},
  journal = {Biostatistics},
  doi = {10.1093/biostatistics/3.4.511},
  author = {Song, Xiao and Davidian, Marie and Tsiatis, Anastasios A.},
  month = dec,
  year = {2002},
  pages = {511-528},
  file = {/home/asher/Zotero/storage/739D8Z6I/Song et al. - 2002 - An estimator for the proportional hazards model wi.pdf;/home/asher/Zotero/storage/N27FIC7H/294606.html}
}

@article{williamson2008,
  title = {Joint Modelling of Longitudinal and Competing Risks Data},
  volume = {27},
  copyright = {Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  issn = {1097-0258},
  abstract = {Available methods for joint modelling of longitudinal and survival data typically have only one failure type for the time to event outcome. We extend the methodology to allow for competing risks data. We fit a cause-specific hazards sub-model to allow for competing risks, with a separate latent association between longitudinal measurements and each cause of failure. The method is applied to data from the SANAD trial of anti-epileptic drugs (AEDs), as a means of investigating the effect of drug titration on the relative effects of lamotrigine (LTG) and carbamazepine (CBZ) on treatment failure. Concern had been expressed that differential titration rates may have been to the disadvantage of CBZ. The beneficial effect of LTG on unacceptable adverse events leading to drug withdrawal did not lessen and indeed increased slightly when a calibrated dose was accounted for in the joint model. Adjustment for the titration rate of LTG relative to CBZ resulted in an unchanged effect of the former on drug withdrawals due to inadequate seizure control. LTG remains the AED of choice from this analysis. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {30},
  journal = {Statistics in Medicine},
  doi = {10.1002/sim.3451},
  author = {Williamson, P. R. and Kolamunnage-Dona, R. and Philipson, P. and Marson, A. G.},
  year = {2008},
  keywords = {longitudinal data,anti-epileptic drugs,competing risks,drug titration,joint modelling},
  pages = {6426-6438},
  file = {/home/asher/Zotero/storage/USW8NDGD/Williamson et al. - 2008 - Joint modelling of longitudinal and competing risk.pdf;/home/asher/Zotero/storage/8KSSP3G3/sim.html}
}

@article{liu2009,
  title = {Joint Analysis of Correlated Repeated Measures and Recurrent Events Processes in the Presence of Death, with Application to a Study on Acquired Immune Deficiency Syndrome},
  volume = {58},
  copyright = {\textcopyright{} 2009 Royal Statistical Society},
  issn = {1467-9876},
  abstract = {Summary. In many longitudinal studies, we observe two correlated processes: a repeated measures process and a recurrent events process, both subject to a dependent terminal event. For example, in the `Terry Beirn community programs for clinical research on AIDS' (CPCRA) study, higher CD4 cell counts are associated with lower risk of recurrent opportunistic diseases. They are also correlated with mortality, e.g. higher CD4 cell repeated measures and a lower rate of opportunistic disease imply better survival for patients infected with the human immunodeficiency virus. We propose a joint random-effects model for the three correlated outcomes. The correlation is modelled by conditioning on shared random effects. Covariate effects can be taken into account in the model. Maximum likelihood estimation and inference are carried out through a Gaussian quadrature technique, assuming piecewise constant baseline hazards for recurrent events and death. The model can be fitted conveniently by Gaussian quadrature tools, e.g. SAS procedure NLMIXED. Simulation studies show that the estimation method yields satisfactory results. We apply this method to the CPCRA data.},
  language = {en},
  number = {1},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  doi = {10.1111/j.1467-9876.2008.00641.x},
  author = {Liu, Lei and Huang, Xuelin},
  year = {2009},
  keywords = {Frailty model,Informative censoring,Longitudinal data analysis,Mixed effects model,Proportional hazards model},
  pages = {65-81},
  file = {/home/asher/Zotero/storage/NC34ES8K/Liu and Huang - 2009 - Joint analysis of correlated repeated measures and.pdf;/home/asher/Zotero/storage/JP6GQG5K/j.1467-9876.2008.00641.html}
}

@article{chi2006,
  title = {Joint {{Models}} for {{Multivariate Longitudinal}} and {{Multivariate Survival Data}}},
  volume = {62},
  issn = {1541-0420},
  abstract = {Joint modeling of longitudinal and survival data is becoming increasingly essential in most cancer and AIDS clinical trials. We propose a likelihood approach to extend both longitudinal and survival components to be multidimensional. A multivariate mixed effects model is presented to explicitly capture two different sources of dependence among longitudinal measures over time as well as dependence between different variables. For the survival component of the joint model, we introduce a shared frailty, which is assumed to have a positive stable distribution, to induce correlation between failure times. The proposed marginal univariate survival model, which accommodates both zero and nonzero cure fractions for the time to event, is then applied to each marginal survival function. The proposed multivariate survival model has a proportional hazards structure for the population hazard, conditionally as well as marginally, when the baseline covariates are specified through a specific mechanism. In addition, the model is capable of dealing with survival functions with different cure rate structures. The methodology is specifically applied to the International Breast Cancer Study Group (IBCSG) trial to investigate the relationship between quality of life, disease-free survival, and overall survival.},
  language = {en},
  number = {2},
  journal = {Biometrics},
  doi = {10.1111/j.1541-0420.2005.00448.x},
  author = {Chi, Yueh-Yun and Ibrahim, Joseph G.},
  year = {2006},
  keywords = {Bayesian inference,Breast cancer clinical trial,Cure rate model,Joint modeling,Positive stable law,Random effects,Shared frailty},
  pages = {432-445},
  file = {/home/asher/Zotero/storage/BUS7TXD2/Chi and Ibrahim - 2006 - Joint Models for Multivariate Longitudinal and Mul.pdf;/home/asher/Zotero/storage/39W7EUFJ/j.1541-0420.2005.00448.html}
}

@article{sia2019,
  title = {Ollivier-{{Ricci Curvature}}-{{Based Method}} to {{Community Detection}} in {{Complex Networks}}},
  volume = {9},
  copyright = {2019 The Author(s)},
  issn = {2045-2322},
  abstract = {Identification of community structures in complex network is of crucial importance for understanding the system's function, organization, robustness and security. Here, we present a novel Ollivier-Ricci curvature (ORC) inspired approach to community identification in complex networks. We demonstrate that the intrinsic geometric underpinning of the ORC offers a natural approach to discover inherent community structures within a network based on interaction among entities. We develop an ORC-based community identification algorithm based on the idea of sequential removal of negatively curved edges symptomatic of high interactions (e.g., traffic, attraction). To illustrate and compare the performance with other community identification methods, we examine the ORC-based algorithm with stochastic block model artificial networks and real-world examples ranging from social to drug-drug interaction networks. The ORC-based algorithm is able to identify communities with either better or comparable performance accuracy and to discover finer hierarchical structures of the network. This opens new geometric avenues for analysis of complex networks dynamics.},
  language = {En},
  number = {1},
  journal = {Scientific Reports},
  doi = {10.1038/s41598-019-46079-x},
  author = {Sia, Jayson and Jonckheere, Edmond and Bogdan, Paul},
  month = jul,
  year = {2019},
  pages = {9800},
  file = {/home/asher/Zotero/storage/GX29XHGC/Sia et al. - 2019 - Ollivier-Ricci Curvature-Based Method to Community.pdf;/home/asher/Zotero/storage/5GPDCW76/s41598-019-46079-x.html}
}

@article{tang2019,
  title = {Gsslasso {{Cox}}: A {{Bayesian}} Hierarchical Model for Predicting Survival and Detecting Associated Genes by Incorporating Pathway Information},
  volume = {20},
  issn = {1471-2105},
  shorttitle = {Gsslasso {{Cox}}},
  abstract = {Group structures among genes encoded in functional relationships or biological pathways are valuable and unique features in large-scale molecular data for survival analysis. However, most of previous approaches for molecular data analysis ignore such group structures. It is desirable to develop powerful analytic methods for incorporating valuable pathway information for predicting disease survival outcomes and detecting associated genes.},
  number = {1},
  journal = {BMC Bioinformatics},
  doi = {10.1186/s12859-019-2656-1},
  author = {Tang, Zaixiang and Lei, Shufeng and Zhang, Xinyan and Yi, Zixuan and Guo, Boyi and Chen, Jake Y. and Shen, Yueping and Yi, Nengjun},
  month = feb,
  year = {2019},
  pages = {94},
  file = {/home/asher/Zotero/storage/BL4VTRN6/Tang et al. - 2019 - Gsslasso Cox a Bayesian hierarchical model for pr.pdf;/home/asher/Zotero/storage/XD2Q9I59/s12859-019-2656-1.html}
}

@article{gelman2014,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  volume = {24},
  issn = {0960-3174, 1573-1375},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  language = {en},
  number = {6},
  journal = {Statistics and Computing},
  doi = {10.1007/s11222-013-9416-2},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  month = nov,
  year = {2014},
  pages = {997-1016},
  file = {/home/asher/Zotero/storage/FNJH96B7/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf}
}

@article{zhou,
  title = {Bayesian {{Model Selection}} in Terms of {{Kullback}}-{{Leibler}} Discrepancy},
  language = {en},
  author = {Zhou, Shouhao},
  pages = {91},
  file = {/home/asher/Zotero/storage/H6PMVE3L/Zhou - Bayesian Model Selection in terms of Kullback-Leib.pdf}
}

@article{vehtari2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.04544},
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  volume = {27},
  issn = {0960-3174, 1573-1375},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  language = {en},
  number = {5},
  journal = {Statistics and Computing},
  doi = {10.1007/s11222-016-9696-4},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  month = sep,
  year = {2017},
  keywords = {Statistics - Methodology,Statistics - Computation},
  pages = {1413-1432},
  file = {/home/asher/Zotero/storage/BK77BD8S/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf}
}

@article{youssef,
  title = {A {{Review On Optimal Experimental Design}}},
  language = {en},
  author = {Youssef, Noha A},
  pages = {7},
  file = {/home/asher/Zotero/storage/R6TSIWNN/Youssef - A Review On Optimal Experimental Design.pdf}
}

@article{chang,
  title = {{{SOME OPTIMAL NESTED ROW}}-{{COLUMN DESIGNS}}},
  abstract = {We consider experiments run in blocks with two way heterog block. The goal is to find "optimal" designs for estimating treatment this paper, a general method for constructing universally optimal neste designs within the class of treatment connected designs is given. Also, a row-column designs, which has a completely symmetric information m not have maximum trace among all possible designs, is shown to have for some a.},
  language = {en},
  author = {Chang, Jane Y and Notz, William I},
  pages = {16},
  file = {/home/asher/Zotero/storage/8KHVVH6N/Chang and Notz - SOME OPTIMAL NESTED ROW-COLUMN DESIGNS.pdf}
}

@article{vandenberg2003,
  title = {Optimal Nonlinear {{Bayesian}} Experimental Design: An Application to Amplitude versus Offset Experiments},
  volume = {155},
  issn = {0956540X, 1365246X},
  shorttitle = {Optimal Nonlinear {{Bayesian}} Experimental Design},
  abstract = {When designing an experiment, the aim is usually to find the design which minimizes expected post-experimental uncertainties on the model parameters. Classical methods for experimental design are shown to fail in nonlinear problems because they incorporate linearized design criteria. A more fundamental criterion is introduced which, in principle, can be used to design any nonlinear problem. The criterion is entropy-based and depends on the calculation of marginal probability distributions. In turn, this requires the numerical calculation of integrals for which we use Monte Carlo sampling. The choice of discretization in the parameter/data space strongly influences the number of samples required. Thus, the only practical limitation for this technique appears to be computational power. A synthetic experiment with an oscillatory, highly nonlinear parameter\textendash{}data relationship and a simple seismic amplitude versus offset (AVO) experiment are used to demonstrate the method. Interestingly, in our AVO example, although overly coarse discretizations lead to incorrect evaluation of the entropy, the optimal design remains unchanged.},
  language = {en},
  number = {2},
  journal = {Geophysical Journal International},
  doi = {10.1046/j.1365-246X.2003.02048.x},
  author = {{van den Berg}, Jojanneke and Curtis, Andrew and Trampert, Jeannot},
  month = nov,
  year = {2003},
  pages = {411-421},
  file = {/home/asher/Zotero/storage/LXGFY7LP/van den Berg et al. - 2003 - Optimal nonlinear Bayesian experimental design an.pdf}
}

@article{ryan2003,
  title = {Estimating {{Expected Information Gains}} for {{Experimental Designs With Application}} to the {{Random Fatigue}}-{{Limit Model}}},
  volume = {12},
  issn = {1061-8600, 1537-2715},
  language = {en},
  number = {3},
  journal = {Journal of Computational and Graphical Statistics},
  doi = {10.1198/1061860032012},
  author = {Ryan, Kenneth J},
  month = sep,
  year = {2003},
  pages = {585-603},
  file = {/home/asher/Zotero/storage/K2V3D88L/Ryan - 2003 - Estimating Expected Information Gains for Experime.pdf}
}

@article{marrugo-ramirez2018,
  title = {Blood-{{Based Cancer Biomarkers}} in {{Liquid Biopsy}}: {{A Promising Non}}-{{Invasive Alternative}} to {{Tissue Biopsy}}},
  volume = {19},
  issn = {1422-0067},
  shorttitle = {Blood-{{Based Cancer Biomarkers}} in {{Liquid Biopsy}}},
  abstract = {Cancer is one of the greatest threats facing our society, being the second leading cause of death globally. Currents strategies for cancer diagnosis consist of the extraction of a solid tissue from the affected area. This sample enables the study of specific biomarkers and the genetic nature of the tumor. However, the tissue extraction is risky and painful for the patient and in some cases is unavailable in inaccessible tumors. Moreover, a solid biopsy is expensive and time consuming and cannot be applied repeatedly. New alternatives that overcome these drawbacks are rising up nowadays, such as liquid biopsy. A liquid biopsy is the analysis of biomarkers in a non-solid biological tissue, mainly blood, which has remarkable advantages over the traditional method; it has no risk, it is non-invasive and painless, it does not require surgery and reduces cost and diagnosis time. The most studied cancer non-invasive biomarkers are circulating tumor cells (CTCs), circulating tumor DNA (ctDNA), and exosomes. These circulating biomarkers play a key role in the understanding of metastasis and tumorigenesis, which could provide a better insight into the evolution of the tumor dynamics during treatment and disease progression. Improvements in isolation technologies, based on a higher grade of purification of CTCs, exosomes, and ctDNA, will provide a better characterization of biomarkers and give rise to a wide range of clinical applications, such as early detection of diseases, and the prediction of treatment responses due to the discovery of personalized tumor-related biomarkers.},
  number = {10},
  journal = {International Journal of Molecular Sciences},
  doi = {10.3390/ijms19102877},
  author = {{Marrugo-Ram{\'i}rez}, Jos{\'e} and Mir, M{\`o}nica and Samitier, Josep},
  month = sep,
  year = {2018},
  file = {/home/asher/Zotero/storage/JTFFBYSR/Marrugo-Ramírez et al. - 2018 - Blood-Based Cancer Biomarkers in Liquid Biopsy A .pdf},
  pmid = {30248975},
  pmcid = {PMC6213360}
}

@article{chiorean2016,
  title = {{{CA19}}-9 Decrease at 8 Weeks as a Predictor of Overall Survival in a Randomized Phase {{III}} Trial ({{MPACT}}) of Weekly {\emph{Nab}} -Paclitaxel plus Gemcitabine versus Gemcitabine Alone in Patients with Metastatic Pancreatic Cancer},
  volume = {27},
  issn = {0923-7534, 1569-8041},
  abstract = {Background: A phase I/II study and subsequent phase III study (MPACT) reported significant correlations between CA19-9 decreases and prolonged overall survival (OS) with nab-paclitaxel plus gemcitabine (nab-P + Gem) treatment for metastatic pancreatic cancer (MPC). CA19-9 changes at week 8 and potential associations with efficacy were investigated as part of an exploratory analysis in the MPACT trial. Patients and methods: Untreated patients with MPC (N = 861) received nab-P + Gem or Gem alone. CA19-9 was evaluated at baseline and every 8 weeks.
Results: Patients with baseline and week-8 CA19-9 measurements were analyzed (nab-P + Gem: 252; Gem: 202). In an analysis pooling the treatments, patients with any CA19-9 decline (80\%) versus those without (20\%) had improved OS (median 11.1 versus 8.0 months; P = 0.005). In the nab-P + Gem arm, patients with (n = 206) versus without (n = 46) any CA19-9 decrease at week 8 had a confirmed overall response rate (ORR) of 40\% versus 13\%, and a median OS of 13.2 versus 8.3 months (P = 0.001), respectively. In the Gem-alone arm, patients with (n = 159) versus without (n = 43) CA199 decrease at week 8 had a confirmed ORR of 15\% versus 5\%, and a median OS of 9.4 versus 7.1 months (P = 0.404), respectively. In the nab-P + Gem and Gem-alone arms, by week 8, 16\% (40/252) and 6\% (13/202) of patients, respectively, had an unconfirmed radiologic response (median OS 13.7 and 14.7 months, respectively), and 79\% and 84\% of patients, respectively, had stable disease (SD) (median OS 11.1 and 9 months, respectively). Patients with SD and any CA19-9 decrease (158/199 and 133/170) had a median OS of 13.2 and 9.4 months, respectively.
Conclusion: This analysis demonstrated that, in patients with MPC, any CA19-9 decrease at week 8 can be an early marker for chemotherapy efficacy, including in those patients with SD. CA19-9 decrease identified more patients with survival benefit than radiologic response by week 8.},
  language = {en},
  number = {4},
  journal = {Annals of Oncology},
  doi = {10.1093/annonc/mdw006},
  author = {Chiorean, E. G. and Von Hoff, D. D. and Reni, M. and Arena, F. P. and Infante, J. R. and Bathini, V. G. and Wood, T. E. and Mainwaring, P. N. and Muldoon, R. T. and Clingan, P. R. and Kunzmann, V. and Ramanathan, R. K. and Tabernero, J. and Goldstein, D. and McGovern, D. and Lu, B. and Ko, A.},
  month = apr,
  year = {2016},
  pages = {654-660},
  file = {/home/asher/Zotero/storage/VEXTY3TJ/Chiorean et al. - 2016 - CA19-9 decrease at 8 weeks as a predictor of overa.pdf}
}

@article{archer2013,
  title = {Bayesian and {{Quasi}}-{{Bayesian Estimators}} for {{Mutual Information}} from {{Discrete Data}}},
  volume = {15},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  abstract = {Mutual information (MI) quantifies the statistical dependency between a pair of random variables, and plays a central role in the analysis of engineering and biological systems. Estimation of MI is difficult due to its dependence on an entire joint distribution, which is difficult to estimate from samples. Here we discuss several regularized estimators for MI that employ priors based on the Dirichlet distribution. First, we discuss three ``quasi-Bayesian'' estimators that result from linear combinations of Bayesian estimates for conditional and marginal entropies. We show that these estimators are not in fact Bayesian, and do not arise from a well-defined posterior distribution and may in fact be negative. Second, we show that a fully Bayesian MI estimator proposed by Hutter (2002), which relies on a fixed Dirichlet prior, exhibits strong prior dependence and has large bias for small datasets. Third, we formulate a novel Bayesian estimator using a mixture-of-Dirichlets prior, with mixing weights designed to produce an approximately flat prior over MI. We examine the performance of these estimators with a variety of simulated datasets and show that, surprisingly, quasi-Bayesian estimators generally outperform our Bayesian estimator. We discuss outstanding challenges for MI estimation and suggest promising avenues for future research.},
  language = {en},
  number = {5},
  journal = {Entropy},
  doi = {10.3390/e15051738},
  author = {Archer, Evan and Park, Il Memming and Pillow, Jonathan W.},
  month = may,
  year = {2013},
  keywords = {Bayes least squares,Dirichlet distribution,entropy,mutual information},
  pages = {1738-1755},
  file = {/home/asher/Zotero/storage/C9AWREE2/Archer et al. - 2013 - Bayesian and Quasi-Bayesian Estimators for Mutual .pdf;/home/asher/Zotero/storage/LCAV4Y65/1738.html}
}

@article{wolpert2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.4548},
  title = {Estimating {{Functions}} of {{Distributions Defined}} over {{Spaces}} of {{Unknown Size}}},
  volume = {15},
  issn = {1099-4300},
  abstract = {We consider Bayesian estimation of information-theoretic quantities from data, using a Dirichlet prior. Acknowledging the uncertainty of the event space size \$m\$ and the Dirichlet prior's concentration parameter \$c\$, we treat both as random variables set by a hyperprior. We show that the associated hyperprior, \$P(c, m)\$, obeys a simple "Irrelevance of Unseen Variables" (IUV) desideratum iff \$P(c, m) = P(c) P(m)\$. Thus, requiring IUV greatly reduces the number of degrees of freedom of the hyperprior. Some information-theoretic quantities can be expressed multiple ways, in terms of different event spaces, e.g., mutual information. With all hyperpriors (implicitly) used in earlier work, different choices of this event space lead to different posterior expected values of these information-theoretic quantities. We show that there is no such dependence on the choice of event space for a hyperprior that obeys IUV. We also derive a result that allows us to exploit IUV to greatly simplify calculations, like the posterior expected mutual information or posterior expected multi-information. We also use computer experiments to favorably compare an IUV-based estimator of entropy to three alternative methods in common use. We end by discussing how seemingly innocuous changes to the formalization of an estimation problem can substantially affect the resultant estimates of posterior expectations.},
  number = {12},
  journal = {Entropy},
  doi = {10.3390/e15114668},
  author = {Wolpert, David H. and DeDeo, Simon},
  month = oct,
  year = {2013},
  keywords = {Mathematics - Statistics Theory,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Quantitative Methods},
  pages = {4668-4699},
  file = {/home/asher/Zotero/storage/IPHSKTV3/Wolpert and DeDeo - 2013 - Estimating Functions of Distributions Defined over.pdf;/home/asher/Zotero/storage/IQPWBKNC/1311.html}
}

@article{wolpert1995,
  title = {Estimating Functions of Probability Distributions from a Finite Set of Samples},
  volume = {52},
  issn = {1063-651X, 1095-3787},
  language = {en},
  number = {6},
  journal = {Physical Review E},
  doi = {10.1103/PhysRevE.52.6841},
  author = {Wolpert, David H. and Wolf, David R.},
  month = dec,
  year = {1995},
  pages = {6841-6854},
  file = {/home/asher/Zotero/storage/M7VZTNA8/Wolpert and Wolf - 1995 - Estimating functions of probability distributions .pdf}
}

@article{kristoffersen2018,
  title = {Observed to Expected or Logistic Regression to Identify Hospitals with High or Low 30-Day Mortality?},
  volume = {13},
  issn = {1932-6203},
  abstract = {Introduction A common quality indicator for monitoring and comparing hospitals is based on death within 30 days of admission. An important use is to determine whether a hospital has higher or lower mortality than other hospitals. Thus, the ability to identify such outliers correctly is essential. Two approaches for detection are: 1) calculating the ratio of observed to expected number of deaths (OE) per hospital and 2) including all hospitals in a logistic regression (LR) comparing each hospital to a form of average over all hospitals. The aim of this study was to compare OE and LR with respect to correctly identifying 30-day mortality outliers. Modifications of the methods, i.e., variance corrected approach of OE (OE-Faris), bias corrected LR (LR-Firth), and trimmed mean variants of LR and LR-Firth were also studied. Materials and methods To study the properties of OE and LR and their variants, we performed a simulation study by generating patient data from hospitals with known outlier status (low mortality, high mortality, non-outlier). Data from simulated scenarios with varying number of hospitals, hospital volume, and mortality outlier status, were analysed by the different methods and compared by level of significance (ability to falsely claim an outlier) and power (ability to reveal an outlier). Moreover, administrative data for patients with acute myocardial infarction (AMI), stroke, and hip fracture from Norwegian hospitals for 2012\textendash{}2014 were analysed. Results None of the methods achieved the nominal (test) level of significance for both low and high mortality outliers. For low mortality outliers, the levels of significance were increased four- to fivefold for OE and OE-Faris. For high mortality outliers, OE and OE-Faris, LR 25\% trimmed and LR-Firth 10\% and 25\% trimmed maintained approximately the nominal level. The methods agreed with respect to outlier status for 94.1\% of the AMI hospitals, 98.0\% of the stroke, and 97.8\% of the hip fracture hospitals. Conclusion We recommend, on the balance, LR-Firth 10\% or 25\% trimmed for detection of both low and high mortality outliers.},
  language = {en},
  number = {4},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0195248},
  author = {Kristoffersen, Doris Tove and Helgeland, Jon and {Clench-Aas}, Jocelyne and Laake, Petter and Veier{\o}d, Marit B.},
  month = apr,
  year = {2018},
  keywords = {Age distribution,Death rates,Hip,Hospitals,Myocardial infarction,Norwegian people,Simulation and modeling,Test statistics},
  pages = {e0195248},
  file = {/home/asher/Zotero/storage/YRLQ43XL/Kristoffersen et al. - 2018 - Observed to expected or logistic regression to ide.pdf;/home/asher/Zotero/storage/BDYY6Q55/article.html}
}

@article{kunstner2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.12558},
  primaryClass = {cs, stat},
  title = {Limitations of the {{Empirical Fisher Approximation}}},
  abstract = {Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher\textemdash{}unlike the Fisher\textemdash{}does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.},
  language = {en},
  journal = {arXiv:1905.12558 [cs, stat]},
  author = {Kunstner, Frederik and Balles, Lukas and Hennig, Philipp},
  month = may,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/K3GV2AZX/Kunstner et al. - 2019 - Limitations of the Empirical Fisher Approximation.pdf}
}

@article{kingma2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  journal = {arXiv:1412.6980 [cs]},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Machine Learning},
  file = {/home/asher/Zotero/storage/Y698PTRD/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/home/asher/Zotero/storage/AHEUCX83/1412.html}
}

@article{wand2007,
  title = {Fisher Information for Generalised Linear Mixed Models},
  volume = {98},
  issn = {0047259X},
  abstract = {The Fisher information for the canonical link exponential family generalised linear mixed model is derived. The contribution from the fixed effects parameters is shown to have a particularly simple form. \textcopyright{} 2007 Elsevier Inc. All rights reserved.},
  language = {en},
  number = {7},
  journal = {Journal of Multivariate Analysis},
  doi = {10.1016/j.jmva.2007.01.001},
  author = {Wand, M.P.},
  month = aug,
  year = {2007},
  pages = {1412-1416},
  file = {/home/asher/Zotero/storage/HL55CBUQ/Wand - 2007 - Fisher information for generalised linear mixed mo.pdf}
}

@article{dequidt2018,
  title = {Measuring and {{Bounding Experimenter Demand}}},
  volume = {108},
  issn = {0002-8282},
  language = {en},
  number = {11},
  journal = {American Economic Review},
  doi = {10.1257/aer.20171330},
  author = {{de Quidt}, Jonathan and Haushofer, Johannes and Roth, Christopher},
  month = nov,
  year = {2018},
  pages = {3266-3302},
  file = {/home/asher/Zotero/storage/5TLTIUWB/de Quidt et al. - 2018 - Measuring and Bounding Experimenter Demand.pdf}
}

@article{nielsen2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.08271},
  primaryClass = {cs, math, stat},
  title = {An Elementary Introduction to Information Geometry},
  abstract = {We describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some uses of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry with proofs omitted for brevity.},
  journal = {arXiv:1808.08271 [cs, math, stat]},
  author = {Nielsen, Frank},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Information Theory},
  file = {/home/asher/Zotero/storage/TMZG2C4D/Nielsen - 2018 - An elementary introduction to information geometry.pdf;/home/asher/Zotero/storage/E32VEYV3/1808.html}
}

@article{skilling2004,
  title = {Nested {{Sampling}}},
  volume = {735},
  number = {1},
  journal = {AIP Conference Proceedings},
  doi = {10.1063/1.1835238},
  author = {Skilling, John},
  month = nov,
  year = {2004},
  pages = {395},
  file = {/home/asher/Zotero/storage/UJ8RDD6H/Skilling - 2004 - Nested Sampling.pdf}
}

@article{gilks1992,
  title = {Adaptive {{Rejection Sampling}} for {{Gibbs Sampling}}},
  volume = {41},
  issn = {00359254},
  abstract = {We propose a method for rejection sampling from any univariate log-concave probability density function. The method is adaptive: as sampling proceeds, the rejection envelope and the squeezing function converge to the density function. The rejection envelope and squeezing function are piecewise exponential functions, the rejection envelope touching the density at previously sampled points, and the squeezing function forming arcs between those points of contact. The technique is intended for situations where evaluation of the density is computationally expensive, in particular for applications of Gibbs sampling to Bayesian models with non-conjugacy. We apply the technique to a Gibbs sampling analysis of monoclonal antibody reactivity.},
  language = {en},
  number = {2},
  journal = {Applied Statistics},
  doi = {10.2307/2347565},
  author = {Gilks, W. R. and Wild, P.},
  year = {1992},
  pages = {337},
  file = {/home/asher/Zotero/storage/AKCVE5CP/Gilks and Wild - 1992 - Adaptive Rejection Sampling for Gibbs Sampling.pdf}
}

@article{gelman1992,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  volume = {7},
  issn = {0883-4237, 2168-8745},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  language = {EN},
  number = {4},
  journal = {Statistical Science},
  doi = {10.1214/ss/1177011136},
  author = {Gelman, Andrew and Rubin, Donald B.},
  month = nov,
  year = {1992},
  keywords = {Bayesian inference,convergence of stochastic processes,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  pages = {457-472},
  file = {/home/asher/Zotero/storage/R2K9H6NZ/Gelman and Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf;/home/asher/Zotero/storage/NN3T9JA7/1177011136.html}
}

@article{metropolis1949,
  title = {The {{Monte Carlo Method}}},
  volume = {44},
  issn = {0162-1459, 1537-274X},
  language = {en},
  number = {247},
  journal = {Journal of the American Statistical Association},
  doi = {10.1080/01621459.1949.10483310},
  author = {Metropolis, Nicholas and Ulam, S.},
  month = sep,
  year = {1949},
  pages = {335-341},
  file = {/home/asher/Zotero/storage/GV34PS9D/Metropolis and Ulam - 1949 - The Monte Carlo Method.pdf}
}

@article{ng2002,
  title = {On {{Discriminative}} vs. {{Generative Classifiers}}: {{A}} Comparison of Logistic Regression and Naive {{Bayes}}},
  abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
  language = {en},
  journal = {Advances in neural information processing systems},
  author = {Ng, Andrew Y and Jordan, Michael I},
  year = {2002},
  pages = {841--848},
  file = {/home/asher/Zotero/storage/M65CXYVW/Ng and Jordan - On Discriminative vs. Generative Classifiers A co.pdf}
}

@article{cox2008,
  title = {The Generalized {{{\emph{F}}}} Distribution: {{An}} Umbrella for Parametric Survival Analysis},
  volume = {27},
  issn = {02776715, 10970258},
  shorttitle = {The Generalized {{{\emph{F}}}} Distribution},
  abstract = {In a recent tutorial my colleagues and I advocated the generalized gamma (GG) distribution as a platform for parametric survival analysis. The GG family includes all four of the common types of hazard functions, making it particularly useful for estimating individual hazard functions as well as both relative hazards and relative times. In addition, the GG includes most of the commonly used parametric survival distributions. Survival analysis based on the GG distribution is practical since regression models are available in commonly used statistical packages. It is well known that the GG is contained in an even larger family, the generalized F (GF) distribution, which also includes the log logistic. The GF thus provides additional flexibility for parametric modeling. In this paper we discuss the GF family from this perspective. We provide a characterization of the hazard functions of the GF, showing that, except for the GG, the available hazard functions are limited to decreasing and arc-shaped hazards and, in particular, that the hazard function can be decreasing but not monotone. We also discuss fitting the GF with an alternative parameterization using standard statistical software and refine a description of the hazard functions for death after a diagnosis of clinical AIDS in four different eras of HIV therapy. Copyright q 2008 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {21},
  journal = {Statistics in Medicine},
  doi = {10.1002/sim.3292},
  author = {Cox, Christopher},
  month = sep,
  year = {2008},
  pages = {4301-4312},
  file = {/home/asher/Zotero/storage/XF9ZVQMH/Cox - 2008 - The generalized iFi distribution An umbrella.pdf}
}

@article{gelman2008,
  title = {A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models},
  volume = {2},
  issn = {1932-6157, 1941-7330},
  abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
  language = {EN},
  number = {4},
  journal = {The Annals of Applied Statistics},
  doi = {10.1214/08-AOAS191},
  author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
  month = dec,
  year = {2008},
  keywords = {Bayesian inference,hierarchical model,multilevel model,noninformative prior distribution,weakly informative prior distribution,generalized linear model,least squares,linear regression,logistic regression},
  pages = {1360-1383},
  file = {/home/asher/Zotero/storage/6A6RL8U5/Gelman et al. - 2008 - A weakly informative default prior distribution fo.pdf;/home/asher/Zotero/storage/Y6AXRTR3/1231424214.html}
}

@article{hampel1998,
  title = {Is Statistics Too Difficult?},
  volume = {26},
  copyright = {Copyright \textcopyright{} 1998 Statistical Society of Canada},
  issn = {1708-945X},
  abstract = {By means of several historical examples, it is shown that it does not appear to be easy to build bridges between rigorous mathematics and reasonable data-analytic procedures for scientific measurements. After mentioning both some positive and some negative aspects of statistics, a formal framework for statistics is presented which contains the concept formation, derivation of results and interpretation of mathematical statistics as three essential steps. The difficulties especially of interpretation are shown for examples in several areas of statistics, such as asymptotics and robustness. Some problems of statistics in two subject-matter sciences are discussed, and a summary and outlook are given.},
  language = {en},
  number = {3},
  journal = {Canadian Journal of Statistics},
  doi = {10.2307/3315772},
  author = {Hampel, Frank and Zurich, Eth},
  year = {1998},
  keywords = {astronomy,asymptotics,field ornithology,interpretation of mathematical results,long-range correlations,mathematical and applied statistics,model choice,robust statistics,statistical paradigms,statistics as subject-matter science,Uses and misuses of statistics},
  pages = {497-513},
  file = {/home/asher/Zotero/storage/B8WFYPGB/Hampel and Zurich - 1998 - Is statistics too difficult.pdf;/home/asher/Zotero/storage/WBU9W8D2/3315772.html}
}

@article{ghosh2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.07170},
  primaryClass = {stat},
  title = {On the {{Use}} of {{Cauchy Prior Distributions}} for {{Bayesian Logistic Regression}}},
  abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on P\textasciiacute{}olya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package in the supplement. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
  language = {en},
  journal = {arXiv:1507.07170 [stat]},
  author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
  month = jul,
  year = {2015},
  keywords = {Statistics - Methodology},
  file = {/home/asher/Zotero/storage/BWE6CT74/Ghosh et al. - 2015 - On the Use of Cauchy Prior Distributions for Bayes.pdf}
}

@article{galiani,
  title = {The Synth\_runner {{Package}}: {{Utilities}} to {{Automate Synthetic Control Estimation Using}}},
  abstract = {The Synthetic Control Methodology (Abadie and Gardeazabal, 2003; Abadie et al., 2010) allows for a data-driven approach to small-sample comparative studies. synth\_runner automates the process of running multiple synthetic control estimations using synth. It conducts placebo estimates in-space (estimations for the same treatment period but on all the control units). Inference (p-values) is provided by comparing the estimated main effect to the distribution of placebo effects. It allows several units to receive treatment, possibly at different time periods. Automatically generating the outcome predictors and diagnostics by splitting the pretreatment into training and validation portions is allowed. Additionally, it provides diagnostics to assess fit and generates visualizations of results.},
  language = {en},
  author = {Galiani, Sebastian and Quistorff, Brian},
  pages = {16},
  file = {/home/asher/Zotero/storage/YLGIMSM6/Galiani and Quistorff - The synth_runner Package Utilities to Automate Sy.pdf}
}

@article{hey2015,
  title = {Complex {{Underdetermination}} and the {{Units}} of {{Clinical Translation}}},
  volume = {30},
  issn = {2171-679X, 0495-4548},
  abstract = {What makes a high-quality biomarker experiment? The success of personalized medicine hinges on the answer to this question. In this paper, I argue that judgment about the quality of biomarker experiments is mediated by the problem of theoretical underdetermination. That is, the network of biological and pathophysiological theories motivating a biomarker experiment is sufficiently complicated that it often frustrates valid interpretation of the experimental results. Drawing on a case-study in biomarker diagnostic development from neurooncology, I argue that this problem of underdetermination can be overcome with greater coordination across the biomarker research trajectory. I then sketch an account for how coordination across a research trajectory can be evaluated. I ultimate conclude that what makes a high-quality biomarker experiment must be judged by the epistemic contribution it makes to this coordinated research effort.},
  language = {en},
  number = {2},
  journal = {THEORIA. An International Journal for Theory, History and Foundations of Science},
  doi = {10.1387/theoria.12697},
  author = {Hey, Spencer Phillips},
  month = jun,
  year = {2015},
  pages = {207},
  file = {/home/asher/Zotero/storage/9KBG4A6L/Hey - 2015 - Complex Underdetermination and the Units of Clinic.pdf}
}

@techreport{swat2017,
  title = {{{ProbOnto}} 2.5: {{Ontology}} and {{Knowledge Base}} of {{Probability Distributions}}},
  institution = {{Drug Disease Model Resources}},
  author = {Swat, Macie J. and Grenon, Pierre and Wimalaratne, Sarala},
  month = jan,
  year = {2017},
  file = {/home/asher/Zotero/storage/RGTHBNGE/Swat et al. - ProbOnto 2.5 Ontology and Knowledge Base of Proba.pdf}
}

@article{swat2016,
  title = {{{ProbOnto}}: Ontology and Knowledge Base of Probability Distributions},
  volume = {32},
  issn = {1367-4803},
  shorttitle = {{{ProbOnto}}},
  abstract = {Motivation: Probability distributions play a central role in mathematical and statistical modelling. The encoding, annotation and exchange of such models could be greatly simplified by a resource providing a common reference for the definition of probability distributions. Although some resources exist, no suitably detailed and complex ontology exists nor any database allowing programmatic access., Results: ProbOnto, is an ontology-based knowledge base of probability distributions, featuring more than 80 uni- and multivariate distributions with their defining functions, characteristics, relationships and re-parameterization formulas. It can be used for model annotation and facilitates the encoding of distribution-based models, related functions and quantities., Availability and Implementation:
http://probonto.org, Contact:
mjswat@ebi.ac.uk, Supplementary information:
Supplementary data are available at Bioinformatics online.},
  number = {17},
  journal = {Bioinformatics},
  doi = {10.1093/bioinformatics/btw170},
  author = {Swat, Maciej J. and Grenon, Pierre and Wimalaratne, Sarala},
  month = sep,
  year = {2016},
  pages = {2719-2721},
  file = {/home/asher/Zotero/storage/WNT4ND3V/Swat et al. - 2016 - ProbOnto ontology and knowledge base of probabili.pdf},
  pmid = {27153608},
  pmcid = {PMC5013898}
}

@book{gelman2007,
  address = {{Cambridge ; New York}},
  series = {Analytical Methods for Social Research},
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  isbn = {978-0-521-86706-1 978-0-521-68689-1},
  lccn = {HA31.3 .G45 2007},
  publisher = {{Cambridge University Press}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  keywords = {Multilevel models (Statistics),Regression analysis},
  file = {/home/asher/Zotero/storage/AVI6SF66/Gelman and Hill - 2007 - Data analysis using regression and multilevelhier.pdf},
  note = {OCLC: ocm67375137}
}

@inproceedings{pirolli1996,
  address = {{New York, NY, USA}},
  series = {{{CHI}} '96},
  title = {Scatter/{{Gather Browsing Communicates}} the {{Topic Structure}} of a {{Very Large Text Collection}}},
  isbn = {978-0-89791-777-3},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/238386.238489},
  author = {Pirolli, Peter and Schank, Patricia and Hearst, Marti and Diehl, Christine},
  year = {1996},
  keywords = {browsing,clustering,information retrieval,Scatter/Gather},
  pages = {213--220},
  file = {/home/asher/Zotero/storage/SH5E32MY/p148.pdf}
}

@incollection{dagum1993,
  title = {Forecasting {{Sleep Apnea}} with {{Dynamic Network Models}}},
  isbn = {978-1-4832-1451-1},
  abstract = {Dynamic network models \{DNMs\} are belief networks for temporal reasoning. The DNM methodology combines techniques from time\- series analysis and probabilistic reasoning to provide (1) a knowledge representation that integrates noncontemporaneous and contem\- poraneous dependencies and (2) methods for iteratively refining these dependencies in re\- sponse to the effects of exogenous influences. We use belief-network inference algorithms to perform forecasting, control, and discrete\- event simulation on DNMs. The belief\- network formulation allows us to move be\- yond the traditional assumptions of linearity in the relationships among time-dependent variables and of normality in their proba\- bility distributions. We demonstrate the DNM methodology on an important forecast\- ing problem in medicine. We conclude with a discussion of how the methodology addresses several limitations found in traditional time\- series analyses.},
  language = {en},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  publisher = {{Elsevier}},
  author = {Dagum, Paul and Galper, Adam},
  year = {1993},
  pages = {64-71},
  file = {/home/asher/Zotero/storage/PFGNEHHQ/Dagum and Galper - 1993 - Forecasting Sleep Apnea with Dynamic Network Model.pdf},
  doi = {10.1016/B978-1-4832-1451-1.50012-3}
}

@inproceedings{dagum1992,
  address = {{San Francisco, CA, USA}},
  series = {{{UAI}}'92},
  title = {Dynamic {{Network Models}} for {{Forecasting}}},
  isbn = {978-1-55860-258-8},
  abstract = {We have developed a probabilistic forecasting methodology through a synthesis of belief-network models and classical time-series analysis. We present the dynamic network model (DNM) and describe methods for constructing, refining, and performing inference with this representation of temporal probabilistic knowledge. The DNM representation extends static belief-network models to more general dynamic forecasting models by integrating and iteratively refining contemporaneous and time-lagged dependencies. We discuss key concepts in terms of a model for forecasting U.S. car sales in Japan.},
  booktitle = {Proceedings of the {{Eighth International Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  author = {Dagum, Paul and Galper, Adam and Horvitz, Eric},
  year = {1992},
  pages = {41--48},
  file = {/home/asher/Zotero/storage/NVRDLA9L/Dagum et al. - 1992 - Dynamic Network Models for Forecasting.pdf}
}

@article{wenzel2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.05532},
  primaryClass = {cs, stat},
  title = {Bayesian {{Nonlinear Support Vector Machines}} for {{Big Data}}},
  abstract = {We propose a fast inference method for Bayesian nonlinear support vector machines that leverages stochastic variational inference and inducing points. Our experiments show that the proposed method is faster than competing Bayesian approaches and scales easily to millions of data points. It provides additional features over frequentist competitors such as accurate predictive uncertainty estimates and automatic hyperparameter search.},
  language = {en},
  journal = {arXiv:1707.05532 [cs, stat]},
  author = {Wenzel, Florian and {Galy-Fajou}, Theo and Deutsch, Matthaeus and Kloft, Marius},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/4TZGNZK4/Wenzel et al. - 2017 - Bayesian Nonlinear Support Vector Machines for Big.pdf}
}

@incollection{karlsson2013,
  title = {Forecasting with {{Bayesian Vector Autoregression}}},
  volume = {2},
  abstract = {This chapter reviews Bayesian methods for inference and forecasting with VAR models. Bayesian inference and, by extension, forecasting depends on numerical methods for simulating from the posterior distribution of the parameters and special attention is given to the implementation of the simulation algorithm.},
  language = {en},
  booktitle = {Handbook of {{Economic Forecasting}}},
  publisher = {{Elsevier}},
  author = {Karlsson, Sune},
  year = {2013},
  keywords = {Cointegration,Conditional forecasts,Large VAR,Markov chain Monte Carlo,Model selection,Stochastic volatility,Structural VAR,Time-varying parameters},
  pages = {791-897},
  file = {/home/asher/Zotero/storage/LXSNCCFU/Karlsson - 2013 - Forecasting with Bayesian Vector Autoregression.pdf;/home/asher/Zotero/storage/BBK6CYSV/2-791.html}
}

@article{munasinghe2017,
  title = {Evaluation of the Effect of the {{EGFR}} Antibody-Drug Conjugate {{ABT}}-414 on {{QT}} Interval Prolongation in Patients with Advanced Solid Tumors Likely to over-Express {{EGFR}}},
  volume = {79},
  issn = {1432-0843},
  abstract = {PurposeABT-414 is an antibody-drug conjugate (ADC) being developed for the treatment of tumors harboring amplification of the epidermal growth factor receptor (EGFR). This study evaluated the potential of ABT-414 to prolong the QT interval as part of the initial phase 1 study (NCT01741727).MethodsData from patients who received ABT-414 monotherapy at a dose of 1\textendash{}4 mg/kg once every 3 weeks or 1 or 1.5 mg/kg weekly for 2 out of every 3 weeks (alternate schedule) by intravenous infusion were included in the analysis of triplicate 12-lead ECGs obtained before dosing and through 168 h after dosing. Data from time-matched pharmacokinetic samples and QT interval assessments were evaluated using linear mixed-effects modeling to determine the effects of ABT-414, total ABT-806, and cysteine-maleimidocaproyl monomethyl auristatin F (Cys-mcMMAF) on the QT interval corrected using Fridericia's formula (QTcF).ResultsFifty-one patients were included in the analyses. ABT-414 had no clinically meaningful effect on QTcF. Using pooled data from doses {$\geq$}2 mg/kg, the estimated mean {$\increment$}QTcF reached a maximum of 4.30 ms after dosing, with a one-sided 95\% upper confidence bound of 8.32 ms. The exposure\textendash{}response analysis showed no statistically significant relationship between {$\Delta$}QTcF and the concentration of any analyte (P {$>$} 0.05). No patient had a QTcF value {$>$}480 ms or a {$\increment$}QTcF value {$>$}30 ms.ConclusionsABT-414 had no clinically meaningful effect on the QTcF interval at doses being evaluated for treatment of patients with solid tumors.},
  language = {en},
  number = {5},
  journal = {Cancer Chemotherapy and Pharmacology},
  doi = {10.1007/s00280-017-3284-y},
  author = {Munasinghe, Wijith P. and Mittapalli, Rajendar K. and Li, Hong and Hoffman, David M. and Holen, Kyle D. and Menon, Rajeev M. and Xiong, Hao},
  month = may,
  year = {2017},
  keywords = {ABT-414,Antibody-drug conjugate,ECG,EGFR,QT interval,Solid tumor,found priors},
  pages = {915-922},
  file = {/home/asher/Zotero/storage/YGXCYQ9H/Munasinghe et al. - 2017 - Evaluation of the effect of the EGFR antibody-drug.pdf}
}

@article{spruance2004,
  title = {Hazard {{Ratio}} in {{Clinical Trials}}},
  volume = {48},
  issn = {0066-4804, 1098-6596},
  language = {en},
  number = {8},
  journal = {Antimicrobial Agents and Chemotherapy},
  doi = {10.1128/AAC.48.8.2787-2792.2004},
  author = {Spruance, S. L. and Reid, J. E. and Grace, M. and Samore, M.},
  month = aug,
  year = {2004},
  pages = {2787-2792},
  file = {/home/asher/Zotero/storage/TCR7SQBG/Spruance et al. - 2004 - Hazard Ratio in Clinical Trials.pdf}
}

@article{vandenbent2017,
  title = {Efficacy of Depatuxizumab Mafodotin ({{ABT}}-414) Monotherapy in Patients with {{EGFR}}-Amplified, Recurrent Glioblastoma: Results from a Multi-Center, International Study},
  volume = {80},
  issn = {0344-5704},
  shorttitle = {Efficacy of Depatuxizumab Mafodotin ({{ABT}}-414) Monotherapy in Patients with {{EGFR}}-Amplified, Recurrent Glioblastoma},
  abstract = {Purpose
Patients with recurrent glioblastoma (rGBM) have a poor prognosis. Epidermal growth factor receptor (EGFR) gene amplification is present in \textasciitilde\,50\% of glioblastomas (GBMs). Depatuxizumab mafodotin (depatux-m), formerly ABT-414, is an antibody\textendash{}drug conjugate that preferentially binds cells with EGFR amplification, is internalized and releases a potent antimicrotubule agent, monomethyl auristatin F (MMAF). Here we report the safety, pharmacokinetics, and efficacy of depatux-m monotherapy at the recommended Phase 2 dose (RPTD) in patients with EGFR-amplified, rGBM.

Methods
M12-356 (NCT01800695) is an open-label study with three escalation and expansion cohorts. Sixty-six patients with EGFR-amplified, rGBM were treated with depatux-m monotherapy at 1.25~mg/kg intravenously every 2~weeks. Adults with measurable rGBM, who were bevacizumab-na{\"i}ve, with EGFR amplification were eligible.

Results
Among 66 patients, median age was 58~years (range 35\textendash{}80). All patients were previously treated with radiotherapy/temozolomide. The most common adverse events (AEs) were eye related (91\%), including blurred vision (65\%), dry eye (29\%), keratitis, and photophobia (27\% each). Grade 3/4 AEs occurred in 42\% of all patients, and ocular Grade 3/4 AEs occurred in 33\% of patients overall. One patient (2\%) had a Grade 4 ocular AE. Ocular AEs were manageable and usually resolved once treatment with depatux-m ceased. The objective response rate was 6.8\%, the 6-month progression-free survival rate was 28.8\%, and the 6-month overall survival rate was 72.5\%.

Conclusion
Depatux-m monotherapy displayed frequent but mostly Grade 1/2 ocular toxicities. A PFS6 of 28.8\% was observed in this rGBM population, warranting further study.

Electronic supplementary material
The online version of this article (doi:10.1007/s00280-017-3451-1) contains supplementary material, which is available to authorized users.},
  number = {6},
  journal = {Cancer Chemotherapy and Pharmacology},
  doi = {10.1007/s00280-017-3451-1},
  author = {{van den Bent}, Martin and Gan, Hui K. and Lassman, Andrew B. and Kumthekar, Priya and Merrell, Ryan and Butowski, Nicholas and Lwin, Zarnie and Mikkelsen, Tom and Nabors, Louis B. and Papadopoulos, Kyriakos P. and {Penas-Prado}, Marta and Simes, John and Wheeler, Helen and Walbert, Tobias and Scott, Andrew M. and Gomez, Erica and Lee, Ho-Jin and {Roberts-Rapp}, Lisa and Xiong, Hao and Bain, Earle and Ansell, Peter J. and Holen, Kyle D. and Maag, David and Reardon, David A.},
  year = {2017},
  keywords = {found priors},
  pages = {1209-1217},
  file = {/home/asher/Zotero/storage/EVZEZLGK/van den Bent et al. - 2017 - Efficacy of depatuxizumab mafodotin (ABT-414) mono.pdf},
  pmid = {29075855},
  pmcid = {PMC5686264}
}

@article{gan2018,
  title = {Safety, Pharmacokinetics, and Antitumor Response of Depatuxizumab Mafodotin as Monotherapy or in Combination with Temozolomide in Patients with Glioblastoma},
  volume = {20},
  issn = {1523-5866},
  abstract = {Background: We recently reported an acceptable safety and pharmacokinetic profile of depatuxizumab mafodotin (depatux-m), formerly called ABT-414, plus radiation and temozolomide in newly diagnosed glioblastoma (arm A). The purpose of this study was to evaluate the safety and pharmacokinetics of depatux-m, either in combination with temozolomide in newly diagnosed or recurrent glioblastoma (arm B) or as monotherapy in recurrent glioblastoma (arm C).
Methods: In this multicenter phase I dose escalation study, patients received depatux-m (0.5-1.5 mg/kg in arm B, 1.25 mg/kg in arm C) every 2 weeks by intravenous infusion. Maximum tolerated dose (MTD), recommended phase II dose (RP2D), and preliminary efficacy were also determined.
Results: Thirty-eight patients were enrolled as of March 1, 2016. The most frequent toxicities were ocular, occurring in 35/38 (92\%) patients. Keratitis was the most common grade 3 adverse event observed in 6/38 (16\%) patients; thrombocytopenia was the most common grade 4 event seen in 5/38 (13\%) patients. The MTD was set at 1.5 mg/kg in arm B and was not reached in arm C. RP2D was declared as 1.25 mg/kg for both arms. Depatux-m demonstrated a linear pharmacokinetic profile. In recurrent glioblastoma patients, the progression-free survival (PFS) rate at 6 months was 30.8\% and the median overall survival was 10.7 months. Best Response Assessment in Neuro-Oncology responses were 1 complete and 2 partial responses.
Conclusion: Depatux-m alone or in combination with temozolomide demonstrated an acceptable safety and pharmacokinetic profile in glioblastoma. Further studies are currently under way to evaluate its efficacy in newly diagnosed (NCT02573324) and recurrent glioblastoma (NCT02343406).},
  language = {eng},
  number = {6},
  journal = {Neuro-Oncology},
  doi = {10.1093/neuonc/nox202},
  author = {Gan, Hui K. and Reardon, David A. and Lassman, Andrew B. and Merrell, Ryan and {van den Bent}, Martin and Butowski, Nicholas and Lwin, Zarnie and Wheeler, Helen and Fichtel, Lisa and Scott, Andrew M. and Gomez, Erica J. and Fischer, JuDee and Mandich, Helen and Xiong, Hao and Lee, Ho-Jin and Munasinghe, Wijith P. and {Roberts-Rapp}, Lisa A. and Ansell, Peter J. and Holen, Kyle D. and Kumthekar, Priya},
  month = may,
  year = {2018},
  keywords = {Aged,Humans,Male,Adult,Brain Neoplasms,Female,Glioblastoma,Middle Aged,Prognosis,Survival Rate,found priors,Antibodies; Monoclonal; Humanized,Antineoplastic Combined Chemotherapy Protocols,Follow-Up Studies,Immunoconjugates,Maximum Tolerated Dose,Safety,Temozolomide,Tissue Distribution,Young Adult},
  pages = {838-847},
  file = {/home/asher/Zotero/storage/AH4JF9F9/Gan et al. - 2018 - Safety, pharmacokinetics, and antitumor response o.pdf},
  pmid = {29077941},
  pmcid = {PMC5961429}
}

@article{kaley2018,
  title = {{{BRAF Inhibition}} in {{BRAFV600}}-{{Mutant Gliomas}}: {{Results From}} the {{VE}}-{{BASKET Study}}},
  volume = {36},
  issn = {0732-183X},
  shorttitle = {{{BRAF Inhibition}} in {{BRAFV600}}-{{Mutant Gliomas}}},
  abstract = {Purpose
BRAFV600 mutations are frequently found in several glioma subtypes, including pleomorphic xanthoastrocytoma (PXA) and ganglioglioma and much less commonly in glioblastoma. We sought to determine the activity of vemurafenib, a selective inhibitor of BRAFV600, in patients with gliomas that harbor this mutation.

Patients and Methods
The VE-BASKET study was an open-label, nonrandomized, multicohort study for BRAFV600-mutant nonmelanoma cancers. Patients with BRAFV600-mutant glioma received vemurafenib 960 mg twice per day continuously until disease progression, withdrawal, or intolerable adverse effects. Key end points included confirmed objective response rate by RECIST version 1.1, progression-free survival, overall survival, and safety.

Results
Twenty-four patients (median age, 32 years; 18 female and six male patients) with glioma, including malignant diffuse glioma (n = 11; six glioblastoma and five anaplastic astrocytoma), PXA (n = 7), anaplastic ganglioglioma (n = 3), pilocytic astrocytoma (n = 2), and high-grade glioma, not otherwise specified (n = 1), were treated. Confirmed objective response rate was 25\% (95\% CI, 10\% to 47\%) and median progression-free survival was 5.5 months (95\% CI, 3.7 to 9.6 months). In malignant diffuse glioma, best response included one partial response and five patients with stable disease, two of whom had disease stabilization that lasted more than 1 year. In PXA, best response included one complete response, two partial responses, and three patients with stable disease. Additional partial responses were observed in patients with pilocytic astrocytoma and anaplastic ganglioglioma (one each). The safety profile of vemurafenib was generally consistent with that of previously published studies.

Conclusion
Vemurafenib demonstrated evidence of durable antitumor activity in some patients with BRAFV600-mutant gliomas, although efficacy seemed to vary qualitatively by histologic subtype. Additional study is needed to determine the optimal use of vemurafenib in patients with primary brain tumors and to identify the mechanisms driving differential responses across histologic subsets.},
  number = {35},
  journal = {Journal of Clinical Oncology},
  doi = {10.1200/JCO.2018.78.9990},
  author = {Kaley, Thomas and Touat, Mehdi and Subbiah, Vivek and Hollebecque, Antoine and Rodon, Jordi and Lockhart, A. Craig and Keedy, Vicki and Bielle, Franck and Hofheinz, Ralf-Dieter and Joly, Florence and Blay, Jean-Yves and Chau, Ian and Puzanov, Igor and Raje, Noopur S. and Wolf, Jurgen and DeAngelis, Lisa M. and Makrutzki, Martina and Riehl, Todd and Pitcher, Bethany and Baselga, Jose and Hyman, David M.},
  month = dec,
  year = {2018},
  keywords = {found priors},
  pages = {3477-3484},
  file = {/home/asher/Zotero/storage/TSHP829M/Kaley et al. - 2018 - BRAF Inhibition in BRAFV600-Mutant Gliomas Result.pdf},
  pmid = {30351999},
  pmcid = {PMC6286161}
}

@article{zhang2013,
  title = {The Prognostic Value of {{MGMT}} Promoter Methylation in {{Glioblastoma}} Multiforme: A Meta-Analysis},
  volume = {12},
  issn = {1573-7292},
  shorttitle = {The Prognostic Value of {{MGMT}} Promoter Methylation in {{Glioblastoma}} Multiforme},
  abstract = {The prognostic significance of O(6)-methylguanine-DNA methyltransferase (MGMT) promoter methylation on Glioblastoma multiforme (GBM) remains controversial. A meta-analysis of published studies investigating the effects of MGMT promoter methylation on both progression-free survival (PFS) and overall survival (OS) among GBM patients was performed. A total of 2,986 patients from 30 studies were included in the meta-analysis. In all, the frequency of MGMT promoter methylation was 44.27 \%. Five studies undertook univariate analyses and nine undertook multivariate analyses of MGMT promoter methylation on PFS. The pooled hazard ratio (HR) estimate for PFS was 0.72 (95 \% CI 0.55-0.95) by univariate analysis and 0.51 (95 \% CI 0.38-0.69) by multivariate analysis. The effect of MGMT promoter methylation on OS was evaluated in 15 studies by univariate analysis and 14 studies by multivariate analysis. The combined HR was 0.67 (95 \% CI 0.58-0.78) and 0.49 (95 \% CI 0.38-0.64), respectively. For GBM patients treated with Alkylating agent, the meta-risk remained highly significant by both univariate (HR = 0.58; 95 \% CI 0.42-0.79) and multivariate analysis (HR = 0.42; 95 \% CI 0.29-0.60). This study showed that MGMT promoter methylation was associated with better PFS and OS in patients with GBM regardless of therapeutic intervention, and associated with longer OS in GBM patients treated with alkylating agents.},
  language = {eng},
  number = {3},
  journal = {Familial Cancer},
  doi = {10.1007/s10689-013-9607-1},
  author = {Zhang, Kui and Wang, Xiao-qin and Zhou, Bin and Zhang, Lin},
  month = sep,
  year = {2013},
  keywords = {Humans,Brain Neoplasms,Glioblastoma,Prognosis,found priors,Clinical Trials as Topic,DNA Methylation,DNA Modification Methylases,DNA Repair Enzymes,Promoter Regions; Genetic,Tumor Suppressor Proteins},
  pages = {449-458},
  file = {/home/asher/Zotero/storage/2ITLPQMF/Zhang et al. - 2013 - The prognostic value of MGMT promoter methylation .pdf},
  pmid = {23397067}
}

@article{stupp2009,
  title = {Effects of Radiotherapy with Concomitant and Adjuvant Temozolomide versus Radiotherapy Alone on Survival in Glioblastoma in a Randomised Phase {{III}} Study: 5-Year Analysis of the {{EORTC}}-{{NCIC}} Trial},
  volume = {10},
  issn = {1470-2045},
  shorttitle = {Effects of Radiotherapy with Concomitant and Adjuvant Temozolomide versus Radiotherapy Alone on Survival in Glioblastoma in a Randomised Phase {{III}} Study},
  abstract = {Summary
Background
In 2004, a randomised phase III trial by the European Organisation for Research and Treatment of Cancer (EORTC) and National Cancer Institute of Canada Clinical Trials Group (NCIC) reported improved median and 2-year survival for patients with glioblastoma treated with concomitant and adjuvant temozolomide and radiotherapy. We report the final results with a median follow-up of more than 5 years.
Methods
Adult patients with newly diagnosed glioblastoma were randomly assigned to receive either standard radiotherapy or identical radiotherapy with concomitant temozolomide followed by up to six cycles of adjuvant temozolomide. The methylation status of the methyl-guanine methyl transferase gene, MGMT, was determined retrospectively from the tumour tissue of 206 patients. The primary endpoint was overall survival. Analyses were by intention to treat. This trial is registered with Clinicaltrials.gov, number NCT00006353.
Findings
Between Aug 17, 2000, and March 22, 2002, 573 patients were assigned to treatment. 278 (97\%) of 286 patients in the radiotherapy alone group and 254 (89\%) of 287 in the combined-treatment group died during 5 years of follow-up. Overall survival was 27{$\cdot$}2\% (95\% CI 22{$\cdot$}2\textendash{}32{$\cdot$}5) at 2 years, 16{$\cdot$}0\% (12{$\cdot$}0\textendash{}20{$\cdot$}6) at 3 years, 12{$\cdot$}1\% (8{$\cdot$}5\textendash{}16{$\cdot$}4) at 4 years, and 9{$\cdot$}8\% (6{$\cdot$}4\textendash{}14{$\cdot$}0) at 5 years with temozolomide, versus 10{$\cdot$}9\% (7{$\cdot$}6\textendash{}14{$\cdot$}8), 4{$\cdot$}4\% (2{$\cdot$}4\textendash{}7{$\cdot$}2), 3{$\cdot$}0\% (1{$\cdot$}4\textendash{}5{$\cdot$}7), and 1{$\cdot$}9\% (0{$\cdot$}6\textendash{}4{$\cdot$}4) with radiotherapy alone (hazard ratio 0{$\cdot$}6, 95\% CI 0{$\cdot$}5\textendash{}0{$\cdot$}7; p{$<$}0{$\cdot$}0001). A benefit of combined therapy was recorded in all clinical prognostic subgroups, including patients aged 60\textendash{}70 years. Methylation of the MGMT promoter was the strongest predictor for outcome and benefit from temozolomide chemotherapy.
Interpretation
Benefits of adjuvant temozolomide with radiotherapy lasted throughout 5 years of follow-up. A few patients in favourable prognostic categories survive longer than 5 years. MGMT methylation status identifies patients most likely to benefit from the addition of temozolomide.
Funding
EORTC, NCIC, N{\'e}lia and Amadeo Barletta Foundation, Schering-Plough.},
  number = {5},
  journal = {The Lancet Oncology},
  doi = {10.1016/S1470-2045(09)70025-7},
  author = {Stupp, Roger and Hegi, Monika E and Mason, Warren P and {van den Bent}, Martin J and Taphoorn, Martin JB and Janzer, Robert C and Ludwin, Samuel K and Allgeier, Anouk and Fisher, Barbara and Belanger, Karl and Hau, Peter and Brandes, Alba A and Gijtenbeek, Johanna and Marosi, Christine and Vecht, Charles J and Mokhtari, Karima and Wesseling, Pieter and Villa, Salvador and Eisenhauer, Elizabeth and Gorlia, Thierry and Weller, Michael and Lacombe, Denis and Cairncross, J Gregory and Mirimanoff, Ren{\'e}-Olivier},
  month = may,
  year = {2009},
  keywords = {found priors},
  pages = {459-466},
  file = {/home/asher/Zotero/storage/LEXM9TEU/Stupp et al. - 2009 - Effects of radiotherapy with concomitant and adjuv.pdf;/home/asher/Zotero/storage/Q7Y72PW9/S1470204509700257.html}
}

@article{hegi2005,
  title = {{{MGMT Gene Silencing}} and {{Benefit}} from {{Temozolomide}} in {{Glioblastoma}}},
  volume = {352},
  issn = {0028-4793},
  abstract = {In this companion to the randomized trial of temozolomide in glioblastoma, also reported in this issue of the Journal, the methylation status of the promoter of the MGMT (O6-methylguanine\textendash{}DNA methyltransferase) DNA-repair gene in glioblastoma was determined. Methylation of the promoter silences the gene and thereby inhibits the repair of DNA damaged by temozolomide. Patients with a tumor that contained a methylated MGMT promoter benefited more from the drug than patients whose tumor contained an unmethylated MGMT promoter.},
  number = {10},
  journal = {New England Journal of Medicine},
  doi = {10.1056/NEJMoa043331},
  author = {Hegi, Monika E. and Diserens, Annie-Claire and Gorlia, Thierry and Hamou, Marie-France and {de Tribolet}, Nicolas and Weller, Michael and Kros, Johan M. and Hainfellner, Johannes A. and Mason, Warren and Mariani, Luigi and Bromberg, Jacoline E.C. and Hau, Peter and Mirimanoff, Ren{\'e} O. and Cairncross, J. Gregory and Janzer, Robert C. and Stupp, Roger},
  month = mar,
  year = {2005},
  keywords = {found priors},
  pages = {997-1003},
  file = {/home/asher/Zotero/storage/J8XR6XAI/Hegi et al. - 2005 - MGMT Gene Silencing and Benefit from Temozolomide .pdf;/home/asher/Zotero/storage/YMWCGP5Y/nejmoa043331.html},
  pmid = {15758010}
}

@article{malmstrom2012,
  title = {Temozolomide versus Standard 6-Week Radiotherapy versus Hypofractionated Radiotherapy in Patients Older than 60 Years with Glioblastoma: The {{Nordic}} Randomised, Phase 3 Trial},
  volume = {13},
  issn = {1470-2045},
  shorttitle = {Temozolomide versus Standard 6-Week Radiotherapy versus Hypofractionated Radiotherapy in Patients Older than 60 Years with Glioblastoma},
  abstract = {Summary
Background
Most patients with glioblastoma are older than 60 years, but treatment guidelines are based on trials in patients aged only up to 70 years. We did a randomised trial to assess the optimum palliative treatment in patients aged 60 years and older with glioblastoma.
Methods
Patients with newly diagnosed glioblastoma were recruited from Austria, Denmark, France, Norway, Sweden, Switzerland, and Turkey. They were assigned by a computer-generated randomisation schedule, stratified by centre, to receive temozolomide (200 mg/m2 on days 1\textendash{}5 of every 28 days for up to six cycles), hypofractionated radiotherapy (34{$\cdot$}0 Gy administered in 3{$\cdot$}4 Gy fractions over 2 weeks), or standard radiotherapy (60{$\cdot$}0 Gy administered in 2{$\cdot$}0 Gy fractions over 6 weeks). Patients and study staff were aware of treatment assignment. The primary endpoint was overall survival. Analyses were done by intention to treat. This trial is registered, number ISRCTN81470623.
Findings
342 patients were enrolled, of whom 291 were randomised across three treatment groups (temozolomide n=93, hypofractionated radiotherapy n=98, standard radiotherapy n=100) and 51 of whom were randomised across only two groups (temozolomide n=26, hypofractionated radiotherapy n=25). In the three-group randomisation, in comparison with standard radiotherapy, median overall survival was significantly longer with temozolomide (8{$\cdot$}3 months [95\% CI 7{$\cdot$}1\textendash{}9{$\cdot$}5; n=93] vs 6{$\cdot$}0 months [95\% CI 5{$\cdot$}1\textendash{}6{$\cdot$}8; n=100], hazard ratio [HR] 0{$\cdot$}70; 95\% CI 0{$\cdot$}52\textendash{}0{$\cdot$}93, p=0{$\cdot$}01), but not with hypofractionated radiotherapy (7{$\cdot$}5 months [6{$\cdot$}5\textendash{}8{$\cdot$}6; n=98], HR 0{$\cdot$}85 [0{$\cdot$}64\textendash{}1{$\cdot$}12], p=0{$\cdot$}24). For all patients who received temozolomide or hypofractionated radiotherapy (n=242) overall survival was similar (8{$\cdot$}4 months [7{$\cdot$}3\textendash{}9{$\cdot$}4; n=119] vs 7{$\cdot$}4 months [6{$\cdot$}4\textendash{}8{$\cdot$}4; n=123]; HR 0{$\cdot$}82, 95\% CI 0{$\cdot$}63\textendash{}1{$\cdot$}06; p=0{$\cdot$}12). For age older than 70 years, survival was better with temozolomide and with hypofractionated radiotherapy than with standard radiotherapy (HR for temozolomide vs standard radiotherapy 0{$\cdot$}35 [0{$\cdot$}21\textendash{}0{$\cdot$}56], p{$<$}0{$\cdot$}0001; HR for hypofractionated vs standard radiotherapy 0{$\cdot$}59 [95\% CI 0{$\cdot$}37\textendash{}0{$\cdot$}93], p=0{$\cdot$}02). Patients treated with temozolomide who had tumour MGMT promoter methylation had significantly longer survival than those without MGMT promoter methylation (9{$\cdot$}7 months [95\% CI 8{$\cdot$}0\textendash{}11{$\cdot$}4] vs 6{$\cdot$}8 months [5{$\cdot$}9\textendash{}7{$\cdot$}7]; HR 0{$\cdot$}56 [95\% CI 0{$\cdot$}34\textendash{}0{$\cdot$}93], p=0{$\cdot$}02), but no difference was noted between those with methylated and unmethylated MGMT promoter treated with radiotherapy (HR 0{$\cdot$}97 [95\% CI 0{$\cdot$}69\textendash{}1{$\cdot$}38]; p=0{$\cdot$}81). As expected, the most common grade 3\textendash{}4 adverse events in the temozolomide group were neutropenia (n=12) and thrombocytopenia (n=18). Grade 3\textendash{}5 infections in all randomisation groups were reported in 18 patients. Two patients had fatal infections (one in the temozolomide group and one in the standard radiotherapy group) and one in the temozolomide group with grade 2 thrombocytopenia died from complications after surgery for a gastrointestinal bleed.
Interpretation
Standard radiotherapy was associated with poor outcomes, especially in patients older than 70 years. Both temozolomide and hypofractionated radiotherapy should be considered as standard treatment options in elderly patients with glioblastoma. MGMT promoter methylation status might be a useful predictive marker for benefit from temozolomide.
Funding
Merck, Lion's Cancer Research Foundation, University of Ume{\aa}, and the Swedish Cancer Society.},
  number = {9},
  journal = {The Lancet Oncology},
  doi = {10.1016/S1470-2045(12)70265-6},
  author = {Malmstr{\"o}m, Annika and Gr{\o}nberg, Bj{\o}rn Henning and Marosi, Christine and Stupp, Roger and Frappaz, Didier and Schultz, Henrik and Abacioglu, Ufuk and Tavelin, Bj{\"o}rn and Lhermitte, Benoit and Hegi, Monika E and Rosell, Johan and Henriksson, Roger},
  month = sep,
  year = {2012},
  keywords = {found priors},
  pages = {916-926},
  file = {/home/asher/Zotero/storage/D6LIG2UK/Malmström et al. - 2012 - Temozolomide versus standard 6-week radiotherapy v.pdf;/home/asher/Zotero/storage/V8DCI4RD/1-s2.0-S1470204512702656-mmc1.pdf;/home/asher/Zotero/storage/785UDRXS/S1470204512702656.html}
}

@article{stein2019,
  title = {Safety and Enhanced Immunostimulatory Activity of the {{DRD2}} Antagonist {{ONC201}} in Advanced Solid Tumor Patients with Weekly Oral Administration},
  volume = {7},
  issn = {2051-1426},
  abstract = {BackgroundONC201 is a small molecule antagonist of DRD2, a G protein-coupled receptor overexpressed in several malignancies, that has prolonged antitumor efficacy and immunomodulatory properties in preclinical models. The first-in-human trial of ONC201 previously established a recommended phase II dose (RP2D) of 625 mg once every three weeks. Here, we report the results of a phase I study that evaluated the safety, pharmacokinetics (PK), and pharmacodynamics (PD) of weekly ONC201.MethodsPatients {$\geq$} 18 years old with an advanced solid tumor refractory to standard treatment were enrolled. Dose escalation proceeded with a 3 + 3 design from 375 mg to 625 mg of ONC201. One cycle, also the dose-limiting toxicity (DLT) window, was 21 days. The primary endpoint was to determine the RP2D of weekly ONC201, which was confirmed in an 11-patient dose expansion cohort.ResultsTwenty patients were enrolled: three at 375 mg and 17 at 625 mg of ONC201. The RP2D was defined as 625 mg with no DLT, treatment discontinuation, or dose modifications due to drug-related toxicity. PK profiles were consistent with every-three-week dosing and similar between the first and fourth dose. Serum prolactin and caspase-cleaved cytokeratin-18 induction were detected, along with intratumoral integrated stress response activation and infiltration of granzyme B+ Natural Killer cells. Induction of immune cytokines and effectors was higher in patients who received ONC201 once weekly versus once every three weeks. Stable disease of {$>$} 6 months was observed in several prostate and endometrial cancer patients.ConclusionsWeekly, oral ONC201 is well-tolerated and results in enhanced immunostimulatory activity that warrants further investigation.Trial registrationNCT02250781 (Oral ONC201 in Treating Patients With Advanced Solid Tumors), NCT02324621 (Continuation of Oral ONC201 in Treating Patients With Advanced Solid Tumors).},
  language = {en},
  number = {1},
  journal = {Journal for ImmunoTherapy of Cancer},
  doi = {10.1186/s40425-019-0599-8},
  author = {Stein, Mark N. and Malhotra, Jyoti and Tarapore, Rohinton S. and Malhotra, Usha and Silk, Ann W. and Chan, Nancy and Rodriguez, Lorna and Aisner, Joseph and Aiken, Robert D. and Mayer, Tina and Haffty, Bruce G. and Newman, Jenna H. and Aspromonte, Salvatore M. and Bommareddy, Praveen K. and Estupinian, Ricardo and Chesson, Charles B. and Sadimin, Evita T. and Li, Shengguo and Medina, Daniel J. and Saunders, Tracie and Frankel, Melissa and Kareddula, Aparna and Damare, Sherrie and Wesolowsky, Elayne and Gabel, Christian and {El-Deiry}, Wafik S. and Prabhu, Varun V. and Allen, Joshua E. and Stogniew, Martin and Oster, Wolfgang and Bertino, Joseph R. and Libutti, Steven K. and Mehnert, Janice M. and Zloza, Andrew},
  month = may,
  year = {2019},
  keywords = {Cancer,found priors,Dopamine,Immuno-oncology,Immunotherapy,ONC201,Solid tumors},
  pages = {136},
  file = {/home/asher/Zotero/storage/CELF5FKU/Stein et al. - 2019 - Safety and enhanced immunostimulatory activity of .pdf}
}

@article{burris2015,
  title = {Abstract {{PL04}}-05: {{The}} First Reported Results of {{AG}}-120, a First-in-Class, Potent Inhibitor of the {{IDH1}} Mutant Protein, in a {{Phase I}} Study of Patients with Advanced {{IDH1}}-Mutant Solid Tumors, Including Gliomas},
  volume = {14},
  copyright = {\textcopyright{}2015 American Association for Cancer Research.},
  issn = {1535-7163, 1538-8514},
  shorttitle = {Abstract {{PL04}}-05},
  abstract = {Introduction: Somatic mutations in the metabolic enzymes isocitrate dehydrogenase (IDH) 1 and 2 occur in a spectrum of solid and hematologic malignancies. Mutant IDH1/2 in cancer cells results in the neomorphic production of the oncometabolite, D-2-hydroxyglutarate (2-HG), which impairs cellular differentiation via an epigenetic mechanism. AG-120 is a first-in-class, oral, potent, reversible and selective inhibitor of mutated IDH1 protein. We report preliminary results from the ongoing, first-in-human, phase 1, open-label, single-arm study of AG-120 (NCT02073994).
Aims: Key objectives are to evaluate the safety, tolerability, and maximum tolerated dose (MTD), pharmacokinetics (PK), pharmacodynamics (PD), and preliminary clinical activity. Key exploratory objectives include an analysis of tumor tissue samples in non-glioma subjects and magnetic resonance imagining/spectroscopy (MRI/MRS) in glioma subjects, pre and on AG-120 treatment.
Methods: Patients with advanced, IDH1-mutant solid tumors, including glioma, who have recurred or progressed following standard therapy, or who have not responded to standard therapy, are eligible to receive continuous, single-agent, oral AG-120 dosed daily in 28-day cycles. Informed consent is obtained prior to entry. Sequential dose cohorts are being enrolled, with expansion cohorts planned. Blood and tumor biopsies are collected for PK/PD assessment. Objective responses are investigator assessed using either RECIST or RANO criteria for subjects with solid tumors and gliomas, respectively.
Results: As of 1 July 2015, 55 patients (glioma: 20, non-glioma: 35) were treated with AG-120. Median age was 54 years (range, 23-88) and median number of prior systemic regimens 3 (range, 1-6). Doses administered were 100 mg BID (n = 4), 300 mg QD (n = 9), 400 mg QD (n = 5), 500 mg QD (n = 17), 600 mg QD (n = 5), 800 mg QD (n = 6), 900 mg QD (n = 4), and 1200 mg QD (n = 5). Median treatment duration was 1.9 months (range, 0.1-12.5). The MTD was not reached. PK analyses showed high plasma exposure and drug accumulation following oral administration and a mean half-life of 73.1 {$\pm$} 66.6 hr. Overall, treatment was well tolerated: 49 patients experienced treatment-emergent adverse events (AEs), regardless of causality. Most frequently occurring AEs (\%) were nausea (21.8), diarrhea (16.4), vomiting (14.5), anemia (12.7), and abdominal pain (10.9). There were no treatment-related serious AEs.
Summary/Conclusion: AG-120 is a first-in-class, oral, potent, selective inhibitor of mutant IDH1 in development for solid and liquid tumors. Updated safety and clinical activity, as well as exploratory PD analyses will be presented. Future development plans for AG-120 in solid tumors will also be highlighted.
Citation Format: Howard Burris, Ingo Mellinghoff, Elizabeth Maher, Patrick Wen, Murali Beeram, Mehdi Touat, Jason Faris, Nilofer Azad, Timothy Cloughesy, Lia Gore, Jonathan Trent, Daniel Von Hoff, Meredith Goldwasser, Bin Fan, Sam Agresta. The first reported results of AG-120, a first-in-class, potent inhibitor of the IDH1 mutant protein, in a Phase I study of patients with advanced IDH1-mutant solid tumors, including gliomas. [abstract]. In: Proceedings of the AACR-NCI-EORTC International Conference: Molecular Targets and Cancer Therapeutics; 2015 Nov 5-9; Boston, MA. Philadelphia (PA): AACR; Mol Cancer Ther 2015;14(12 Suppl 2):Abstract nr PL04-05.},
  language = {en},
  number = {12 Supplement 2},
  journal = {Molecular Cancer Therapeutics},
  doi = {10.1158/1535-7163.TARG-15-PL04-05},
  author = {Burris, Howard and Mellinghoff, Ingo and Maher, Elizabeth and Wen, Patrick and Beeram, Murali and Touat, Mehdi and Faris, Jason and Azad, Nilofer and Cloughesy, Timothy and Gore, Lia and Trent, Jonathan and Hoff, Daniel Von and Goldwasser, Meredith and Fan, Bin and Agresta, Sam},
  month = dec,
  year = {2015},
  keywords = {found priors},
  pages = {PL04-05-PL04-05},
  file = {/home/asher/Zotero/storage/ETA9S5QQ/PL04-05.html}
}

@article{chi2018,
  title = {Integrated Clinical Experience with {{ONC201}} in {{H3 K27M}} Glioma.},
  volume = {36},
  issn = {0732-183X},
  abstract = {2059Background: ONC201 is a DRD2 antagonist in Phase II trials for cancers that exhibit dysregulation of the dopamine pathway. We previously reported an objective response in the first recurrent H3 K27M midline glioma patient who received ONC201 and that H3 K27M gliomas exhibit enhanced sensitivity to the compound in vitro. Here, we report the clinical experience with ONC201 to date in adult and pediatric H3 K27M glioma. Methods: As of February 1, 2018, 14 patients with H3 K27M glioma have received single agent ONC201 and had at least one post-treatment MRI. This includes 9 adult patients (8 glioblastoma, 1 diffuse midline glioma) and 5 pediatric patients (3 DIPG, 2 other diffuse midline gliomas). All patients had recurrent disease, except for 2 DIPG patients who had completed radiotherapy. Seven patients were enrolled on a clinical trial and seven were enrolled on compassionate use protocols. ONC201 was orally administered at 625 mg to adult patients and scaled based on body weight for pediatric patients. All but one patient were dosed once a week. Results: Six out of the 14 patients remain on therapy with a median follow up of 5.4 months (range: 2.9-22.6) with durable radiographic and/or clinical stability or improvement. This includes the first responder who has now been on treatment for 22 months with a 96\% overall regression. All 6 patients who experienced benefit had 1-2 prior lines of therapy, whereas 5 patients who had at least 3 prior lines of therapy did not experience benefit. Among the five patients with thalamic glioma, two experienced complete regressions of their thalamic lesions while another underwent a 30\% regression. One patient with previously-irradiated DIPG that exhibited a 300nM IC50 ex vivo has experienced improvements in hemiparesis and other disease-related symptoms. Another previously-irradiated DIPG patient has experienced improvements in her facial palsy and other disease-related symptoms, along with a 40\% regression. Conclusions: Preliminary clinical data indicates ONC201 induces durable radiographic regressions and clinical benefit in a subset of patients with H3 K27M glioma. The clinical activity of ONC201 in pediatric and adult H3 K27M gliomas is being evaluated in ongoing clinical trials. Clinical trial information: NCT03295396, NCT02525692, NCT03416530.},
  number = {15\_suppl},
  journal = {Journal of Clinical Oncology},
  doi = {10.1200/JCO.2018.36.15_suppl.2059},
  author = {Chi, Andrew S. and Gardner, Sharon L. and Arrillaga, Isabel and Wen, Patrick Y. and Batchelor, Tracy and Hall, Matthew David and Odia, Yazmin and Zaky, Wafik Tharwat and Khatua, Soumen and Shonka, Nicole A. and Khatib, Ziad and Tarapore, Rohinton and Schalop, Lee and Allen, Joshua E. and Oster, Wolfgang and Mehta, Minesh P.},
  month = may,
  year = {2018},
  keywords = {found priors},
  pages = {2059-2059},
  file = {/home/asher/Zotero/storage/HDVB4YAX/JCO.2018.36.15_suppl.html}
}

@article{schreiber2017,
  title = {Finding the Optimal {{Bayesian}} Network given a Constraint Graph},
  volume = {3},
  issn = {2376-5992},
  abstract = {Despite recent algorithmic improvements, learning the optimal structure of a Bayesian network from data is typically infeasible past a few dozen variables. Fortunately, domain knowledge can frequently be exploited to achieve dramatic computational savings, and in many cases domain knowledge can even make structure learning tractable. Several methods have previously been described for representing this type of structural prior knowledge, including global orderings, super-structures, and constraint rules. While super-structures and constraint rules are flexible in terms of what prior knowledge they can encode, they achieve savings in memory and computational time simply by avoiding considering invalid graphs. We introduce the concept of a ``constraint graph'' as an intuitive method for incorporating rich prior knowledge into the structure learning task. We describe how this graph can be used to reduce the memory cost and computational time required to find the optimal graph subject to the encoded constraints, beyond merely eliminating invalid graphs. In particular, we show that a constraint graph can break the structure learning task into independent subproblems even in the presence of cyclic prior knowledge. These subproblems are well suited to being solved in parallel on a single machine or distributed across many machines without excessive communication cost.},
  language = {en},
  journal = {PeerJ Computer Science},
  doi = {10.7717/peerj-cs.122},
  author = {Schreiber, Jacob M. and Noble, William S.},
  month = jul,
  year = {2017},
  pages = {e122},
  file = {/home/asher/Zotero/storage/JUXUJGYP/Schreiber and Noble - 2017 - Finding the optimal Bayesian network given a const.pdf;/home/asher/Zotero/storage/MC4WA2V9/cs-122.html}
}

@inproceedings{stachniss2005,
  title = {Information {{Gain}}-Based {{Exploration Using Rao}}-{{Blackwellized Particle Filters}}},
  isbn = {978-0-262-70114-3},
  abstract = {This paper presents an integrated approach to exploration, mapping, and localization. Our algorithm uses a highly efficient Rao-Blackwellized particle filter to represent the posterior about maps and poses. It applies a decision-theoretic framework which simultaneously considers the uncertainty in the map and in the pose of the vehicle to evaluate potential actions. Thereby, it trades off the cost of executing an action with the expected information gain and takes into account possible sensor measurements gathered along the path taken by the robot. We furthermore describe how to utilize the properties of the Rao-Blackwellization to efficiently compute the expected information gain. We present experimental results obtained in the real world and in simulation to demonstrate the effectiveness of our approach.},
  language = {en},
  booktitle = {Robotics: {{Science}} and {{Systems I}}},
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2005.I.009},
  author = {Stachniss, Cyrill and Grisetti, Giorgio and Burgard, Wolfram},
  month = jun,
  year = {2005},
  file = {/home/asher/Zotero/storage/3BCMGYTL/Stachniss et al. - 2005 - Information Gain-based Exploration Using Rao-Black.pdf}
}

@article{tsalatsanis2010,
  title = {A Regret Theory Approach to Decision Curve Analysis: {{A}} Novel Method for Eliciting Decision Makers' Preferences and Decision-Making},
  volume = {10},
  issn = {1472-6947},
  shorttitle = {A Regret Theory Approach to Decision Curve Analysis},
  abstract = {Background
Decision curve analysis (DCA) has been proposed as an alternative method for evaluation of diagnostic tests, prediction models, and molecular markers. However, DCA is based on expected utility theory, which has been routinely violated by decision makers. Decision-making is governed by intuition (system 1), and analytical, deliberative process (system 2), thus, rational decision-making should reflect both formal principles of rationality and intuition about good decisions. We use the cognitive emotion of regret to serve as a link between systems 1 and 2 and to reformulate DCA.

Methods
First, we analysed a classic decision tree describing three decision alternatives: treat, do not treat, and treat or no treat based on a predictive model. We then computed the expected regret for each of these alternatives as the difference between the utility of the action taken and the utility of the action that, in retrospect, should have been taken. For any pair of strategies, we measure the difference in net expected regret. Finally, we employ the concept of acceptable regret to identify the circumstances under which a potentially wrong strategy is tolerable to a decision-maker.

Results
We developed a novel dual visual analog scale to describe the relationship between regret associated with "omissions" (e.g. failure to treat) vs. "commissions" (e.g. treating unnecessary) and decision maker's preferences as expressed in terms of threshold probability. We then proved that the Net Expected Regret Difference, first presented in this paper, is equivalent to net benefits as described in the original DCA. Based on the concept of acceptable regret we identified the circumstances under which a decision maker tolerates a potentially wrong decision and expressed it in terms of probability of disease.

Conclusions
We present a novel method for eliciting decision maker's preferences and an alternative derivation of DCA based on regret theory. Our approach may be intuitively more appealing to a decision-maker, particularly in those clinical situations when the best management option is the one associated with the least amount of regret (e.g. diagnosis and treatment of advanced cancer, etc).},
  journal = {BMC Medical Informatics and Decision Making},
  doi = {10.1186/1472-6947-10-51},
  author = {Tsalatsanis, Athanasios and Hozo, Iztok and Vickers, Andrew and Djulbegovic, Benjamin},
  month = sep,
  year = {2010},
  pages = {51},
  file = {/home/asher/Zotero/storage/T9WU9M2X/Tsalatsanis et al. - 2010 - A regret theory approach to decision curve analysi.pdf},
  pmid = {20846413},
  pmcid = {PMC2954854}
}

@inproceedings{srinivas2010,
  address = {{Haifa, Israel}},
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}:  {{No Regret}} and {{Experimental Design}}},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  language = {en},
  booktitle = {Proc. 7th {{Int}}. {{Conf}}. on {{Machine Learning}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  year = {2010},
  pages = {8},
  file = {/home/asher/Zotero/storage/BSFSGSSK/Srinivas et al. - Gaussian Process Optimization in the Bandit Settin.pdf}
}

@article{matt2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04293},
  title = {Toward an {{Algebraic Theory}} of {{Systems}}},
  volume = {747},
  issn = {03043975},
  abstract = {We propose the concept of a system algebra with a parallel composition operation and an interface connection operation, and formalize composition-order invariance, which postulates that the order of composing and connecting systems is irrelevant, a generalized form of associativity. Composition-order invariance explicitly captures a common property that is implicit in any context where one can draw a figure (hiding the drawing order) of several connected systems, which appears in many scientific contexts. This abstract algebra captures settings where one is interested in the behavior of a composed system in an environment and wants to abstract away anything internal not relevant for the behavior. This may include physical systems, electronic circuits, or interacting distributed systems. One specific such setting, of special interest in computer science, are functional system algebras, which capture, in the most general sense, any type of system that takes inputs and produces outputs depending on the inputs, and where the output of a system can be the input to another system. The behavior of such a system is uniquely determined by the function mapping inputs to outputs. We consider several instantiations of this very general concept. In particular, we show that Kahn networks form a functional system algebra and prove their composition-order invariance. Moreover, we define a functional system algebra of causal systems, characterized by the property that inputs can only influence future outputs, where an abstract partial order relation captures the notion of "later". This system algebra is also shown to be composition-order invariant and appropriate instantiations thereof allow to model and analyze systems that depend on time.},
  journal = {Theoretical Computer Science},
  doi = {10.1016/j.tcs.2018.06.001},
  author = {Matt, Christian and Maurer, Ueli and Portmann, Christopher and Renner, Renato and Tackmann, Bj{\"o}rn},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Other Computer Science},
  pages = {1-25},
  file = {/home/asher/Zotero/storage/GAHNDQTY/Matt et al. - 2018 - Toward an Algebraic Theory of Systems.pdf;/home/asher/Zotero/storage/79JXRSYX/1609.html}
}

@article{2019,
  title = {Reproducing Kernel {{Hilbert}} Space},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {In functional analysis (a branch of mathematics), a reproducing kernel Hilbert space (RKHS) is a Hilbert space of functions in which point evaluation is a continuous linear functional. Roughly speaking, this means that if two functions 
  
    
      
        f
      
    
    \{\textbackslash{}displaystyle f\}
   and 
  
    
      
        g
      
    
    \{\textbackslash{}displaystyle g\}
   in the RKHS are close in norm, i.e., 
  
    
      
        {$\Vert$}
        f
        -
        g
        {$\Vert$}
      
    
    \{\textbackslash{}displaystyle \textbackslash{}|f-g\textbackslash{}|\}
   is small, then 
  
    
      
        f
      
    
    \{\textbackslash{}displaystyle f\}
   and 
  
    
      
        g
      
    
    \{\textbackslash{}displaystyle g\}
   are also pointwise close, i.e., 
  
    
      
        
          |
        
        f
        (
        x
        )
        -
        g
        (
        x
        )
        
          |
        
      
    
    \{\textbackslash{}displaystyle |f(x)-g(x)|\}
   is small for all 
  
    
      
        x
      
    
    \{\textbackslash{}displaystyle x\}
  . The reverse need not be true.
It is not entirely straightforward to construct a Hilbert space of functions which is not an RKHS. Note that L2 spaces are not  Hilbert spaces of functions (and hence not RKHSs), but rather Hilbert spaces of equivalence classes of functions (for example, the functions 
  
    
      
        f
      
    
    \{\textbackslash{}displaystyle f\}
   and 
  
    
      
        g
      
    
    \{\textbackslash{}displaystyle g\}
   defined by 
  
    
      
        f
        (
        x
        )
        =
        0
      
    
    \{\textbackslash{}displaystyle f(x)=0\}
   and 
  
    
      
        g
        (
        x
        )
        =
        
          1
          
            
              Q
            
          
        
      
    
    \{\textbackslash{}displaystyle g(x)=1\_\{\textbackslash{}mathbb \{Q\} \}\}
   are equivalent in L2). However, there are RKHSs in which the norm is an L2-norm, such as the space of band-limited functions (see the example below).
An RKHS is associated with a kernel that reproduces every function in the space in the sense that for any 
  
    
      
        x
      
    
    \{\textbackslash{}displaystyle x\}
   in the set on which the functions are defined, "evaluation at 
  
    
      
        x
      
    
    \{\textbackslash{}displaystyle x\}
  " can be performed by taking an inner product with a function determined by the kernel. Such a reproducing kernel exists if and only if every evaluation functional is continuous.
The reproducing kernel was first introduced in the 1907 work of Stanis{\l}aw Zaremba concerning boundary value problems for harmonic and biharmonic functions.  James Mercer simultaneously examined functions which satisfy the reproducing property in the theory of integral equations. The idea of the reproducing kernel remained untouched for nearly twenty years until it appeared in the dissertations of G{\'a}bor Szeg{\H o}, Stefan Bergman, and Salomon Bochner.  The subject was eventually systematically developed in the early 1950s by Nachman Aronszajn and Stefan Bergman.These spaces have wide applications, including complex analysis, harmonic analysis, and quantum mechanics.  Reproducing kernel Hilbert spaces are particularly important in the field of statistical learning theory because of the celebrated representer theorem which states that every function in an RKHS that minimises an empirical risk functional can be written as a linear combination of the kernel function evaluated at the training points.  This is a practically useful result as it effectively simplifies the empirical risk minimization problem from an infinite dimensional to a finite dimensional optimization problem.
For ease of understanding, we provide the framework for real-valued Hilbert spaces.  The theory can be easily extended to spaces of complex-valued functions and hence include the many important examples of reproducing kernel Hilbert spaces that are spaces of analytic functions.},
  language = {en},
  journal = {Wikipedia},
  month = jul,
  year = {2019},
  file = {/home/asher/Zotero/storage/Q35SDID8/index.html},
  note = {Page Version ID: 907992913}
}

@article{2019a,
  title = {Representer Theorem},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {In statistical learning theory, a representer theorem is any of several related results stating that a minimizer 
  
    
      
        
          f
          
            {${_\ast}$}
          
        
      
    
    \{\textbackslash{}displaystyle f\^\{*\}\}
   of a regularized empirical risk functional defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.},
  language = {en},
  journal = {Wikipedia},
  month = may,
  year = {2019},
  file = {/home/asher/Zotero/storage/JLSE9Q2N/index.html},
  note = {Page Version ID: 898949448}
}

@article{2019b,
  title = {Empirical Risk Minimization},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true "risk") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the "empirical" risk).},
  language = {en},
  journal = {Wikipedia},
  month = jun,
  year = {2019},
  file = {/home/asher/Zotero/storage/5C8VHPI7/index.html},
  note = {Page Version ID: 901119500}
}

@article{auer,
  title = {Using {{Confidence Bounds}} for {{Exploitation}}-{{Exploration Trade}}-Offs},
  abstract = {We show how a standard tool from statistics \textemdash{} namely confidence bounds \textemdash{} can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process.},
  language = {en},
  author = {Auer, Peter},
  pages = {26},
  file = {/home/asher/Zotero/storage/8THY35CD/Auer - Using Conﬁdence Bounds for Exploitation-Exploratio.pdf}
}

@article{frazier,
  title = {Tutorial: {{Bayesian Methods}} for   {{Global}} and {{Simulation Optimization}}},
  language = {en},
  author = {Frazier, Peter I},
  pages = {141},
  file = {/home/asher/Zotero/storage/DD6H8VE8/Frazier - Tutorial Bayesian Methods for   Global and Simula.pdf}
}

@misc{zotero-860,
  title = {Optimal {{Experiment Design}} \textemdash{} {{Pyro}} Documentation},
  howpublished = {http://docs.pyro.ai/en/stable/contrib.oed.html},
  file = {/home/asher/Zotero/storage/BCV7DJSX/contrib.oed.html}
}

@article{linden2013,
  title = {Assessing Regression to the Mean Effects in Health Care Initiatives},
  volume = {13},
  issn = {1471-2288},
  abstract = {Background
Interventions targeting individuals classified as ``high-risk'' have become common-place in health care. High-risk may represent outlier values on utilization, cost, or clinical measures. Typically, such individuals are invited to participate in an intervention intended to reduce their level of risk, and after a period of time, a follow-up measurement is taken. However, individuals initially identified by their outlier values will likely have lower values on re-measurement in the absence of an intervention. This statistical phenomenon is known as ``regression to the mean'' (RTM) and often leads to an inaccurate conclusion that the intervention caused the effect. Concerns about RTM are rarely raised in connection with most health care interventions, and it is uncommon to find evaluators who estimate its effect. This may be due to lack of awareness, cognitive biases that may cause people to systematically misinterpret RTM effects by creating (erroneous) explanations to account for it, or by design.

Methods
In this paper, the author fully describes the RTM phenomenon, and tests the accuracy of the traditional approach in calculating RTM assuming normality, using normally distributed data from a Monte Carlo simulation and skewed data from a control group in a pre-post evaluation of a health intervention. Confidence intervals are generated around the traditional RTM calculation to provide more insight into the potential magnitude of the bias introduced by RTM. Finally, suggestions are offered for designing interventions and evaluations to mitigate the effects of RTM.

Results
On multivariate normal data, the calculated RTM estimates are identical to true estimates. As expected, when using skewed data the calculated method underestimated the true RTM effect. Confidence intervals provide helpful guidance on the magnitude of the RTM effect.

Conclusion
Decision-makers should always consider RTM to be a viable explanation of the observed change in an outcome in a pre-post study, and evaluators of health care initiatives should always take the appropriate steps to estimate the magnitude of the effect and control for it when possible. Regardless of the cause, failure to address RTM may result in wasteful pursuit of ineffective interventions, both at the organizational level and at the policy level.},
  journal = {BMC Medical Research Methodology},
  doi = {10.1186/1471-2288-13-119},
  author = {Linden, Ariel},
  month = sep,
  year = {2013},
  pages = {119},
  file = {/home/asher/Zotero/storage/NF5XAVLW/Linden - 2013 - Assessing regression to the mean effects in health.pdf},
  pmid = {24073634},
  pmcid = {PMC3849564}
}

@article{barnett2005,
  title = {Regression to the Mean: What It Is and How to Deal with It},
  volume = {34},
  issn = {0300-5771},
  shorttitle = {Regression to the Mean},
  abstract = {Abstract.  Background Regression to the mean (RTM) is a statistical phenomenon that can make natural variation in repeated data look like real change. It happen},
  language = {en},
  number = {1},
  journal = {International Journal of Epidemiology},
  doi = {10.1093/ije/dyh299},
  author = {Barnett, Adrian G. and {van der Pols}, Jolieke C. and Dobson, Annette J.},
  month = feb,
  year = {2005},
  pages = {215-220},
  file = {/home/asher/Zotero/storage/AT3X8CM5/Barnett et al. - 2005 - Regression to the mean what it is and how to deal.pdf;/home/asher/Zotero/storage/ZDBBZ34L/638499.html}
}

@article{ailon2014,
  title = {Active {{Learning Using Smooth Relative Regret Approximations}} with {{Applications}}},
  volume = {15},
  abstract = {The disagreement coefficient of Hanneke has become a central data independent invariant in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coefficient at an optimal solution allows active learning rates that are superior to passive learning ones.},
  language = {en},
  number = {1},
  journal = {Journal of Machine Learning Research},
  author = {Ailon, Nir and Begleiter, Ron and Ezra, Esther},
  month = mar,
  year = {2014},
  pages = {36},
  file = {/home/asher/Zotero/storage/SLXSPLSH/Ailon et al. - Active Learning Using Smooth Relative Regret Appro.pdf}
}

@inproceedings{hanneke2007,
  address = {{Corvalis, Oregon}},
  title = {A Bound on the Label Complexity of Agnostic Active Learning},
  isbn = {978-1-59593-793-3},
  abstract = {We study the label complexity of pool-based active learning in the agnostic PAC model. Specifically, we derive a general upper bound on the number of label requests made by the A2 algorithm proposed by Balcan et al. [1]. This represents the first nontrivial general-purpose upper bound on label complexity in the agnostic PAC model.},
  language = {en},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning - {{ICML}} '07},
  publisher = {{ACM Press}},
  doi = {10.1145/1273496.1273541},
  author = {Hanneke, Steve},
  year = {2007},
  pages = {353-360},
  file = {/home/asher/Zotero/storage/ZDGD4A9S/Hanneke - 2007 - A bound on the label complexity of agnostic active.pdf}
}

@article{schapire1990,
  title = {The Strength of Weak Learnability},
  volume = {5},
  issn = {1573-0565},
  abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error {$o$}.},
  language = {en},
  number = {2},
  journal = {Machine Learning},
  doi = {10.1007/BF00116037},
  author = {Schapire, Robert E.},
  month = jun,
  year = {1990},
  keywords = {Machine learning,learnability theory,learning from examples,PAC learning,polynomial-time identification},
  pages = {197-227},
  file = {/home/asher/Zotero/storage/LBS8M7L8/Schapire - 1990 - The strength of weak learnability.pdf}
}

@article{balcan2009,
  series = {Learning {{Theory}} 2006},
  title = {Agnostic Active Learning},
  volume = {75},
  issn = {0022-0000},
  abstract = {We state and analyze the first active learning algorithm that finds an {$\epsilon$}-optimal hypothesis in any hypothesis class, when the underlying distribution has arbitrary forms of noise. The algorithm, A2 (for Agnostic Active), relies only upon the assumption that it has access to a stream of unlabeled examples drawn i.i.d. from a fixed distribution. We show that A2 achieves an exponential improvement (i.e., requires only O(ln1{$\epsilon$}) samples to find an {$\epsilon$}-optimal classifier) over the usual sample complexity of supervised learning, for several settings considered before in the realizable case. These include learning threshold classifiers and learning homogeneous linear separators with respect to an input distribution which is uniform over the unit sphere.},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  doi = {10.1016/j.jcss.2008.07.003},
  author = {Balcan, Maria-Florina and Beygelzimer, Alina and Langford, John},
  month = jan,
  year = {2009},
  keywords = {Active learning,Agnostic setting,Linear separators,Sample complexity},
  pages = {78-89},
  file = {/home/asher/Zotero/storage/NCN5B5BF/Balcan et al. - 2009 - Agnostic active learning.pdf;/home/asher/Zotero/storage/MUF77Z7Q/S0022000008000652.html}
}

@article{dasgupta,
  title = {Coarse Sample Complexity Bounds for Active Learning},
  abstract = {We characterize the sample complexity of active learning problems in terms of a parameter which takes into account the distribution over the input space, the specific target hypothesis, and the desired accuracy.},
  language = {en},
  author = {Dasgupta, Sanjoy},
  keywords = {Active learning},
  pages = {8},
  file = {/home/asher/Zotero/storage/4B8PMQ4E/Dasgupta - Coarse sample complexity bounds for active learnin.pdf}
}

@article{liepe2013,
  title = {Maximizing the {{Information Content}} of {{Experiments}} in {{Systems Biology}}},
  volume = {9},
  issn = {1553-7358},
  abstract = {Our understanding of most biological systems is in its infancy. Learning their structure and intricacies is fraught with challenges, and often side-stepped in favour of studying the function of different gene products in isolation from their physiological context. Constructing and inferring global mathematical models from experimental data is, however, central to systems biology. Different experimental setups provide different insights into such systems. Here we show how we can combine concepts from Bayesian inference and information theory in order to identify experiments that maximize the information content of the resulting data. This approach allows us to incorporate preliminary information; it is global and not constrained to some local neighbourhood in parameter space and it readily yields information on parameter robustness and confidence. Here we develop the theoretical framework and apply it to a range of exemplary problems that highlight how we can improve experimental investigations into the structure and dynamics of biological systems and their behavior.},
  language = {en},
  number = {1},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1002888},
  author = {Liepe, Juliane and Filippi, Sarah and Komorowski, Micha{\l} and Stumpf, Michael P. H.},
  month = jan,
  year = {2013},
  keywords = {Entropy,Simulation and modeling,AKT signaling cascade,Experimental design,Information entropy,Mathematical models,Monte Carlo method,Probability distribution},
  pages = {e1002888},
  file = {/home/asher/Zotero/storage/C2R9CKJN/Liepe et al. - 2013 - Maximizing the Information Content of Experiments .pdf;/home/asher/Zotero/storage/FIVC6YGK/article.html}
}

@article{vanlier2012,
  title = {A {{Bayesian}} Approach to Targeted Experiment Design},
  volume = {28},
  issn = {1367-4803, 1460-2059},
  abstract = {Motivation: Systems biology employs mathematical modelling to further our understanding of biochemical pathways. Since the amount of experimental data on which the models are parameterized is often limited, these models exhibit large uncertainty in both parameters and predictions. Statistical methods can be used to select experiments that will reduce such uncertainty in an optimal manner. However, existing methods for optimal experiment design (OED) rely on assumptions that are inappropriate when data are scarce considering model complexity.},
  language = {en},
  number = {8},
  journal = {Bioinformatics},
  doi = {10.1093/bioinformatics/bts092},
  author = {Vanlier, J. and Tiemann, C. A. and Hilbers, P. A. J. and {van Riel}, N. A. W.},
  month = apr,
  year = {2012},
  pages = {1136-1142},
  file = {/home/asher/Zotero/storage/HYLRFSBJ/Vanlier et al. - 2012 - A Bayesian approach to targeted experiment design.pdf}
}

@misc{simsek2019,
  type = {Research Article},
  title = {Metronomic {{Chemotherapy}}: {{A Systematic Review}} of the {{Literature}} and {{Clinical Experience}}},
  shorttitle = {Metronomic {{Chemotherapy}}},
  abstract = {Metronomic chemotherapy, continuous and dose-dense administration of chemotherapeutic drugs with lowered doses, is being evaluated for substituting, augmenting, or appending conventional maximum tolerated dose regimens, with preclinical and clinical studies for the past few decades. To date, the principle mechanisms of its action include impeding tumoral angiogenesis and modulation of hosts' immune system, affecting directly tumor cells, their progenitors, and neighboring stromal cells. Its better toxicity profile, lower cost, and easier use are main advantages over conventional therapies. The evidence of metronomic chemotherapy for personalized medicine is growing, starting with unfit elderly patients and also for palliative treatment. The literature reviewed in this article mainly demonstrates that metronomic chemotherapy is advantageous for selected patients and for certain types of malignancies, which make it a promising therapeutic approach for filling in the gaps. More clinical studies are needed to establish a solidified role for metronomic chemotherapy with other treatment models in modern cancer management.},
  language = {en},
  journal = {Journal of Oncology},
  howpublished = {https://www.hindawi.com/journals/jo/2019/5483791/},
  author = {Simsek, Cem and Esin, Ece and Yalcin, Suayib},
  year = {2019},
  file = {/home/asher/Zotero/storage/U36M2DP9/5483791.html;/home/asher/Zotero/storage/VSQB4VQA/Simsek et al. - 2019 - Metronomic Chemotherapy A Systematic Review of th.pdf},
  doi = {10.1155/2019/5483791}
}

@article{kurtz2019,
  title = {Dynamic {{Risk Profiling Using Serial Tumor Biomarkers}} for {{Personalized Outcome Prediction}}},
  volume = {178},
  issn = {0092-8674, 1097-4172},
  language = {English},
  number = {3},
  journal = {Cell},
  doi = {10.1016/j.cell.2019.06.011},
  author = {Kurtz, David M. and Esfahani, Mohammad S. and Scherer, Florian and Soo, Joanne and Jin, Michael C. and Liu, Chih Long and Newman, Aaron M. and D{\"u}hrsen, Ulrich and H{\"u}ttmann, Andreas and Casasnovas, Olivier and Westin, Jason R. and Ritgen, Matthais and B{\"o}ttcher, Sebastian and Langerak, Anton W. and Roschewski, Mark and Wilson, Wyndham H. and Gaidano, Gianluca and Rossi, Davide and Bahlo, Jasmin and Hallek, Michael and Tibshirani, Robert and Diehn, Maximilian and Alizadeh, Ash A.},
  month = jul,
  year = {2019},
  keywords = {cancer,personalized medicine,biomarkers,liquid biopsy,predictive modeling},
  pages = {699-713.e19},
  file = {/home/asher/Zotero/storage/DLMB4CA7/Kurtz et al. - 2019 - Dynamic Risk Profiling Using Serial Tumor Biomarke.pdf;/home/asher/Zotero/storage/52N7YTTM/S0092-8674(19)30639-7.html},
  pmid = {31280963}
}

@article{bru2003,
  title = {The {{Universal Dynamics}} of {{Tumor Growth}}},
  volume = {85},
  issn = {0006-3495},
  abstract = {Scaling techniques were used to analyze the fractal nature of colonies of 15 cell lines growing in vitro as well as of 16 types of tumor developing in vivo. All cell colonies were found to exhibit exactly the same growth dynamics\textemdash{}which correspond to the molecular beam epitaxy (MBE) universality class. MBE dynamics are characterized by 1), a linear growth rate, 2), the constraint of cell proliferation to the colony/tumor border, and 3), surface diffusion of cells at the growing edge. These characteristics were experimentally verified in the studied colonies. That these should show MBE dynamics is in strong contrast with the currently established concept of tumor growth: the kinetics of this type of proliferation rules out exponential or Gompertzian growth. Rather, a clear linear growth regime is followed. The importance of new cell movements\textemdash{}cell diffusion at the tumor border\textemdash{}lies in the fact that tumor growth must be conceived as a competition for space between the tumor and the host, and not for nutrients or other factors. Strong experimental evidence is presented for 16 types of tumor, the growth of which cell surface diffusion may be the main mechanism responsible in vivo. These results explain most of the clinical and biological features of colonies and tumors, offer new theoretical frameworks, and challenge the wisdom of some current clinical strategies.},
  number = {5},
  journal = {Biophysical Journal},
  author = {Br{\'u}, Antonio and Albertos, Sonia and Luis Subiza, Jos{\'e} and {Garc{\'i}a-Asenjo}, Jos{\'e} L{\'o}pez and Br{\'u}, Isabel},
  month = nov,
  year = {2003},
  pages = {2948-2961},
  file = {/home/asher/Zotero/storage/QZMLMEZU/Brú et al. - 2003 - The Universal Dynamics of Tumor Growth.pdf},
  pmid = {14581197},
  pmcid = {PMC1303573}
}

@article{west2001,
  title = {A General Model for Ontogenetic Growth},
  volume = {413},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {6856},
  journal = {Nature},
  doi = {10.1038/35098076},
  author = {West, Geoffrey B. and Brown, James H. and Enquist, Brian J.},
  month = oct,
  year = {2001},
  pages = {628-631},
  file = {/home/asher/Zotero/storage/D429KUZX/West et al. - 2001 - A general model for ontogenetic growth.pdf}
}

@article{ness2018,
  title = {A {{Bayesian Active Learning Experimental Design}} for {{Inferring Signaling Networks}}},
  volume = {25},
  abstract = {Machine learning methods for learning network structure are applied to quantitative proteomics experiments and reverse-engineer intracellular signal transduction networks. They provide insight into the rewiring of signaling within the context of a disease or a phenotype. To learn the causal patterns of influence between proteins in the network, the methods require experiments that include targeted interventions that fix the activity of specific proteins. However, the interventions are costly and add experimental complexity. We describe an active learning strategy for selecting optimal interventions. Our approach takes as inputs pathway databases and historic data sets, expresses them in form of prior probability distributions on network structures, and selects interventions that maximize their expected contribution to structure learning. Evaluations on simulated and real data show that the strategy reduces the detection error of validated edges as compared with an unguided choice of interventions and avoids redundant interventions, thereby increasing the effectiveness of the experiment.},
  number = {7},
  journal = {Journal of computational biology : a journal of computational molecular cell biology},
  doi = {10.1089/cmb.2017.0247},
  author = {Ness, Robert Osazuwa and Sachs, Karen and Mallick, Parag and Vitek, Olga},
  year = {2018},
  keywords = {Machine learning,Active learning (machine learning),Causality,Database,Experiment,Gene regulatory network,Proteomics,Reverse engineering,Transduction (machine learning)},
  pages = {709-725},
  file = {/home/asher/Zotero/storage/ZRHY2SU8/Ness et al. - 2018 - A Bayesian Active Learning Experimental Design for.pdf}
}

@article{andre2013,
  title = {Mathematical Model of Cancer Growth Controled by Metronomic Chemotherapies},
  volume = {41},
  issn = {1270-900X},
  abstract = {We propose in this article to compare the efficiency of two chemotherapeutic schedules: the traditional and the metronomic. For this, we develop a new mathematical model describing the growth dynamics of tumor and endothelial cells as well as the impact of molecules as oxygen or vascular endothelial growth factor on this dynamics. The model construction: biological assumptions, description of the equations and their discretization, constitutes the core of the article. Numerical experiments illustrate the expected behavior of the disease under the two chemotherapeutic schedules.},
  language = {en},
  journal = {ESAIM: Proceedings},
  doi = {10.1051/proc/201341004},
  author = {Andr{\'e}, N. and Barbolosi, D. and Billy, F. and Chapuisat, G. and Hubert, F. and Grenier, E. and Rovini, A.},
  editor = {Chupin, Laurent and M{\"u}nch, Arnaud},
  month = dec,
  year = {2013},
  pages = {77-94},
  file = {/home/asher/Zotero/storage/UFS4MGS5/André et al. - 2013 - Mathematical model of cancer growth controled by m.pdf}
}

@article{benzekry2012,
  title = {A New Mathematical Model for Optimizing the Combination between Antiangiogenic and Cytotoxic Drugs in Oncology},
  volume = {350},
  issn = {1631-073X},
  abstract = {We present in this Note a new mathematical model designed to optimize and to rationalize the association between cytotoxic and antiangiogenic drugs in the treatment of a primary tumor. This model takes into account the non-linear interlinkings between both drugs. In particular, this original model integrates the influence of the vasculature state on the delivery of the drugs to the tumor. In the future, this model could be used in clinical oncology to optimize antiangiogenic-based combinational regimen so as to ensure a maximum efficacy.
R{\'e}sum{\'e}
Nous proposons ici un nouveau mod{\`e}le math{\'e}matique d{\'e}di{\'e} {\`a} l'optimisation th{\'e}rapeutique des combinaisons antiangiog{\'e}niques/cytotoxiques en canc{\'e}rologie. Ce mod{\`e}le permet de rendre compte des interactions non lin{\'e}aires entre les deux traitements et notamment de l'influence de l'{\'e}tat de la vasculature sur l'acc{\`e}s {\`a} la tumeur des mol{\'e}cules administr{\'e}es. A terme, ce mod{\`e}le pourra {\^e}tre utilis{\'e} en oncologie clinique pour piloter les protocoles chimioth{\'e}rapeutiques reposant sur la combinaison d'un antiangiog{\'e}nique et de m{\'e}dicaments cytotoxiques, afin d'en am{\'e}liorer l'efficacit{\'e} antitumorale.},
  number = {1},
  journal = {Comptes Rendus Mathematique},
  doi = {10.1016/j.crma.2011.11.019},
  author = {Benzekry, S{\'e}bastien and Chapuisat, Guillemette and Ciccolini, Joseph and Erlinger, Alice and Hubert, Florence},
  month = jan,
  year = {2012},
  pages = {23-28},
  file = {/home/asher/Zotero/storage/MH2VNP3L/Benzekry et al. - 2012 - A new mathematical model for optimizing the combin.pdf;/home/asher/Zotero/storage/5QW3BD7H/S1631073X11003566.html}
}

@article{west2017,
  title = {Chemotherapeutic {{Dose Scheduling Based}} on {{Tumor Growth Rates Provides}} a {{Case}} for {{Low}}-{{Dose Metronomic High}}-{{Entropy Therapies}}},
  volume = {77},
  copyright = {\textcopyright{}2017 American Association for Cancer Research.},
  issn = {0008-5472, 1538-7445},
  abstract = {We extended the classical tumor regression models such as Skipper's laws and the Norton\textendash{}Simon hypothesis from instantaneous regression rates to the cumulative effect over repeated cycles of chemotherapy. To achieve this end, we used a stochastic Moran process model of tumor cell kinetics coupled with a prisoner's dilemma game-theoretic cell\textendash{}cell interaction model to design chemotherapeutic strategies tailored to different tumor growth characteristics. Using the Shannon entropy as a novel tool to quantify the success of dosing strategies, we contrasted MTD strategies as compared with low-dose, high-density metronomic strategies (LDM) for tumors with different growth rates. Our results show that LDM strategies outperformed MTD strategies in total tumor cell reduction. This advantage was magnified for fast-growing tumors that thrive on long periods of unhindered growth without chemotherapy drugs present and was not evident after a single cycle of chemotherapy but grew after each subsequent cycle of repeated chemotherapy. The evolutionary growth/regression model introduced in this article agrees well with murine models. Overall, this model supports the concept of designing different chemotherapeutic schedules for tumors with different growth rates and develops quantitative tools to optimize these schedules for maintaining low-volume tumors. Cancer Res; 77(23); 6717\textendash{}28. \textcopyright{}2017 AACR.},
  language = {en},
  number = {23},
  journal = {Cancer Research},
  doi = {10.1158/0008-5472.CAN-17-1120},
  author = {West, Jeffrey and Newton, Paul K.},
  month = dec,
  year = {2017},
  pages = {6717-6728},
  file = {/home/asher/Zotero/storage/5ABT7THZ/West and Newton - 2017 - Chemotherapeutic Dose Scheduling Based on Tumor Gr.pdf;/home/asher/Zotero/storage/HHLBG35Y/6717.html},
  pmid = {28986381}
}

@article{west2002,
  title = {Allometric Scaling of Metabolic Rate from Molecules and Mitochondria to Cells and Mammals},
  volume = {99},
  copyright = {Copyright \textcopyright{} 2002, The National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  abstract = {The fact that metabolic rate scales as the three-quarter power of body mass (M) in unicellular, as well as multicellular, organisms suggests that the same principles of biological design operate at multiple levels of organization. We use the framework of a general model of fractal-like distribution networks together with data on energy transformation in mammals to analyze and predict allometric scaling of aerobic metabolism over a remarkable 27 orders of magnitude in mass encompassing four levels of organization: individual organisms, single cells, intact mitochondria, and enzyme molecules. We show that, whereas rates of cellular metabolism in vivo scale as M-1/4, rates for cells in culture converge to a single predicted value for all mammals regardless of size. Furthermore, a single three-quarter power allometric scaling law characterizes the basal metabolic rates of isolated mammalian cells, mitochondria, and molecules of the respiratory complex; this overlaps with and is indistinguishable from the scaling relationship for unicellular organisms. This observation suggests that aerobic energy transformation at all levels of biological organization is limited by the transport of materials through hierarchical fractal-like networks with the properties specified by the model. We show how the mass of the smallest mammal can be calculated ({$\approx$}1 g), and the observed numbers and densities of mitochondria and respiratory complexes in mammalian cells can be understood. Extending theoretical and empirical analyses of scaling to suborganismal levels potentially has important implications for cellular structure and function as well as for the metabolic basis of aging.},
  language = {en},
  number = {suppl 1},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.012579799},
  author = {West, Geoffrey B. and Woodruff, William H. and Brown, James H.},
  month = feb,
  year = {2002},
  pages = {2473-2478},
  file = {/home/asher/Zotero/storage/H6QQHSLY/West et al. - 2002 - Allometric scaling of metabolic rate from molecule.pdf;/home/asher/Zotero/storage/GR5BB7HB/2473.html},
  pmid = {11875197}
}

@article{freyer1986,
  title = {Regulation of {{Growth Saturation}} and {{Development}} of {{Necrosis}} in {{EMT6}}/{{Ro Multicellular Spheroids}} by the {{Glucose}} and {{Oxygen Supply}}},
  volume = {46},
  copyright = {\textcopyright{}1986 American Association for Cancer Research.},
  issn = {0008-5472, 1538-7445},
  abstract = {To investigate the effects of glucose and oxygen on spheroid growth, EMT6/Ro mouse mammary carcinoma cell spheroids were cultured in suspension in either 0.28 mm (20\%) or 0.07 mm (5\%) oxygen and 16.5, 5.5, 1.7, and 0.8 mm glucose. The spheroids initially grew at the same exponential rate in all culture conditions, with spheroid volume and cell number doubling times of 20\textendash{}24 h. The growth rates slowed as the spheroids grew, and the maximum volume and cell number attained at growth saturation were proportional to the oxygen and glucose concentrations in the medium. There was a 500-fold difference in saturation sizes comparing spheroids cultured in the highest oxygen and glucose concentrations to those grown in the lowest. The thickness of the viable cell rims was also positively correlated with the oxygen and glucose concentrations in the medium. Comparison of the growth saturation and viable cell rim data showed an excellent correlation between the onset of central necrosis and the cessation of spheroid growth. A model is presented to explain the observed spheroid growth characteristics by proposing a competition between externally supplied growth and viability-promoting factors and internally generated inhibitory factors produced by the process of necrosis. This model has critical implications for the use of spheroids as models of cellular growth in tumors.},
  language = {en},
  number = {7},
  journal = {Cancer Research},
  author = {Freyer, James P. and Sutherland, Robert M.},
  month = jul,
  year = {1986},
  pages = {3504-3512},
  file = {/home/asher/Zotero/storage/JSLWER5R/Freyer and Sutherland - 1986 - Regulation of Growth Saturation and Development of.pdf;/home/asher/Zotero/storage/FUWMLSPW/3504.html},
  pmid = {3708582}
}

@article{watanabe2016,
  title = {A Mathematical Model of Tumor Growth and Its Response to Single Irradiation},
  volume = {13},
  issn = {1742-4682},
  abstract = {Background
Mathematical modeling of biological processes is widely used to enhance quantitative understanding of bio-medical phenomena. This quantitative knowledge can be applied in both clinical and experimental settings. Recently, many investigators began studying mathematical models of tumor response to radiation therapy. We developed a simple mathematical model to simulate the growth of tumor volume and its response to a single fraction of high dose irradiation. The modelling study may provide clinicians important insights on radiation therapy strategies through identification of biological factors significantly influencing the treatment effectiveness.

Methods
We made several key assumptions of the model. Tumor volume is composed of proliferating (or dividing) cancer cells and non-dividing (or dead) cells. Tumor growth rate (or tumor volume doubling time) is proportional to the ratio of the volumes of tumor vasculature and the tumor. The vascular volume grows slower than the tumor by introducing the vascular growth retardation factor, \texttheta. Upon irradiation, the proliferating cells gradually die over a fixed time period after irradiation. Dead cells are cleared away with cell clearance time. The model was applied to simulate pre-treatment growth and post-treatment radiation response of rat rhabdomyosarcoma tumors and metastatic brain tumors of five patients who were treated with Gamma Knife stereotactic radiosurgery (GKSRS).

Results
By selecting appropriate model parameters, we showed the temporal variation of the tumors for both the rat experiment and the clinical GKSRS cases could be easily replicated by the simple model. Additionally, the application of our model to the GKSRS cases showed that the {$\alpha$}-value, which is an indicator of radiation sensitivity in the LQ model, and the value of \texttheta{} could be predictors of the post-treatment volume change.

Conclusions
The proposed model was successful in representing both the animal experimental data and the clinically observed tumor volume changes. We showed that the model can be used to find the potential biological parameters, which may be able to predict the treatment outcome. However, there is a large statistical uncertainty of the result due to the small sample size. Therefore, a future clinical study with a larger number of patients is needed to confirm the finding.

Electronic supplementary material
The online version of this article (doi:10.1186/s12976-016-0032-7) contains supplementary material, which is available to authorized users.},
  journal = {Theoretical Biology \& Medical Modelling},
  doi = {10.1186/s12976-016-0032-7},
  author = {Watanabe, Yoichi and Dahlman, Erik L. and Leder, Kevin Z. and Hui, Susanta K.},
  month = feb,
  year = {2016},
  file = {/home/asher/Zotero/storage/TR49Q7E7/Watanabe et al. - 2016 - A mathematical model of tumor growth and its respo.pdf},
  pmid = {26921069},
  pmcid = {PMC4769590}
}

@article{enderling2014,
  title = {Mathematical {{Modeling}} of {{Tumor Growth}} and {{Treatment}}},
  volume = {20},
  issn = {13816128},
  abstract = {Using mathematical models to simulate dynamic biological processes has a long history. Over the past couple of decades or so, quantitative approaches have also made their way into cancer research. An increasing number of mathematical, physical, computational and engineering techniques have been applied to various aspects of tumor growth, with the ultimate goal of understanding the response of the cancer population to clinical intervention. So-called in silico trials that predict patient-specific response to various dose schedules or treatment combinations and sequencing are on the way to becoming an invaluable tool to optimize patient care. Herein we describe fundamentals of mathematical modeling of tumor growth and tumor-host interactions, and summarize some of the seminal and most prominent approaches.},
  language = {en},
  number = {30},
  journal = {Current Pharmaceutical Design},
  doi = {10.2174/1381612819666131125150434},
  author = {Enderling, Heiko and Chaplain, Mark},
  month = aug,
  year = {2014},
  pages = {4934-4940},
  file = {/home/asher/Zotero/storage/Z7RSI4XA/Enderling and Chaplain - 2014 - Mathematical Modeling of Tumor Growth and Treatmen.pdf}
}

@article{louzoun2014,
  title = {A Mathematical Model for Pancreatic Cancer Growth and Treatments},
  volume = {351},
  issn = {1095-8541},
  abstract = {Pancreatic cancer is one of the most deadly types of cancer and has extremely poor prognosis. This malignancy typically induces only limited cellular immune responses, the magnitude of which can increase with the number of encountered cancer cells. On the other hand, pancreatic cancer is highly effective at evading immune responses by inducing polarization of pro-inflammatory M1 macrophages into anti-inflammatory M2 macrophages, and promoting expansion of myeloid derived suppressor cells, which block the killing of cancer cells by cytotoxic T cells. These factors allow immune evasion to predominate, promoting metastasis and poor responsiveness to chemotherapies and immunotherapies. In this paper we develop a mathematical model of pancreatic cancer, and use it to qualitatively explain a variety of biomedical and clinical data. The model shows that drugs aimed at suppressing cancer growth are effective only if the immune induced cancer cell death lies within a specific range, that is, the immune system has a specific window of opportunity to effectively suppress cancer under treatment. The model results suggest that tumor growth rate is affected by complex feedback loops between the tumor cells, endothelial cells and the immune response. The relative strength of the different loops determines the cancer growth rate and its response to immunotherapy. The model could serve as a starting point to identify optimal nodes for intervention against pancreatic cancer.},
  language = {eng},
  journal = {Journal of Theoretical Biology},
  doi = {10.1016/j.jtbi.2014.02.028},
  author = {Louzoun, Yoram and Xue, Chuan and Lesinski, Gregory B. and Friedman, Avner},
  month = jun,
  year = {2014},
  keywords = {Humans,Prognosis,Immunotherapy,Cell Proliferation,ErbB Receptors,Gene Silencing,Immune response,Models; Immunological,Pancreatic cancer,Pancreatic Neoplasms,RNA; Messenger,Transforming Growth Factor beta,Tumor Escape},
  pages = {74-82},
  file = {/home/asher/Zotero/storage/NA87INQV/Louzoun et al. - 2014 - A mathematical model for pancreatic cancer growth .pdf},
  pmid = {24594371},
  pmcid = {PMC4011486}
}

@article{sachs2001,
  title = {Simple {{ODE}} Models of Tumor Growth and Anti-Angiogenic or Radiation Treatment},
  volume = {33},
  issn = {0895-7177},
  abstract = {Models of tumor growth and treatment based on one or two ordinary differential equations are heavily used in practice because they are simple but can often still capture the essence of complicated interactions. Currently relevant examples of such models are given here: some classic growth equations, an ODE pair for the interplay between tumor and neovascularization during cancer growth or therapy, and an ODE pair for response to ionizing radiation. Mathematically more sophisticated generalizations of various kinds, usually more realistic but less practical, are mentioned very briefly.},
  number = {12},
  journal = {Mathematical and Computer Modelling},
  doi = {10.1016/S0895-7177(00)00316-2},
  author = {Sachs, R. K. and Hlatky, L. R. and Hahnfeldt, P.},
  month = jun,
  year = {2001},
  keywords = {Angiogenic inhibitors,Radiotherapy and chemotherapy,Time factors,Tumor growth kinetics},
  pages = {1297-1305},
  file = {/home/asher/Zotero/storage/BEFFR4F4/Sachs et al. - 2001 - Simple ODE models of tumor growth and anti-angioge.pdf;/home/asher/Zotero/storage/LQQNDD7X/S0895717700003162.html}
}

@book{gilewski2000,
  title = {Cytokinetics},
  abstract = {Cytokinetics is the study of the kinetics of cellular growth, a fundamental attribute of all multi-cellular life. Because oncology is the study of malignant growth, it is rooted, in a very fundamental sense, in this discipline. All the cardinal features of a cancer\textemdash{}its proclivity to increase in size, to disseminate, and to destroy the function of normal organs\textemdash{}are dependent on the reproduction of its cells. For this reason, growth kinetic concepts pervade clinical thinking in both overt and obscure ways. As evidence, we need only refer to the everyday language of clinicians, which is replete with kinetic terms: indolent growth, rapid growth, slow or no regression (``refractory to therapy''), and brisk regression (``responsive to therapy''). The meanings of these descriptive terms seem intuitive. They are, however, more complex and profound than a superficial familiarity would reveal. Is indolent growth always slow, never to accelerate? Is rapid growth always virulent, never to decelerate? How does the quantification of cellular proliferation relate to macroscopic growth patterns? What are the connections between growth rate and other attributes of cancer? Do the presumed sites of action of anticancer drugs, the disruption of mitosis, associate growth pattern with response to therapy? Is there a difference in this regard between the impact of drugs on cancer cells and on such rapidly proliferating host tissues as hematopoietic progenitors and gastrointestinal mucosa? How do new markers of oncogene expression, themselves related to multiple growth-related processes, relate to prognosis, natural history, and response to therapy?},
  language = {en},
  publisher = {{BC Decker}},
  author = {Gilewski, Teresa Ann and Dang, Chau and Surbone, Antonella and Norton, Larry},
  year = {2000},
  file = {/home/asher/Zotero/storage/RJKNTHMS/NBK20771.html}
}

@book{2003,
  edition = {6th},
  title = {Holland-{{Frei Cancer Medicine}}},
  isbn = {978-1-55009-213-4},
  abstract = {ExcerptThe editors have strived to make every chapter in this book present the composite of relevant science and of clinical knowledge about each cancer. The evolution of disease in the patient is critical to appreciating the special molecular characteristics of that particular type of cancer cell.The practice of cancer medicine in its broadest sense will move forward from the threshold most effectively when the science that has been accomplished and that is evolving is joined at the hip, inseparably, with clinical knowledge of the disease and how it disables the patient. Cancer Medicine-6 offers such a synthesis.},
  publisher = {{BC Decker}},
  year = {2003}
}

@article{rodriguez-brenes2013,
  title = {Tumor Growth Dynamics: Insights into Evolutionary Processes},
  volume = {28},
  issn = {0169-5347},
  shorttitle = {Tumor Growth Dynamics},
  language = {English},
  number = {10},
  journal = {Trends in Ecology \& Evolution},
  doi = {10.1016/j.tree.2013.05.020},
  author = {{Rodriguez-Brenes}, Ignacio A. and Komarova, Natalia L. and Wodarz, Dominik},
  month = oct,
  year = {2013},
  pages = {597-604},
  file = {/home/asher/Zotero/storage/TXEE2CQC/S0169-5347(13)00142-0.html},
  pmid = {23816268}
}

@article{vieira2017,
  title = {Dynamic Modelling of N-of-1 Data: Powerful and Flexible Data Analytics Applied to Individualised Studies},
  volume = {11},
  issn = {1743-7199},
  shorttitle = {Dynamic Modelling of N-of-1 Data},
  abstract = {N-of-1 studies are based on repeated observations within an individual or unit over time and are acknowledged as an important research method for generating scientific evidence about the health or behaviour of an individual. Statistical analyses of n-of-1 data require accurate modelling of the outcome while accounting for its distribution, time-related trend and error structures (e.g., autocorrelation) as well as reporting readily usable contextualised effect sizes for decision-making. A number of statistical approaches have been documented but no consensus exists on which method is most appropriate for which type of n-of-1 design. We discuss the statistical considerations for analysing n-of-1 studies and briefly review some currently used methodologies. We describe dynamic regression modelling as a flexible and powerful approach, adaptable to different types of outcomes and capable of dealing with the different challenges inherent to n-of-1 statistical modelling. Dynamic modelling borrows ideas from longitudinal and event history methodologies which explicitly incorporate the role of time and the influence of past on future. We also present an illustrative example of the use of dynamic regression on monitoring physical activity during the retirement transition. Dynamic modelling has the potential to expand researchers' access to robust and user-friendly statistical methods for individualised studies.},
  number = {3},
  journal = {Health Psychology Review},
  doi = {10.1080/17437199.2017.1343680},
  author = {Vieira, Rute and McDonald, Suzanne and {Ara{\'u}jo-Soares}, Vera and Sniehotta, Falko F. and Henderson, Robin},
  month = jul,
  year = {2017},
  keywords = {dynamic regression,individualised study,N-of-1 methods,single-case},
  pages = {222-234},
  file = {/home/asher/Zotero/storage/SX4U8FK8/Vieira et al. - 2017 - Dynamic modelling of n-of-1 data powerful and fle.pdf;/home/asher/Zotero/storage/P5N86BP5/17437199.2017.html},
  pmid = {28629262}
}

@article{lillie2011,
  title = {The N-of-1 Clinical Trial: The Ultimate Strategy for Individualizing Medicine?},
  volume = {8},
  issn = {1741-0541},
  shorttitle = {The N-of-1 Clinical Trial},
  abstract = {N-of-1 or single subject clinical trials consider an individual patient as the sole unit of observation in a study investigating the efficacy or side-effect profiles of different interventions. The ultimate goal of an n-of-1 trial is to determine the optimal or best intervention for an individual patient using objective data-driven criteria. Such trials can leverage study design and statistical techniques associated with standard population-based clinical trials, including randomization, washout and crossover periods, as well as placebo controls. Despite their obvious appeal and wide use in educational settings, n-of-1 trials have been used sparingly in medical and general clinical settings. We briefly review the history, motivation and design of n-of-1 trials and emphasize the great utility of modern wireless medical monitoring devices in their execution. We ultimately argue that n-of-1 trials demand serious attention among the health research and clinical care communities given the contemporary focus on individualized medicine.},
  number = {2},
  journal = {Personalized medicine},
  doi = {10.2217/pme.11.7},
  author = {Lillie, Elizabeth O and Patay, Bradley and Diamant, Joel and Issell, Brian and Topol, Eric J and Schork, Nicholas J},
  month = mar,
  year = {2011},
  pages = {161-173},
  file = {/home/asher/Zotero/storage/Z9KRFZBK/Lillie et al. - 2011 - The n-of-1 clinical trial the ultimate strategy f.pdf},
  pmid = {21695041},
  pmcid = {PMC3118090}
}

@article{kidd2015,
  title = {Integrative Network Modeling Approaches to Personalized Cancer Medicine},
  volume = {12},
  issn = {1741-0541},
  abstract = {The ability to collect millions of molecular measurements from patients is a now a reality for clinical medicine. This reality has created the challenge of how to integrate these vast amounts of data into models that accurately predict complex pathophysiology and can translate this complexity into clinically actionable outputs. Integrative informatics and data-driven approaches provide a framework for analyzing large-scale datasets and combining them into multiscale models that can be used to determine the key drivers of disease and identify optimal therapies for treating tumors. In this perspective we discuss how an integrative modeling approach is being used to inform individual treatment decisions, highlighting a recent case report that illustrates the challenges and opportunities for personalized oncology.},
  number = {3},
  journal = {Personalized medicine},
  doi = {10.2217/pme.14.87},
  author = {Kidd, Brian A and Readhead, Ben P and Eden, Caroline and Parekh, Samir and Dudley, Joel T},
  month = jun,
  year = {2015},
  pages = {245-257},
  file = {/home/asher/Zotero/storage/MUU9TKLP/Kidd et al. - 2015 - Integrative network modeling approaches to persona.pdf},
  pmid = {27019658},
  pmcid = {PMC4806849}
}

@article{berlow2018,
  title = {Probabilistic Modeling of Personalized Drug Combinations from Integrated Chemical Screen and Molecular Data in Sarcoma},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Cancer patients with advanced disease exhaust available clinical regimens and lack actionable genomic medicine results, leaving a large patient population without effective treatments options when their disease inevitably progresses. To address the unmet clinical need for evidence-based therapy assignment when standard clinical approaches have failed, we have developed a probabilistic computational modeling approach which integrates sequencing data with functional assay data to develop patient-specific combination cancer treatments. This computational modeling approach addresses three major challenges in personalized cancer therapy, which we validate across multiple species via computationally-designed personalized synergistic drug combination predictions, identification of unifying therapeutic targets to overcome intra-tumor heterogeneity, and mitigation of cancer cell resistance and rewiring mechanisms. These proof-of-concept studies support the use of an integrative functional approach to personalized combination therapy prediction for the population of high-risk cancer patients lacking viable clinical options and without actionable DNA sequencing-based therapy.{$<$}/p{$>$}},
  language = {en},
  journal = {bioRxiv},
  doi = {10.1101/396358},
  author = {Berlow, Noah E. and Rikhi, Rikhi and Geltzeiler, Mathew N. and Abraham, Jinu and Svalina, Matthew N. and Davis, Lara E. and Wise, Erin and Mancini, Maria and Noujaim, Jonathan and Mansoor, Atiya and Quist, Michael J. and Matlock, Kevin L. and Goros, Martin W. and Hernandez, Brian S. and Doung, Yee C. and Thway, Khin and Tsukahara, Tomohide and Nishio, Jun and Huang, Elaine C. Huang and Airhart, Susan and Bult, Carol J. and {Gandour-Edwards}, Regina and Maki, Robert G. and Jones, Robin L. and Michalek, Joel E. and Milovancev, Milan and Ghosh, Souparno and Pal, Ranadip and Keller, Charles},
  month = oct,
  year = {2018},
  pages = {396358},
  file = {/home/asher/Zotero/storage/VKP3YACA/Berlow et al. - 2018 - Probabilistic modeling of personalized drug combin.pdf;/home/asher/Zotero/storage/WPYHUN8N/396358v2.html}
}

@article{2019c,
  title = {Latin Hypercube Sampling},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {Latin hypercube sampling (LHS) is a statistical method for generating a near-random sample of parameter values from a multidimensional distribution. The sampling method is often used to construct computer experiments or for Monte Carlo integration.
The LHS was described by McKay in 1979. An independently equivalent technique was proposed by Egl{\=a}js in 1977.   It was further elaborated by Ronald L. Iman and coauthors in 1981. Detailed computer codes and manuals were later published.In the context of statistical sampling, a square grid containing sample positions is a Latin square if (and only if) there is only one sample in each row and each column. A Latin hypercube is the generalisation of this concept to an arbitrary number of dimensions, whereby each sample is the only one in each axis-aligned hyperplane containing it.
When sampling a function of 
  
    
      
        N
      
    
    \{\textbackslash{}displaystyle N\}
   variables, the range of each variable is divided into 
  
    
      
        M
      
    
    \{\textbackslash{}displaystyle M\}
   equally probable intervals. 
  
    
      
        M
      
    
    \{\textbackslash{}displaystyle M\}
   sample points are then placed to satisfy the Latin hypercube requirements; note that this forces the number of divisions, 
  
    
      
        M
      
    
    \{\textbackslash{}displaystyle M\}
  , to be equal for each variable. Also note that this sampling scheme does not require more samples for more dimensions (variables); this independence is one of the main advantages of this sampling scheme. Another advantage is that random samples can be taken one at a time, remembering which samples were taken so far.

In two dimensions the difference between random sampling, Latin Hypercube sampling, and orthogonal sampling can be explained as follows:

In random sampling new sample points are generated without taking into account the previously generated sample points. One does not necessarily need to know beforehand how many sample points are needed.
In Latin Hypercube sampling one must first decide how many sample points to use and for each sample point remember in which row and column the sample point was taken. Note that such configuration is similar to having N rooks on a chess board without threatening each other.
In Orthogonal sampling, the sample space is divided into equally probable subspaces. All sample points are then chosen simultaneously making sure that the total set of sample points is a Latin Hypercube sample and that each subspace is sampled with the same density.Thus, orthogonal sampling ensures that the set of random numbers is a very good representative of the real variability, LHS ensures that the set of random numbers is representative of the real variability whereas traditional random sampling (sometimes called brute force) is just a set of random numbers without any guarantees.},
  language = {en},
  journal = {Wikipedia},
  month = jul,
  year = {2019},
  file = {/home/asher/Zotero/storage/UZUX3TAU/index.html},
  note = {Page Version ID: 908459106}
}

@article{doncaster2014,
  title = {Prospective Evaluation of Designs for Analysis of Variance without Knowledge of Effect Sizes},
  volume = {21},
  issn = {1352-8505, 1573-3009},
  language = {en},
  number = {2},
  journal = {Environmental and Ecological Statistics},
  doi = {10.1007/s10651-013-0253-4},
  author = {Doncaster, C. Patrick and Davey, Andrew J. H. and Dixon, Philip M.},
  month = jun,
  year = {2014},
  pages = {239-261}
}

@misc{zotero-957,
  title = {Examples of {{Analysis}} of {{Variance}} and {{Covariance}}},
  howpublished = {http://www.southampton.ac.uk/\textasciitilde{}cpd/anovas/datasets/index.htm}
}

@article{lin2016,
  title = {Robust Inference for Responder Analysis: {{Innovative}} Clinical Trial Design Using a Minimum p-Value Approach},
  volume = {3},
  issn = {2451-8654},
  shorttitle = {Robust Inference for Responder Analysis},
  abstract = {Responder analysis is in common use in clinical trials, and has been described and endorsed in regulatory guidance documents, especially in trials where ``soft'' clinical endpoints such as rating scales are used. The procedure is useful, because responder rates can be understood more intuitively than a difference in means of rating scales. However, two major issues arise: 1) such dichotomized outcomes are inefficient in terms of using the information available and can seriously reduce the power of the study; and 2) the results of clinical trials depend considerably on the response cutoff chosen, yet in many disease areas there is no consensus as to what is the most appropriate cutoff. This article addresses these two issues, offering a novel approach for responder analysis that could both improve the power of responder analysis and explore different responder cutoffs if an agreed-upon common cutoff is not present. Specifically, we propose a statistically rigorous clinical trial design that pre-specifies multiple tests of responder rates between treatment groups based on a range of pre-specified responder cutoffs, and uses the minimum of the p-values for formal inference. The critical value for hypothesis testing comes from permutation distributions. Simulation studies are carried out to examine the finite sample performance of the proposed method. We demonstrate that the new method substantially improves the power of responder analysis, and in certain cases, yields power that is approaching the analysis using the original continuous (or ordinal) measure.},
  journal = {Contemporary Clinical Trials Communications},
  doi = {10.1016/j.conctc.2016.04.001},
  author = {Lin, Yunzhi},
  month = aug,
  year = {2016},
  keywords = {Clinical trials,Multiple testing,Responder analysis,Robust inference},
  pages = {65-69},
  file = {/home/asher/Zotero/storage/9LXN3ZIW/Lin - 2016 - Robust inference for responder analysis Innovativ.pdf;/home/asher/Zotero/storage/99NNR35E/S2451865415300491.html}
}

@inproceedings{cusumano-towner2019,
  address = {{New York, NY, USA}},
  series = {{{PLDI}} 2019},
  title = {Gen: {{A General}}-Purpose {{Probabilistic Programming System}} with {{Programmable Inference}}},
  isbn = {978-1-4503-6712-7},
  shorttitle = {Gen},
  abstract = {Although probabilistic programming is widely used for some restricted classes of statistical models, existing systems lack the flexibility and efficiency needed for practical use with more challenging models arising in fields like computer vision and robotics. This paper introduces Gen, a general-purpose probabilistic programming system that achieves modeling flexibility and inference efficiency via several novel language constructs: (i) the generative function interface for encapsulating probabilistic models; (ii) interoperable modeling languages that strike different flexibility/efficiency trade-offs; (iii) combinators that exploit common patterns of conditional independence; and (iv) an inference library that empowers users to implement efficient inference algorithms at a high level of abstraction. We show that Gen outperforms state-of-the-art probabilistic programming systems, sometimes by multiple orders of magnitude, on diverse problems including object tracking, estimating 3D body pose from a depth image, and inferring the structure of a time series.},
  booktitle = {Proceedings of the 40th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/3314221.3314642},
  author = {{Cusumano-Towner}, Marco F. and Saad, Feras A. and Lew, Alexander K. and Mansinghka, Vikash K.},
  year = {2019},
  keywords = {Markov chain Monte Carlo,Probabilistic programming,sequential Monte Carlo,variational inference},
  pages = {221--236},
  file = {/home/asher/Zotero/storage/FP9TWNEV/Cusumano-Towner et al. - 2019 - Gen A General-purpose Probabilistic Programming S.pdf}
}

@article{hopkins2018,
  title = {Suppression of Insulin Feedback Enhances the Efficacy of {{PI3K}} Inhibitors},
  volume = {560},
  copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
  issn = {1476-4687},
  abstract = {Glucose\textendash{}insulin feedback can reactivate PI3K in tumours treated with PI3K inhibitors, reducing therapeutic efficacy, but this effect can be reduced by using drugs or diet to suppress the insulin response.},
  language = {en},
  number = {7719},
  journal = {Nature},
  doi = {10.1038/s41586-018-0343-4},
  author = {Hopkins, Benjamin D. and Pauli, Chantal and Du, Xing and Wang, Diana G. and Li, Xiang and Wu, David and Amadiume, Solomon C. and Goncalves, Marcus D. and Hodakoski, Cindy and Lundquist, Mark R. and Bareja, Rohan and Ma, Yan and Harris, Emily M. and Sboner, Andrea and Beltran, Himisha and Rubin, Mark A. and Mukherjee, Siddhartha and Cantley, Lewis C.},
  month = aug,
  year = {2018},
  pages = {499-503},
  file = {/home/asher/Zotero/storage/GDF944BJ/Hopkins et al. - 2018 - Suppression of insulin feedback enhances the effic.pdf;/home/asher/Zotero/storage/Q2YYDGFZ/s41586-018-0343-4.html}
}

@article{juric2018,
  title = {Phosphatidylinositol 3-{{Kinase}} {$\alpha$}\textendash{{Selective Inhibition With Alpelisib}} ({{BYL719}}) in {{PIK3CA}}-{{Altered Solid Tumors}}: {{Results From}} the {{First}}-in-{{Human Study}}},
  volume = {36},
  issn = {0732-183X},
  shorttitle = {Phosphatidylinositol 3-{{Kinase}} {$\alpha$}\textendash{{Selective Inhibition With Alpelisib}} ({{BYL719}}) in {{PIK3CA}}-{{Altered Solid Tumors}}},
  abstract = {Purpose
We report the first-in-human phase Ia study to our knowledge (ClinicalTrials.gov identifier: NCT01219699) identifying the maximum tolerated dose and assessing safety and preliminary efficacy of single-agent alpelisib (BYL719), an oral phosphatidylinositol 3-kinase {$\alpha$} (PI3K{$\alpha$})\textendash{}selective inhibitor.

Patients and Methods
In the dose-escalation phase, patients with PIK3CA-altered advanced solid tumors received once-daily or twice-daily oral alpelisib on a continuous schedule. In the dose-expansion phase, patients with PIK3CA-altered solid tumors and PIK3CA-wild-type, estrogen receptor\textendash{}positive/human epidermal growth factor receptor 2\textendash{}negative breast cancer received alpelisib 400 mg once daily.

Results
One hundred thirty-four patients received treatment. Alpelisib maximum tolerated doses were established as 400 mg once daily and 150 mg twice daily. Nine patients (13.2\%) in the dose-escalation phase had dose-limiting toxicities of hyperglycemia (n = 6), nausea (n = 2), and both hyperglycemia and hypophosphatemia (n = 1). Frequent all-grade, treatment-related adverse events included hyperglycemia (51.5\%), nausea (50.0\%), decreased appetite (41.8\%), diarrhea (40.3\%), and vomiting (31.3\%). Alpelisib was rapidly absorbed; half-life was 7.6 hours at 400 mg once daily with minimal accumulation. Objective tumor responses were observed at doses {$\geq$} 270 mg once daily; overall response rate was 6.0\% (n = 8; one patient with endometrial cancer had a complete response, and seven patients with cervical, breast, endometrial, colon, and rectal cancers had partial responses). Stable disease was achieved in 70 (52.2\%) patients and was maintained {$>$} 24 weeks in 13 (9.7\%) patients; disease control rate (complete and partial responses and stable disease) was 58.2\%. In patients with estrogen receptor\textendash{}positive/human epidermal growth factor receptor 2\textendash{}negative breast cancer, median progression-free survival was 5.5 months. Frequently mutated genes ({$\geq$} 10\% tumors) included TP53 (51.3\%), APC (23.7\%), KRAS (22.4\%), ARID1A (13.2\%), and FBXW7 (10.5\%).

Conclusion
Alpelisib demonstrated a tolerable safety profile and encouraging preliminary activity in patients with PIK3CA-altered solid tumors, supporting the rationale for selective PI3K{$\alpha$} inhibition in combination with other agents for the treatment of PIK3CA-mutant tumors.},
  number = {13},
  journal = {Journal of Clinical Oncology},
  doi = {10.1200/JCO.2017.72.7107},
  author = {Juric, Dejan and Rodon, Jordi and Tabernero, Josep and Janku, Filip and Burris, Howard A. and Schellens, Jan H.M. and Middleton, Mark R. and Berlin, Jordan and Schuler, Martin and {Gil-Martin}, Marta and Rugo, Hope S. and {Seggewiss-Bernhardt}, Ruth and Huang, Alan and Bootle, Douglas and Demanse, David and Blumenstein, Lars and Coughlin, Christina and Quadt, Cornelia and Baselga, Jos{\'e}},
  month = may,
  year = {2018},
  pages = {1291-1299},
  file = {/home/asher/Zotero/storage/H837GXUJ/Juric et al. - 2018 - Phosphatidylinositol 3-Kinase α–Selective Inhibiti.pdf},
  pmid = {29401002},
  pmcid = {PMC5920739}
}

@article{gelman1992a,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  volume = {7},
  issn = {0883-4237},
  abstract = {The Gibbs sampler, the algorithm of Metropolis, and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  language = {en},
  number = {4},
  journal = {Statistical Science},
  doi = {10.1214/ss/1177011136},
  author = {Gelman, Andrew and Rubin, Donald B.},
  month = nov,
  year = {1992},
  pages = {457-472},
  file = {/home/asher/Zotero/storage/7SZ4VLPJ/Gelman and Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf}
}

@article{kurland2005,
  title = {Directly Parameterized Regression Conditioning on Being Alive: Analysis of Longitudinal Data Truncated by Deaths},
  volume = {6},
  issn = {1465-4644},
  shorttitle = {Directly Parameterized Regression Conditioning on Being Alive},
  abstract = {Abstract.  For observational longitudinal studies of geriatric populations, outcomes such as disability or cognitive functioning are often censored by death. St},
  language = {en},
  number = {2},
  journal = {Biostatistics},
  doi = {10.1093/biostatistics/kxi006},
  author = {Kurland, Brenda F. and Heagerty, Patrick J.},
  month = apr,
  year = {2005},
  pages = {241-258},
  file = {/home/asher/Zotero/storage/QKIDAVS7/Kurland and Heagerty - 2005 - Directly parameterized regression conditioning on .pdf;/home/asher/Zotero/storage/N5AVHQS5/242290.html}
}

@article{lan1983,
  title = {Discrete {{Sequential Boundaries}} for {{Clinical Trials}}},
  volume = {70},
  issn = {00063444},
  abstract = {Pocock (1977), O'Brien \& Fleming (1979) and Slud \& Wei (1982) have proposed different methods to construct discrete sequential boundaries for clinical trials. These methods require that the total number of decision times be specified in advance. In the present paper, we propose a more flexible way to construct discrete sequential boundaries. The method is based on the choice of a function, a*(t), which characterizes the rate at which the errorlevel acis spent. The boundary at a decision time is determined by a*(t), and by past and current decision times, but does not depend on the future decision times or the total number of decision times.},
  language = {en},
  number = {3},
  journal = {Biometrika},
  doi = {10.2307/2336502},
  author = {Lan, K. K. Gordon and DeMets, David L.},
  month = dec,
  year = {1983},
  pages = {659},
  file = {/home/asher/Zotero/storage/99CUUA7K/Lan and DeMets - 1983 - Discrete Sequential Boundaries for Clinical Trials.pdf}
}

@article{gordonlan1983,
  title = {Discrete Sequential Boundaries for Clinical Trials},
  volume = {70},
  issn = {0006-3444},
  abstract = {AbstractSUMMARY.  Pocock (1977), O'Brien \&amp; Fleming (1979) and Slud \&amp; Wei (1982) have proposed different methods to construct discrete sequential boundar},
  language = {en},
  number = {3},
  journal = {Biometrika},
  doi = {10.1093/biomet/70.3.659},
  author = {Gordon Lan, K. K. and Demets, David L.},
  month = dec,
  year = {1983},
  pages = {659-663},
  file = {/home/asher/Zotero/storage/9YTYNYK2/247777.html}
}

@book{pampallona,
  title = {Address for Correspondence},
  abstract = {monitoring of group sequential trials using spending functions},
  author = {Pampallona, Ro and Tsiatis, Anastasios A. and Kim, Kyungmann and Chales, Les and Pampallona, Sandro},
  file = {/home/asher/Zotero/storage/FFJ43WFF/Pampallona et al. - Address for correspondence.pdf;/home/asher/Zotero/storage/DQ8LXHTP/summary.html}
}

@article{spena2013,
  title = {Supratentorial {{Gliomas}} in {{Eloquent Areas}}: {{Which Parameters Can Predict Functional Outcome}} and {{Extent}} of {{Resection}}?},
  volume = {8},
  issn = {1932-6203},
  shorttitle = {Supratentorial {{Gliomas}} in {{Eloquent Areas}}},
  abstract = {Background To date, few parameters have been found that can aid in patient selection and surgical strategy for eloquent area gliomas. Aims The aim of the study was to analyze preoperative and intraoperative factors that can predict functional outcome and extent of resection in eloquent area tumors. Patients and Methods A retrospective analysis was conducted on 60 patients harboring supratentorial gliomas in eloquent areas undergoing awake surgery. The analysis considered clinical, neuroradiologic (morphologic), intraoperative, and postoperative factors. End-points were extent of resection (EOR) as well as functional short- and long-term outcome. Postoperatively, MRI objectively established the EOR. {$\chi$}2 analyses were used to evaluate parameters that could be predictive. Multivariate logistic regression analyses were used to evaluate the best combination to predict binary positive outcomes. Results In 90\% of the cases, subcortical stimulation was positive in the margins of the surgical cavity. Postoperatively, 51\% of the patients deteriorated but 90\% of the patients regained their preoperative neurological score. Factors negatively affecting EOR were volume, degree of subcortical infiltration, and presence of paresis (P{$<$}0.01). Sharp margins and cystic components were more amenable to gross total resection (P{$<$}0.01). Contrast enhancement (P{$<$}0.02), higher grade (P{$<$}0.01), paresis (P{$<$}0.01), and residual tumor in the cortex (P{$<$}0.02) negatively affected long-term functional outcomes, whereas postoperative deterioration could not be predicted for any factor other than paresis. Subcortical stimulation did not correlate with deterioration, both postoperatively (P{$<$}0.08) and at follow-up (P{$<$}0.042). Conclusions Biological and morphological factors such as type of margins, volume, preoperative neurological status, cystic components, histology and the type of infiltration into the white matter must be considered when planning intraoperative mapping.},
  language = {en},
  number = {12},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0080916},
  author = {Spena, Giannantonio and D'Agata, Federico and Panciani, Pier Paolo and di Monale, Michela Buglione and Fontanella, Marco Maria},
  month = dec,
  year = {2013},
  keywords = {Central nervous system,Glioma,Magnetic resonance imaging,Neurological tumors,Surgical and invasive medical procedures,Surgical oncology,Surgical resection,Tumor resection},
  pages = {e80916},
  file = {/home/asher/Zotero/storage/FE3D9H8L/Spena et al. - 2013 - Supratentorial Gliomas in Eloquent Areas Which Pa.pdf;/home/asher/Zotero/storage/HGS44GRP/article.html}
}

@article{cantrell2019,
  title = {Progress toward Long-Term Survivors of Glioblastoma},
  volume = {94},
  issn = {0025-6196},
  abstract = {ObjectiveTo identify the frequency and characteristics of long-term survivors of glioblastoma. Patients and MethodsUsing all cases of glioblastoma with histopathological confirmation in the National Cancer Database from January 1, 2004, through December 31, 2009, clinical, institutional, and treatment-related factors were evaluated with multivariable logistic regression models so as to elucidate factors independently associated with higher than 5-year overall survival after diagnosis. ResultsA total of 48,652 patients met the inclusion criteria, with 2249 (4.6\%) achieving 5-year survival. Factors associated with odds of improved 5-year overall survival in multivariable analysis were younger age, female sex, less medical comorbidities, nonwhite race, highest median income quartile, left-sided tumors and tumors outside the brainstem, and treatment with radiotherapy (P\textexclamdown.05 for all). The percentage of patients surviving 5 years remained relatively unchanged over the 6-year study period (P=.97). ConclusionDespite improvements in median and short-term overall survival shown in recent large clinical trials for glioblastoma, the percentage of patients with glioblastoma achieving 5-year overall survival remains low. This observation calls for the development of practice-redefining therapies and justifies the increased application of radical novel and experimental treatment paradigms for all patients with glioblastoma.},
  language = {English},
  number = {7},
  journal = {Mayo Clinic Proceedings},
  author = {Cantrell, J. Nathan and Waddle, Mark R. and Rotman, Maarten and Peterson, Jennifer L. and {Ruiz-Garcia}, Henry and Heckman, Michael G. and {Qui{\~n}ones-Hinojosa}, Alfredo and Rosenfeld, Steven S. and Brown, Paul D. and Trifiletti, Daniel M.},
  month = jul,
  year = {2019},
  keywords = {FDA,Gbm,Idh-1,Kps,Mgmt,Ncdb,OR},
  pages = {1278,1286},
  file = {/home/asher/Zotero/storage/PL6UHM7N/Cantrell et al. - 2019 - Progress toward long-term survivors of glioblastom.PDF},
  publisher = {{Elsevier Inc.}}
}

@article{das2017,
  title = {Re-Inventing Drug Development: {{A}} Case Study of the {{I}}-{{SPY}} 2 Breast Cancer Clinical Trials Program},
  volume = {62},
  issn = {15517144},
  shorttitle = {Re-Inventing Drug Development},
  abstract = {Background: In this case study, we profile the I-SPY 2 TRIAL (Investigation of Serial studies to Predict Your Therapeutic Response with Imaging And molecular anaLysis 2), a unique breast cancer clinical trial led by researchers at 20 leading cancer centers across the US, and examine its potential to serve as a model of drug development for other disease areas. This multicenter collaboration launched in 2010 to reengineer the drug development process to be more efficient and patient-centered.
Methods: We conduct several interviews with the I-SPY leadership as well as a literature review of relevant publications to assess the I-SPY 2 initiative.
Results: To date, six drugs have graduated from I-SPY 2, identified as excellent candidates for phase 3 trials in their corresponding tumor subtype, and several others have been or are still being evaluated. These trials are also more efficient, typically involving fewer subjects and reaching conclusions more quickly, and candidates have more than twice the predicted likelihood of success in a smaller phase 3 setting compared to traditional trials.
Conclusions: We observe that I-SPY 2 possesses several novel features that could be used as a template for more efficient and cost effective drug development, namely its adaptive trial design; precompetitive network of stakeholders; and flexible infrastructure to accommodate innovation.},
  language = {en},
  journal = {Contemporary Clinical Trials},
  doi = {10.1016/j.cct.2017.09.002},
  author = {Das, Sonya and Lo, Andrew W.},
  month = nov,
  year = {2017},
  pages = {168-174},
  file = {/home/asher/Zotero/storage/7DNLZ8G3/Das and Lo - 2017 - Re-inventing drug development A case study of the.pdf}
}

@article{benitez1997,
  title = {Are Artificial Neural Networks Black Boxes?},
  volume = {8},
  abstract = {Artificial neural networks are efficient computing models which have shown their strengths in solving hard problems in artificial intelligence. They have also been shown to be universal approximators. Notwithstanding, one of the major criticisms is their being black boxes, since no satisfactory explanation of their behavior has been offered. In this paper, we provide such an interpretation of neural networks so that they will no longer be seen as black boxes. This is stated after establishing the equality between a certain class of neural nets and fuzzy rule-based systems. This interpretation is built with fuzzy rules using a new fuzzy logic operator which is defined after introducing the concept of f-duality. In addition, this interpretation offers an automated knowledge acquisition procedure.},
  number = {5},
  journal = {IEEE transactions on neural networks},
  doi = {10.1109/72.623216},
  author = {Ben{\'i}tez, Jos{\'e} Manuel and Castro, Juan Luis and Requena, Ignacio},
  year = {1997},
  keywords = {Artificial intelligence,Artificial neural network,Black box,Fuzzy logic,Fuzzy rule,Knowledge acquisition,Rule (guideline),Rule-based system,interpretable machine learning},
  pages = {1156-1164},
  file = {/home/asher/Zotero/storage/N96B5UX8/Benítez et al. - 1997 - Are artificial neural networks black boxes.pdf}
}

@article{cheng2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.06850},
  primaryClass = {cs, stat},
  title = {Polynomial {{Regression As}} an {{Alternative}} to {{Neural Nets}}},
  abstract = {Despite the success of neural networks (NNs), there is still a concern among many over their "black box" nature. Why do they work? Here we present a simple analytic argument that NNs are in fact essentially polynomial regression models. This view will have various implications for NNs, e.g. providing an explanation for why convergence problems arise in NNs, and it gives rough guidance on avoiding overfitting. In addition, we use this phenomenon to predict and confirm a multicollinearity property of NNs not previously reported in the literature. Most importantly, given this loose correspondence, one may choose to routinely use polynomial models instead of NNs, thus avoiding some major problems of the latter, such as having to set many tuning parameters and dealing with convergence issues. We present a number of empirical results; in each case, the accuracy of the polynomial approach matches or exceeds that of NN approaches. A many-featured, open-source software package, polyreg, is available.},
  journal = {arXiv:1806.06850 [cs, stat]},
  author = {Cheng, Xi and Khomtchouk, Bohdan and Matloff, Norman and Mohanty, Pete},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,interpretable machine learning},
  file = {/home/asher/Zotero/storage/RXMRPR8T/Cheng et al. - 2018 - Polynomial Regression As an Alternative to Neural .pdf;/home/asher/Zotero/storage/ZT446QM4/1806.html}
}

@article{stupp2014,
  title = {Cilengitide Combined with Standard Treatment for Patients with Newly Diagnosed Glioblastoma with Methylated {{MGMT}} Promoter ({{CENTRIC EORTC}} 26071-22072 Study): A Multicentre, Randomised, Open-Label, Phase 3 Trial},
  volume = {15},
  issn = {14702045},
  shorttitle = {Cilengitide Combined with Standard Treatment for Patients with Newly Diagnosed Glioblastoma with Methylated {{MGMT}} Promoter ({{CENTRIC EORTC}} 26071-22072 Study)},
  abstract = {BACKGROUND Cilengitide is a selective \dbend{}v\dbend{}3 and \dbend{}v\dbend{}5 integrin inhibitor. Data from phase 2 trials suggest that it has antitumour activity as a single agent in recurrent glioblastoma and in combination with standard temozolomide chemoradiotherapy in newly diagnosed glioblastoma (particularly in tumours with methylated MGMT promoter). We aimed to assess cilengitide combined with temozolomide chemoradiotherapy in patients with newly diagnosed glioblastoma with methylated MGMT promoter. METHODS In this multicentre, open-label, phase 3 study, we investigated the efficacy of cilengitide in patients from 146 study sites in 25 countries. Eligible patients (newly diagnosed, histologically proven supratentorial glioblastoma, methylated MGMT promoter, and age \dbend{}18 years) were stratified for prognostic Radiation Therapy Oncology Group recursive partitioning analysis class and geographic region and centrally randomised in a 1:1 ratio with interactive voice response system to receive temozolomide chemoradiotherapy with cilengitide 2000 mg intravenously twice weekly (cilengitide group) or temozolomide chemoradiotherapy alone (control group). Patients and investigators were unmasked to treatment allocation. Maintenance temozolomide was given for up to six cycles, and cilengitide was given for up to 18 months or until disease progression or unacceptable toxic effects. The primary endpoint was overall survival. We analysed survival outcomes by intention to treat. This study is registered with ClinicalTrials.gov, number NCT00689221. FINDINGS Overall, 3471 patients were screened. Of these patients, 3060 had tumour MGMT status tested; 926 patients had a methylated MGMT promoter, and 545 were randomly assigned to the cilengitide (n=272) or control groups (n=273) between Oct 31, 2008, and May 12, 2011. Median overall survival was 26\textbullet{}3 months (95\% CI 23\textbullet{}8-28\textbullet{}8) in the cilengitide group and 26\textbullet{}3 months (23\textbullet{}9-34\textbullet{}7) in the control group (hazard ratio 1\textbullet{}02, 95\% CI 0\textbullet{}81-1\textbullet{}29, p=0\textbullet{}86). None of the predefined clinical subgroups showed a benefit from cilengitide. We noted no overall additional toxic effects with cilengitide treatment. The most commonly reported adverse events of grade 3 or worse in the safety population were lymphopenia (31 [12\%] in the cilengitide group vs 26 [10\%] in the control group), thrombocytopenia (28 [11\%] vs 46 [18\%]), neutropenia (19 [7\%] vs 24 [9\%]), leucopenia (18 [7\%] vs 20 [8\%]), and convulsion (14 [5\%] vs 15 [6\%]). INTERPRETATION The addition of cilengitide to temozolomide chemoradiotherapy did not improve outcomes; cilengitide will not be further developed as an anticancer drug. Nevertheless, integrins remain a potential treatment target for glioblastoma. FUNDING Merck KGaA, Darmstadt, Germany.},
  language = {en},
  number = {10},
  journal = {The Lancet Oncology},
  doi = {10.1016/S1470-2045(14)70379-1},
  author = {Stupp, Roger and Hegi, Monika E and Gorlia, Thierry and Erridge, Sara C and Perry, James and Hong, Yong-Kil and Aldape, Kenneth D and Lhermitte, Benoit and Pietsch, Torsten and Grujicic, Danica and Steinbach, Joachim Peter and Wick, Wolfgang and Tarnawski, Rafa{\l} and Nam, Do-Hyun and Hau, Peter and Weyerbrock, Astrid and Taphoorn, Martin J B and Shen, Chiung-Chyi and Rao, Nalini and Thurzo, L{\'a}szl{\'o} and Herrlinger, Ulrich and Gupta, Tejpal and Kortmann, Rolf-Dieter and Adamska, Krystyna and McBain, Catherine and Brandes, Alba A and Tonn, Joerg Christian and Schnell, Oliver and Wiegel, Thomas and Kim, Chae-Yong and Nabors, Louis Burt and Reardon, David A and {van den Bent}, Martin J and Hicking, Christine and Markivskyy, Andriy and Picard, Martin and Weller, Michael},
  month = sep,
  year = {2014},
  keywords = {found priors},
  pages = {1100-1108},
  file = {/home/asher/Zotero/storage/FNJ7I92W/Stupp et al. - 2014 - Cilengitide combined with standard treatment for p.pdf}
}

@article{ribeiro2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.04938},
  primaryClass = {cs, stat},
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  journal = {arXiv:1602.04938 [cs, stat]},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/KZVXHL4Q/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf;/home/asher/Zotero/storage/3KM8EG6E/1602.html}
}

@article{luna2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.06793},
  primaryClass = {cs, stat},
  title = {Tree-{{Structured Boosting}}: {{Connections Between Gradient Boosted Stumps}} and {{Full Decision Trees}}},
  shorttitle = {Tree-{{Structured Boosting}}},
  abstract = {Additive models, such as produced by gradient boosting, and full interaction models, such as classification and regression trees (CART), are widely used algorithms that have been investigated largely in isolation. We show that these models exist along a spectrum, revealing never-before-known connections between these two approaches. This paper introduces a novel technique called tree-structured boosting for creating a single decision tree, and shows that this method can produce models equivalent to CART or gradient boosted stumps at the extremes by varying a single parameter. Although tree-structured boosting is designed primarily to provide both the model interpretability and predictive performance needed for high-stake applications like medicine, it also can produce decision trees represented by hybrid models between CART and boosted stumps that can outperform either of these approaches.},
  journal = {arXiv:1711.06793 [cs, stat]},
  author = {Luna, Jos{\'e} Marcio and Eaton, Eric and Ungar, Lyle H. and Diffenderfer, Eric and Jensen, Shane T. and Gennatas, Efstathios D. and Wirth, Mateo and Simone II, Charles B. and Solberg, Timothy D. and Valdes, Gilmer},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Machine Learning,interpretable machine learning,Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/5WFLCSBY/Luna et al. - 2017 - Tree-Structured Boosting Connections Between Grad.pdf;/home/asher/Zotero/storage/N7QENIEM/1711.html}
}

@article{lundberg2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.07874},
  primaryClass = {cs, stat},
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  journal = {arXiv:1705.07874 [cs, stat]},
  author = {Lundberg, Scott and Lee, Su-In},
  month = may,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,interpretable machine learning,Statistics - Machine Learning},
  file = {/home/asher/Zotero/storage/HSLKKBSM/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf;/home/asher/Zotero/storage/4SLD4KRX/1705.html}
}

@article{barratt2017,
  title = {{{InterpNET}}: {{Neural Introspection}} for {{Interpretable Deep Learning}}},
  shorttitle = {{{InterpNET}}},
  abstract = {Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.},
  language = {en},
  author = {Barratt, Shane},
  month = oct,
  year = {2017},
  file = {/home/asher/Zotero/storage/8ISRDSAS/Barratt - 2017 - InterpNET Neural Introspection for Interpretable .pdf;/home/asher/Zotero/storage/KPVI6PT5/1710.html}
}

@article{gaspar1997,
  title = {Recursive Partitioning Analysis ({{RPA}}) of Prognostic Factors in Three Radiation Therapy Oncology Group ({{RTOG}}) Brain Metastases Trials},
  volume = {37},
  issn = {03603016},
  abstract = {Purpose: Promising results from new approaches such as radiosurgery or stereotactic surgery of brain metastases have recently been reported. Are these results due to the therapy alone or can the results he attributed in part to patient selection? An analysis of tumor/patient characteristics and treatment variables in previous Radiation Therapy Oncology Group (RTOG ) brain metastasesstudies was considered necessaryto fully evaluate the benefit of these new interventions.
Methods and Materials: The database included 1200 patients from three consecutive RTOG trials conducted between 1979 and 1993, which tested several different dose fractionation schemesand radiation sensitizers.Using recursive partitioning analysis (RPA), a statistical methodology which creates a regression tree according to prognostic significance, eighteen pretreatment characteristics and three treatment-related variables were analyzed.
Results: According to the RPA tree the best survival (median: 7.1 months) was observed in patients {$<$} 65 years ofiwith a Karnofsky Performance Status (KPS) of at least 70, and a controlled primary tumor with the brain the only site of metastases.The worst survival (median: 2.3 months) was seenin patients with a KPS lessthan 70. All other patients had relatively minor differences in observed survival, with a median of 4.2 months.
Conclusions: Based on this analysis, we suggest the following three classes:Class 1: patients with KPS 2 70, {$<$} 65 years of age with controlled primary and no extracranial metastases; Class3: KPS {$<$} 70; Class 2- all others. Using these classesor stages, new treatment techniaAues can be tested on homogeneous patient groups. 0 1997 ElseGer ScienceInc. - '},
  language = {en},
  number = {4},
  journal = {International Journal of Radiation Oncology*Biology*Physics},
  doi = {10.1016/S0360-3016(96)00619-0},
  author = {Gaspar, Laurie and Scott, Charles and Rotman, Marvin and Asbell, Sucha and Phillips, Theodore and Wasserman, Todd and McKenna, W.Gillies and Byhardt, Roger},
  month = mar,
  year = {1997},
  pages = {745-751},
  file = {/home/asher/Zotero/storage/JKYF5V9P/Gaspar et al. - 1997 - Recursive partitioning analysis (RPA) of prognosti.pdf}
}

@article{chinot2016,
  title = {Upfront Bevacizumab May Extend Survival for Glioblastoma Patients Who Do Not Receive Second-Line Therapy: An Exploratory Analysis of {{AVAglio}}},
  volume = {18},
  issn = {1522-8517},
  shorttitle = {Upfront Bevacizumab May Extend Survival for Glioblastoma Patients Who Do Not Receive Second-Line Therapy},
  abstract = {Background
In this post-hoc, exploratory analysis, we examined outcomes for patients enrolled in the AVAglio trial of front-line bevacizumab or placebo plus radiotherapy/temozolomide who received only a single line of therapy.

Methods
Patients with newly diagnosed glioblastoma received protocol-defined treatment until progressive disease (PD). Co-primary endpoints were investigator-assessed progression-free survival (PFS) and overall survival (OS). After confirmed PD, patients were treated at the investigators' discretion. PFS/OS were assessed in patients with a PFS event who did not receive post-PD therapy (Group 1) and patients with a PFS event who received post-PD therapy plus patients who did not have a PFS event at the final data cutoff (Group 2). Kaplan\textendash{}Meier methodology was used. A multivariate Cox proportional hazards model for known prognostic variables was generated.

Results
Baseline characteristics were balanced. In patients with a PFS event who did not receive post-PD therapy (Group 1; n = 225 [24.4\% of the intent-to-treat population]), the addition of bevacizumab to radiotherapy/temozolomide resulted in a 3.6-month extension in both median PFS (hazard ratio [HR]: 0.62, P = .0016) and median OS (HR: 0.67, P = .0102). Multivariate analyses supported this OS benefit (HR: 0.66). In the remaining patients (Group 2; n = 696), a 5.2-month PFS extension was observed in bevacizumab-treated patients (HR: 0.61, P {$<$} .0001); OS was comparable between the treatment arms (HR: 0.88, P = .1502). No significant differences in safety were observed between the 2 groups.

Conclusion
This exploratory analysis suggests that the addition of bevacizumab to standard glioblastoma treatment prolongs PFS and OS for patients with PD who receive only one line of therapy.},
  number = {9},
  journal = {Neuro-Oncology},
  doi = {10.1093/neuonc/now046},
  author = {Chinot, Olivier L. and Nishikawa, Ryo and Mason, Warren and Henriksson, Roger and Saran, Frank and Cloughesy, Timothy and Garcia, Josep and Revil, Cedric and Abrey, Lauren and Wick, Wolfgang},
  month = sep,
  year = {2016},
  keywords = {found priors},
  pages = {1313-1318},
  file = {/home/asher/Zotero/storage/CS63V99B/Chinot et al. - 2016 - Upfront bevacizumab may extend survival for gliobl.pdf},
  pmid = {27006178},
  pmcid = {PMC4999000}
}

@article{schoeneberger2016,
  title = {The {{Impact}} of {{Sample Size}} and {{Other Factors When Estimating Multilevel Logistic Models}}},
  volume = {84},
  issn = {0022-0973},
  abstract = {The design of research studies utilizing binary multilevel models must necessarily incorporate knowledge of multiple factors, including estimation method, variance component size, or number of predictors, in addition to sample sizes. This Monte Carlo study examined the performance of random effect binary outcome multilevel models under varying methods of estimation, level-1 and level-2 sample size, outcome prevalence, variance component sizes, and number of predictors using SAS software. Mean estimates of statistical power were influenced primarily by sample sizes at both levels. In addition, confidence interval coverage and width and the likelihood of nonpositive definite random effect covariance matrices were impacted by variance component size and estimation method. The interactions of these and other factors with various model performance outcomes are explored.},
  number = {2},
  journal = {The Journal of Experimental Education},
  doi = {10.1080/00220973.2015.1027805},
  author = {Schoeneberger, Jason A.},
  month = apr,
  year = {2016},
  keywords = {estimation,multilevel logistic models,sample size,simulation,statistical power},
  pages = {373-397},
  file = {/home/asher/Zotero/storage/N8HM7B37/Schoeneberger - 2016 - The Impact of Sample Size and Other Factors When E.pdf;/home/asher/Zotero/storage/8IUX94UC/00220973.2015.html}
}

@article{owens,
  title = {Meta-{{Analysis}} of {{Single}}-{{Case Data}}: {{A Monte Carlo Investigation}} of a {{Three Level Model}}},
  language = {en},
  author = {Owens, Corina M},
  pages = {220},
  file = {/home/asher/Zotero/storage/E46RE3A7/Owens - Meta-Analysis of Single-Case Data A Monte Carlo I.pdf}
}

@article{siah2019,
  title = {Machine-{{Learning}} and {{Stochastic Tumor Growth Models}} for {{Predicting Outcomes}} in {{Patients With Advanced Non}}\textendash{{Small}}-{{Cell Lung Cancer}}},
  copyright = {\textcopyright{} 2019 by American Society of Clinical Oncology},
  abstract = {PURPOSE The prediction of clinical outcomes for patients with cancer is central to precision medicine and the design of clinical trials. We developed and validated machine-learning models for three important clinical end points in patients with advanced non\textendash{}small-cell lung cancer (NSCLC)\textemdash{}objective response (OR), progression-free survival (PFS), and overall survival (OS)\textemdash{}using routinely collected patient and disease variables. METHODS We aggregated patient-level data from 17 randomized clinical trials recently submitted to the US Food and Drug Administration evaluating molecularly targeted therapy and immunotherapy in patients with advanced NSCLC. To our knowledge, this is one of the largest studies of NSCLC to consider biomarker and inhibitor therapy as candidate predictive variables. We developed a stochastic tumor growth model to predict tumor response and explored the performance of a range of machine-learning algorithms and survival models. Models were evaluated on out-of-sample data using the standard area under the receiver operating characteristic curve and concordance index (C-index) performance metrics. RESULTS Our models achieved promising out-of-sample predictive performances of 0.79 area under the receiver operating characteristic curve (95\% CI, 0.77 to 0.81), 0.67 C-index (95\% CI, 0.66 to 0.69), and 0.73 C-index (95\% CI, 0.72 to 0.74) for OR, PFS, and OS, respectively. The calibration plots for PFS and OS suggested good agreement between actual and predicted survival probabilities. In addition, the Kaplan-Meier survival curves showed that the difference in survival between the low- and high-risk groups was significant (log-rank test P {$<$} .001) for both PFS and OS. CONCLUSION Biomarker status was the strongest predictor of OR, PFS, and OS in patients with advanced NSCLC treated with immune checkpoint inhibitors and targeted therapies. However, single biomarkers have limited predictive value, especially for programmed death-ligand 1 immunotherapy. To advance beyond the results achieved in this study, more comprehensive data on composite multiomic signatures is required.},
  language = {EN},
  journal = {JCO Clinical Cancer Informatics},
  doi = {10.1200/CCI.19.00046},
  author = {Siah, Kien Wei and Khozin, Sean and Wong, Chi Heem and Lo, Andrew W.},
  month = sep,
  year = {2019},
  keywords = {precision medicine},
  file = {/home/asher/Zotero/storage/UUKTF8PL/Siah et al. - 2019 - Machine-Learning and Stochastic Tumor Growth Model.pdf;/home/asher/Zotero/storage/EH7ZUVKU/CCI.19.html}
}

@article{george2007,
  title = {Response {{Rate}} as an {{Endpoint}} in {{Clinical Trials}}},
  volume = {99},
  issn = {0027-8874},
  abstract = {In this issue of the Journal, Lewis et al. ( 1 ) report the results of a randomized clinical trial in operable osteosarcoma of an extremity designed to compare},
  language = {en},
  number = {2},
  journal = {JNCI: Journal of the National Cancer Institute},
  doi = {10.1093/jnci/djk024},
  author = {George, Stephen L.},
  month = jan,
  year = {2007},
  pages = {98-99},
  file = {/home/asher/Zotero/storage/I5WPC5N5/George - 2007 - Response Rate as an Endpoint in Clinical Trials.pdf;/home/asher/Zotero/storage/S26G8F2J/2522177.html}
}

@article{shrager2019,
  title = {Is {{Cancer Solvable}}? {{Towards Efficient}} and {{Ethical Biomedical Science}}},
  volume = {47},
  issn = {1073-1105, 1748-720X},
  shorttitle = {Is {{Cancer Solvable}}?},
  abstract = {Global Cumulative Treatment Analysis (GCTA) is a novel clinical research model combining expert knowledge, and treatment coordination based upon global information-gain, to treat every patient optimally while efficiently searching the vast space that is the realm of cancer research.},
  language = {en},
  number = {3},
  journal = {The Journal of Law, Medicine \& Ethics},
  doi = {10.1177/1073110519876164},
  author = {Shrager, Jeff and Shapiro, Mark and Hoos, William},
  month = sep,
  year = {2019},
  pages = {362-368},
  file = {/home/asher/Zotero/storage/I3KVKPTG/Shrager et al. - 2019 - Is Cancer Solvable Towards Efficient and Ethical .pdf}
}

@article{crowther2012,
  title = {Individual Patient Data Meta-Analysis of Survival Data Using {{Poisson}} Regression Models},
  volume = {12},
  issn = {1471-2288},
  abstract = {An Individual Patient Data (IPD) meta-analysis is often considered the gold-standard for synthesising survival data from clinical trials. An IPD meta-analysis can be achieved by either a two-stage or a one-stage approach, depending on whether the trials are analysed separately or simultaneously. A range of one-stage hierarchical Cox models have been previously proposed, but these are known to be computationally intensive and are not currently available in all standard statistical software. We describe an alternative approach using Poisson based Generalised Linear Models (GLMs).},
  number = {1},
  journal = {BMC Medical Research Methodology},
  doi = {10.1186/1471-2288-12-34},
  author = {Crowther, Michael J. and Riley, Richard D. and Staessen, Jan A. and Wang, Jiguang and Gueyffier, Francois and Lambert, Paul C.},
  month = mar,
  year = {2012},
  keywords = {reading list},
  pages = {34},
  file = {/home/asher/Zotero/storage/57H3RKBB/Crowther et al. - 2012 - Individual patient data meta-analysis of survival .pdf;/home/asher/Zotero/storage/ALNJVJXN/1471-2288-12-34.html}
}

@article{fitts2018,
  title = {Variable Criteria Sequential Stopping Rule: {{Validity}} and Power with Repeated Measures {{ANOVA}}, Multiple Correlation, {{MANOVA}} and Relation to {{Chi}}-Square Distribution},
  volume = {50},
  issn = {1554-3528},
  shorttitle = {Variable Criteria Sequential Stopping Rule},
  abstract = {The variable criteria sequential stopping rule (vcSSR) is an efficient way to add sample size to planned ANOVA tests while holding the observed rate of Type I errors, {$\alpha$}o, constant. The only difference from regular null hypothesis testing is that criteria for stopping the experiment are obtained from a table based on the desired power, rate of Type I errors, and beginning sample size. The vcSSR was developed using between-subjects ANOVAs, but it should work with p values from any type of F test. In the present study, the {$\alpha$}o remained constant at the nominal level when using the previously published table of criteria with repeated measures designs with various numbers of treatments per subject, Type I error rates, values of {$\rho$}, and four different sample size models. New power curves allow researchers to select the optimal sample size model for a repeated measures experiment. The criteria held {$\alpha$}o constant either when used with a multiple correlation that varied the sample size model and the number of predictor variables, or when used with MANOVA with multiple groups and two levels of a within-subject variable at various levels of {$\rho$}. Although not recommended for use with {$\chi$}2 tests such as the Friedman rank ANOVA test, the vcSSR produces predictable results based on the relation between F and {$\chi$}2. Together, the data confirm the view that the vcSSR can be used to control Type I errors during sequential sampling with any t- or F-statistic rather than being restricted to certain ANOVA designs.},
  language = {en},
  number = {5},
  journal = {Behavior Research Methods},
  doi = {10.3758/s13428-017-0968-5},
  author = {Fitts, Douglas A.},
  month = oct,
  year = {2018},
  pages = {1988-2003},
  file = {/home/asher/Zotero/storage/EUTGDWGH/Fitts - 2018 - Variable criteria sequential stopping rule Validi.pdf}
}


